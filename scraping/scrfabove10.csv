title,url,category,total_likes,paper link,tags,comments
                New to the forum? Start here              ,https://www.smartcontractresearch.org/t/new-to-the-forum-start-here/1162,Community,38,[],['about'],"['What is SCRFThe Smart Contract Research Forum 51 (SCRF) is a premier, open discourse hub dedicated to web3 advancement with tools in place to amplify the latest quality research, reward contributions, deliver insights, and facilitate industry connections. Composed of an active international community, SCRF connects academic researchers with blockchain innovators to collaborate on solutions.This post is for new community members who want to learn about and get involved with SCRF. Please note SCRF pays bounties and recognizes community members who write comments and research summaries for the forum.Welcome to SCRF ChecklistJoin SCRF’s Discord 158 and introduce yourself in the #general chat to start engaging with our open and welcoming communityAttend a Community Call 85 that works best with your timezone to learn about projects and how to get involvedRelevant LinksCommunityDiscord: 158 meet other SCRF community membersCommunity calendar 85: learn about projects at SCRFGithubContent board 111: peruse forum research and content proposalsProject board 39: review projects underway at SCRFDocs 49: learn how SCRF worksSocialsTwitter 41: join SCRF’s public web3 conversationsLinkedIn 25: connect with SCRF supportersMediaYouTube 36: watch videos on blockchain advancementsApple Podcasts 12: listen to discussions on web3 research topicsWhat Next?Feel free to hop in the discord or email us at info@smartcontractresearch.org to connect.If you are interested in crafting a research summary, feel free to submit an Idea Proposal Form 59. Otherwise, visit the Open Positions section on our GitHub 93 to see what roles are open at SCRF.', 'It’s really amazing I got to find a DAO like this, I’m  a newbie that love anything research and stuffs, though I’ve worked with some DAO’s, but I find SCRF so unique. I have some questions for the SCRF team:how can a new SCRF member gain cred?is the SCRF team thinking of a plan to have its own token?Is there a possibility of the SCRF running into bankruptcy soon ?', 'Hi @Freakytainment, thanks for hopping on the forum and for the questions.One thing I want to point out is that we are not trying to be a DAO at this point, though we do place a lot of emphasis on growing our community and enabling them to get more involved.Please refer to this github doc that layouts SourceCred 30 at SCRFNo, as of this time, we are not planning a token. We want to see how much is possible via a forum, grants, and community prior to considering other incentive mechanism structuresNot at this point in time. We are in a fortunate position of having a committed, well-capitalized funder who is sticking with us in the current market conditionsHope to see you around in the discord and on the forum!', 'Hey everyone!  I’m Gift  I am an undergraduate in social science and computer literate,with a long time interest in blockchain – I run a club on my campus, excited learn and experiment more with all of the tools and research already developed from the SCRF! I I’ve been studying about what the smart contract and the forum entails and I would like to communicate with others here. I can’t wait to engage and learn from this community and fit in this great community', 'Welcome to the forum, Gift! Thank you for the brief introduction. I’m looking forward to your various contributions to the discussions here on the forum. If you are also looking to connect with people, you should also join our Discord chat. eleventh:Relevant LinksCommunityDiscord:  meet other SCRF community membersCommunity calendar : learn about projects at SCRF', 'Good evening everyone, I’m really happy to join this platform, just got in while I was making a research. I’m a marketer, designer and entrepreneur. I’m also good in writing and doing some editing jobs. I’d be so pleased to contribute my little quarter here.', 'I am owolabi by name, i am new on this wonderful platform, i am into web designer, researcher and a writer. Got referred by a friend and if presented the opportunity will wish to have a significant contribution.', 'You are welcome @Howodee.You should check out this amazing resource link by @Tolulope SCRF Recommends', 'Hey everyone, I’m lisayanky, I’m happy to join this platform, I’m good in creating and innovating ideas also good at writing. I’d be pleased to study and make significant contribution to smart contract research forum.', 'Hello Everyone. I’m Kimberly. I am relatively new to the Forum. As a former academic, new to the web 3.0 space, I was really drawn in by the research summaries.', 'HiI am Charles Freeborn from Nigeria, and I learned about the SCRF from a former colleague at Protocol Labs yesterday. I am a technical writer with a background in Computer Science. I worked briefly with Protocol Labs and got hooked on helping to build the next generation of the internet. Super excited to be here.', 'Hi My name is @Kvngdrvy and its nice to be in this wonder platform am very interested in the idea of web3 and blockchain and what it can offer to the world of technology innovation .', 'Hello @Kvngdrvy welcome to the SCRF forum, it’s really cool to have you here onboard. It’s really amazing here, you can follow up SCRF on Twitter and also on Discord for more info']"
                Research Summary: Towards A first step to understand flash loan and its application in Defi Ecosystem              ,https://www.smartcontractresearch.org/t/research-summary-towards-a-first-step-to-understand-flash-loan-and-its-application-in-defi-ecosystem/1551,Auditing and Security,45,['https://yajin.org/papers/flashloan.pdf'],"['defi', 'summary']","['TLDRThe researchers explore what flash loans are, how they work, which platforms provide this service, and their overall impact to the defi ecosystem.They identify four major applications (arbitrage, wash trading, flash liquidation, and collateral swap) by studying over 76,000 transcations.Flash loans open up opportunities for financial activities that were previously unavailable without access to large amounts of capital. They are growing in popularity.Core Research QuestionWhat is a flash loan and what are its applications?CitationDabao Wang, Siwei Wu, Ziling Lin, Lei Wu1, Xingliang Yuan, Yajin Zhou, Haoyu Wang, and Kui Ren https://yajin.org/papers/flashloan.pdf 11BackgroundLending Platform: A platform (usually in the DeFi ecosystem) that allows depositors to lock their capital (“collateral”) to borrow assets.Collateralization Ratio: The minimal ratio of collateral value to the value of the debt borrowed, which once reached will trigger the lending platform to sell the collateral and clear the loan (“liquidate”).Flash Loan: A new functionality that enables loan borrowing without collateral, allowing users to take out huge loans they wouldn’t otherwise be able to have without sufficient collateral, so long as they return the funds within a single transaction.Transaction: In the paper, a transaction unless noted otherwise refers to an external transaction, which is an action invoked by accounts controlled by private keys (“Externally Owned Accounts (EOA)”), and will contribute to a state change on the Ethereum blockchain. EOAs are different than accounts that are governed by smart contracts as they are controlled by user actions.Arbitrage: Taking advantage of price differences between markets by buying low from one place and selling high in another.Wash Trading: Trading to create artificial activity with the intention to lead people to perceive a platform or asset as more popular that it actually is. This controversial behavior is widely banned in traditional markets.Flash Liquidation: A liquidation backed by flash loans. Once a loan reaches its collateral ratio, the collateral becomes available for anyone (“liquidator”) with capital to trade in via a fixed discount or by auction (“liquidation”).Collateral Swap: To avoid liquidation at severe price changes, the loan taker redeems the collateral from the old loan and opens a new position with a different asset.SummaryFlash loans revolutionize a specific type of loan-taking by locking an entire lend-and-borrow workflow within a single transaction.First, flash loan providers transfer requested assets to users. Then they invoke users’ pre-designed operations. Users interact with other contracts to execute operations with borrowed assets. Once the execution is completed, users return the borrowed assets with or without the extra fee charged by flash loan providers. Finally, flash loan providers will check their balance. If they discover that no or non-sufficient assets are returned by users, they will revert the transaction immediately.The figure below (Figure 1) shows the workflow of a Flash Loan transaction.image694×264 85.6 KBThe authors design three patterns to identify flash loan transactions. Based on the patterns identified, 76,303 transactions were found within the Ethereum ledger. The numbers suggest that flash loan services are becoming increasingly popular over time.Flash loans can be applied to other purposes such as arbitrage, wash trading, flash liquidation and collateral swap.Malicious users may capitalize on flash loans function to launch operations with large amounts of assets that they do not have.MethodThe researchers use both qualitative and quantitative methods to explore the nature of flash loans and their applications.They consult existing literature on flash loans generally and specifically to the Aave, dYdX and Uniswap V2 platforms to study the nature of flash loans and to identify flash loan transactions.Under the quantitative approach, the researchers collected over 1,000,000,000 transactions from the Ethereum blockchain ledger, identified flash loan transactions by the patterns found qualitatively, and conducted a time series analysis to understand the application’s popularity.ResultsThe research identified over 76,303 Flash Loan transactions and 1,454 receivers; their distribution among Flash Load providers is detailed in table 1.image882×269 72.3 KBThey’ve also quantified how popular flash loans have become over time.image936×508 52.4 KBArbitrage opportunities are enabled by flash loans because they allow users to take on price differences without pre-owning collateral.Wash trading, used to create trading volume, requries huge amounts of capital, and are enabled by flash loans taken out to contribute to this activity.Flash liquidation enables people without capital to seek profit by buying undercollateralized assets at discount.Collateral swaps powered by flash loans would solve users’ need for swapping by providing capital for them to pay back their loan and swap collateral.Discussion and Key TakeawaysFlash Loans: The research sheds a light on the function of flash loans and their application using real world examples.Malicious operation: The research identifies potential manipulation that can occur during flash loans transactions and proposes future research on it.Applications: Flash loans can be applied to arbitrage, wash trading, flash liquidation and collateral swap.Implications and Follow-upsThe researchers suggest two potential research directions: arbitration and DeFi attacks. Flash loans have enabled arbitration bots, which take on price differences in a timely manner to maximize profits. DeFi attacks are proliferating. Effective countering strategies are an open question. These attacks are sometimes powered by flash loans.ApplicabilityFlash loans enable financial activities that were before not possible without large amounts of capital. They are a double-edged sword in this ecosystem and should be studied carefully.The research could be useful to DeFi operators. Anyone transacting with DeFi can use this research to understand the what a flash loan is, the problems associated with them, and how best to overcome them.', '@Samuel94 thanks so much for contributing this summary – I’m curious, what are the positive applications for a flash loan? Are there constructive things that one could do having a million dollars for a microsecond?', 'Flash Loan functionality will be very useful for small medium enterprise and startups that require  loan to establish their business ideas with little or no collateral  most especially in the developing countries', 'Would having a loan for a moment be much use though? I thought a flash loan was a single transaction worth of value. I guess there could be legitimate uses of flash loans like in defending a DAO from a governance attack or punishing a sandwich attacker 2. Do you have any other suggestions for potential use cases? I’m sure I’m missing something big and obvious.', 'Flash loan plays an important role or aids arbitrage traders. So for instance,  where two trading platforms offer different prices for same token, the instantaneous nature of flashloan could aid a trader access loan to sell in one platform and purchase in another platform  thereby taking advantage of low and high prices in the different platforms.', '@Samuel94 on balance do you think that the crypto community should take steps to counter flash loan based arbitrage? Or do you see them as a vital part of the ecosystem? They definitely seem to help make sure that oracles are accurate.', 'Good summary here Samuel. I was excited reading and learning about flash loan for the first time.As someone who has used a  borrowing platform to borrow crypto fund for trading, I was curious about a lot of things on flash loan. Specifically, I wanted to understand the differences between flash loan and the traditional borrowing in DeFi. Here are my findings:Flash loan has four specific end use for any user intending to maximise it. You could either use the loaned fund for arbitrage, flash liquidation, wash trading or collateral swap. But traditional DeFi borrowed funds could be used for staking, spot trading, etc.Flash loans are executed using smart contracts where all intended actions have been written to be performed by the contract once it has been executed. But traditional borrowing on DeFi is done manually by pressing buttons on DeFi platforms. In otherwords, the actions are not automatic.Flash loan providers might or might not charge transaction fees. Flash loan does not also require a collateral, but DeFi traditional borrowing comes with both collateral and transaction fees charged by the loan providers.Flash loan, like the name implies, begins and gets completed in one single transaction executed by a smart contract, but Traditional DeFi loans involves series of actions that could last as long as the borrower can afford or the initial time fixed for termination before borrowing.PS: I am intrigued that Compound (Aave) charges a transaction fee of 0.25% , Uniswap charges 0.3%, but dydx charges nothing for flash loans. What then incentivizes dydx to operate the flash loan function?', 'Thank you Mathew for taking time to read my summary and doing further research on it.For your question, I will start with a premise that the essence of Flash loan is to reduce fees payable in borrowing or lending money.  dYdX embodies this feature. It doesn’t charge gas fee for facilitating the transaction but requires bortowers to return the borrowed amount in addition to 2wei. The 2wei that accompanies the borrowed sum is their gain or profit.Let me know if i answered you question', 'Thank you @Samuel94. This, sure, answers my question.', 'Great and well-detailed summary Samuel.I first read about flash loans from an email I got from Luke Willis, of Koinos blockchain.He talked about how users can initiate loan transactions worth millions of dollars with zero collateral to do anything they please, so long as they return the loan in the same transaction.The concept went over my head a little then even though he used a trading and arbitrage analogy, but I get it now.I for one think flash loans are a great feature in DeFi. It would definitely provide more unbanked people with financial services and ease.However, just like you said, they are double edged swords. The possibility of malicious actors getting into the transactions and stealing funds remains.So our focus going forward should also be on security measures and minimizing losses.', 'You are on point.  Attack on flash loan seems to be the worst in  DeFi ecosystem', 'Thank you @Samuel94 for this take on Flash Loan. It’s my first time of learning about it and I am excited I did.Just as you said, it’s a double-edged sword. Is there a possible security measure in flash loan applicability to minimize any attack associated with it in Defi ecosystem?', 'Thank you @Chrisarch for your kind words. Just as i pointed out in the research, flash loan attacks and preventive mechanisms are still subject of research.  However, some authors have pointed out that using custodial wallet and decentralised lending system could minimise the attacks. Also, a research summary here suggested using Flashsyn which could help detect flaws in smart contract that is capable of leading to an attack', 'Thank you @Samuel94 for the clarification.', 'I have some personal contributions which I’d like to share   though I’d make it as a summary→ When taking flash loans you must pay back almost immediately let’s say within 13 second→ Flashloans in Trading arbitrage involves borrowing for trading and profit purpose, for example We can borrow $100 flash loans from a lender and use it to buy crypto from Coinbase at $100 then sell it to Gemini at $101 thereby making $1 profit.But with flashloans we can borrow $2,000,000 and buy the crypto then resell the Gemini thereby making $202,000,000 which is a profit of $2m.→ The fee charged by Flash loan lenders is only a little amount, I think 0.09% which is cool to go with cause a reasonable profit is made.→ Collateral swap has less potential for profit but it’s a useful tool for those borrowing and lending out.→ Flash loan attacks happen as a result of bugs and exploits. (Most developers believe the exploitation is ethical and should be allowed, do you think this is correct) @Samuel94', 'Thank you @Freakytainment for further explaining this concept in a simpler terms. To your question, as an attorney I will start my response by providing the definition of the word “ethical” from Merriamwebster which defines “ethica” asof or relating to ethics2: involving or expressing moral approval or disapproval//ethical3: conforming to accepted standards of conductAttack as defined by same dictionary means:to set upon or work against forcefully2 to assail with unfriendly or bitter words3 to begin to affect or to act on injuriouslyTherefore, i will say that a combined reading of the definitions of the two words shows contradiction and two poles that can never meet. Manipulation is never ethical. So an undue advantage taken over a defect system can never been seen as ethical since such is not generally acceptable by the society or even Blockchain ecosystem.Therefore, i do not agree with the views of the developers.', 'Thanks @samuel94 for this, I was making a research an hour ago on it after waiting for your point of view on this, well I’d also agree with you that it’s never ethical.After taking advantage of the bug in the system and taking a flash loan for maybe a trading arbitrage, those supporting this action pay back the flash loan to the lenders but never with the little fee attached to it, this is unfair and therefore an act of dishonesty.', 'I agree with you @Freakytainment', '@Samuel94, on the 7th of September, some attackers stole about $370,000 from Avax through a flash loan attack. Again, today about $1.25 million was stolen from New Free DAO, a recent DeFi project. Much more money has been lost to flash loan attacks, but I picked on these two as they are the most recent cases.Research Summary: Towards Understanding Flash Loan and Its Applications in DeFi EcosystemAaveFlash loan service: The first platform to officially provide the ‘flashLoan’ function.Flash loan fee: 0.09% of the flash loanTotal value locked: $1.12BdYdXFlash loan service: no official function, can be performed by calling the function ’withdraw’ before ’deposit.’Flash loan fee: 0Total value locked: $24MbZxFlash loan service: no official function.Flash loan fee: 0Total value locked: $250KUniswapV2Flash loan service: they provide a feature called ‘flash swap.’Flash loan fee: 0.3% of the borrowed assetTotal value locked: $2.7BDeFi platforms charge these little percentages in fees which I see as the only incentive for enabling flash loans on their platforms. Except you want to include generating trading activities as an added advantage. In fact, some platforms do not even charge a fee. So considering the losses incurred during flash loan attacks, is enabling flash loan functionality a good return on investment for these platforms?Also, are there other gains or incentives that obligates them to provide the service? Are they just being “patriotic” to blockchain technology innovation?', 'Thank you @Ulysses for this brilliant question.Every Flash loan protocol makes money from the transaction either during the act of borrowing or inform of interest,  albeit very small amount.The act of charging small amount in my understanding is a sort of business strategy which  every business devises to scale up. Very unfortunate there is now attack here and there but that is not sufficient to see flash loan as a failed  project. The series of Flash loan attacks will propel developers to create a solution.  For protocol that hasn’t suffered attack, it will still be a very good business strategy.Also the desire to develop and grow web 3 is a motivating factor which won’t be bad if you term it patriotism.']"
                Research Summary: NFT Wash Trading: Quantifying Suspicious Behaviour in NFT Markets              ,https://www.smartcontractresearch.org/t/research-summary-nft-wash-trading-quantifying-suspicious-behaviour-in-nft-markets/1982,Oracles and Data,47,['https://arxiv.org/abs/2202.03866'],"['summary', 'oracles', 'defi', 'scalability', 'iot']","['TLDRThis paper tries to answer the question of how prevalent wash trading is in smart contract-based NFT markets on Ethereum.Wash trading is defined as a set of trades, without taking market risks, that lead to no change of the initial position of the adversarial agents. As these trades are topologically closed directed cycles, the method used to detect them is by using the deep first search algorithm.At least 2.04% of NFT transactions and 3.93% of all addresses show suspicious behavior.Core Research QuestionTo what extent does wash trading occur in smart contract-based NFT markets on Ethereum and to which extent does this practice distort prices?Citationvon Wachter, Victor, et al. “NFT Wash Trading: Quantifying suspicious behaviour in NFT markets.” arXiv preprint arXiv:2202.03866 (2022).arXiv.orgNFT Wash Trading: Quantifying suspicious behaviour in NFT markets 5The smart contract-based markets for non-fungible tokens (NFTs) on theEthereum blockchain have seen tremendous growth in 2021, with trading volumespeaking at 3.5b in September 2021. This dramatic surge has led to industryobservers questioning the...BackgroundNFT: A non-fungible token that represents a unique digital or physical asset on a blockchain. In contrast to other tokens (e.g. ERC20), NFTs are not divisible and have a unique identifier.Washtrading: A set of trades, without taking market risks, that lead to no change of the initial position of the adversarial agents.Collections: Items traded on NFT marketplaces are organized in collections. These are sets of NFTs that usually share some common features.Special characteristics of NFT markets: NFT markets differ significantly from traditional financial markets: Users connect to the market through wallets, thus on-chain trading does not impose extensive onboarding and identity requirements. As users can create an arbitrary number of addresses for free and remain pseudonymous, these characteristics make preventing illicit activities a challenging task.Directed Multigraph: Each blockchain address is treated as a node and each transaction as an edge. Transactions are directional, thus the edges are well.SummaryThe subject of the paper is the occurrence of wash trading in smart contract-based NFT markets.Wash trading is well-researched and usually prohibited in traditional financial markets. However, not much data exists for blockchain-based marketplaces.The data analyzed comprises the 52 leading ERC721 NFT collections on the Ethereum blockchain.In total, 21,310,982 transactions of 3,572,483 NFTs, conducted between 2015 and late October 2021 are analyzed.These transactions are modeled as directed, cyclical graphs and analyzed using a Deep-First-Search-Algorithm.Path trades with an unusually high velocity are treated as suspicious behavior. Additionally, clusters of addresses that trade with significantly fewer trade partners than others are suspicious as well.The results indicate a high concentration of illicit activities around a few NFTs. Despite finding evidence for a significant amount of wash trading activity, less suspicious activity occurs than the authors initially estimated. (See the section below for further details).MethodDataset: The dataset contains 21,310,982 transactions of 3,572,483 NFTs conducted by 459,954 addresses. Collectively, the dataset represents $6.9b of the $12.3b total trading volume (49.5%) on all NFT markets. The timespan analyzed reaches from the genesis of the Ethereum blockchain in 2015 to 10/2021.Data collection method: Smart contract transaction data for the 4 largest marketplaces (OpenSea API, Foundation, Rarible and Superrare) is collected. The authors focus on the 52 leading ERC721 NFT collections on the Ethereum blockchain. This event data is parsed and enriched with USD prices from the coingecko API and blockchain data from Etherscan. The dataset is then pre-processed to eliminate technical errors or transactions with faulty data.Analysis: The transaction history of each NFT is modeled as a directed multigraph Gnft=(N,E)G_{nft}=(N, E)Gnft\u200b=(N,E), where NNN is the set of addresses and EEE is the set of ERC721 transactions between addresses. The authors utilize the Deep-First-Search-Algorithm to identify closed cycles within the data set. Examples of closed cycles:Results3.93% of all addresses are involved in wash trading activities. These flagged addresses processed 2.04% of the total sale transactions, inflating the trading volume by $149.5m or 2.17%.Of the 36,385 flagged sale transactions, 30,467 were conducted in clusters of cyclical patterns whereas 5,918 were conducted as a rapid sequence. 48.4% of all detected cycles happen within 1 day and 13.4% of all detected cycles happen within 7 days.The elapsed time to close a cycle with respect to the number of transactions involved are shown in the figure below:788×392 99.3 KBThe dominating form of closed cycles are simple patterns with 2-leg (e.g. Alice to Bob and Bob back to Alice).The suspicious activity was executed with just 0.45% of the NFTs in the dataset, indicating a high concentration of illicit activities around a few NFTs.Further, through the analysis of the relationship between executed trades per address and unique trade partners per address, a cluster of suspicious addresses was identified that traded 25-37 times with only 12-17 unique trade partners.Discussion and Key TakeawaysThe authors argue that the amount of $149.5m and the median of 2.04% of suspicious sale transactions indicate that wash trading may be less common than previous estimations by industry observers.However, the level of suspicious activity varies significantly across NFT collections.The data shows that adversarial market participants prefer fast and simple cyclical patterns.Age and sentiment are often more relevant to price discovery than illicit trading activities.The results can be seen as a lower bound estimation for suspicious trading behavior on decentralized NFT markets.Implications and Follow-UpsThe paper represents a solid theoretical contribution in the form of empirical statistics on fraudulent behavior in NFT markets.An additional aspect worth studying is the incentivization of NFT trading volume through tokens (e.g. $LOOK) and the impact on price, trading volume, and suspicious behavior.The correlation between sentiment data and suspicious behavior is another potentially interesting research topic.Further, the authors suggest looking into the utilization of flash loans for NFT wash trading.A limitation is that only operations conducted within the Ethereum Virtual Machine (EVM) were analyzed, so any “off-chain” transactions are not included in the dataset and thus out of scope.Only Ethereum mainnet transactions were analyzed, other blockchain networks were out of scope - this represents a research opportunity!ApplicabilityThe paper provides valuable insights for NFT collectors to prevent them from buying NFTs at potentially inflated prices.The authors contribute to a deeper understanding of the prevalence of wash trading and discuss practical countermeasures for NFT marketplaces.', 'Thank you @f13r for this summary. Since Wash trading are involved in over 3.95% of the transactions carried out, are they practical steps to mitigate it in NFT transactions since it’s deemd an illegal trading?', 'How does the number compare with other recent papers? I remember SCRF ran another summary of a different study that also flagged huge wash trading on $LOOK but it seemed to find about 3% elsewhere. Does that feel right? In my own experience trading NFTs, it seems as though there’s plenty of undetectable backroom dealing which would suggest the numbers should be much higher. Curious how all of this has changed given the recent chill in the NFT markets too. Any thoughts? Are the bad actors moving elsewhere?', 'Hi there, According to an article on coindesk.com 1 , there are signs to watch out when an NFT have been wash traded. These signs include the following:Price: It’s possible that the NFT you’re looking to purchase has been wash-traded if its price is significantly higher than the collection’s floor price (the lowest price an NFT is selling for in a specific collection). This is especially true if the NFT in question has few to no rare characteristics that might justify a higher price point.Transaction history: The transaction history of an NFT can be checked using programs like Etherscan and BscScan. These details are also displayed on the listing sites of some marketplaces, such OpenSea. Wash trading may be detected by a rapid, inactive increase in the price of an NFT.Previous owners: A wallet address like CryptoPunk 9998 that appears several times in the transaction history should be avoided. If the same wallet has made several purchases of an NFT, wash trading may be taking place. Another possible indicator that the wallets may be closely related to one another is to check individual wallet addresses to see whether they have interacted with other wallet addresses included in an NFTs transaction history.I believe that with those signs in mind, potential NFT buyers will be able to mitigate NFT washed- trade transactions.@f13r , you can still add more if you have. I really enjoyed reading your summary.', 'Before @f13r comes in to take this question, I think that a good number of the wash trading happens at a peer to peer level. Anybody can create any amounts of wallets at ease and transact with oneself. This could be hard to control but not impossible.Perhaps,  NFT marketplaces could come in here by watching suspicious transactions to suspend them or deanonymizing wallets on a worst case scenario. But this breaches privacy anyway. In the end it gets complicated.', 'I believe that carrying put surveillance on fraudulent activities with the aim of curbing it will not amount to data protection and Privacy breach. However, caution must be taken why doing so to ensure that the principles of necessity and proportionality is complied with.Deanonymisation  on its own doesn’t amount to data breach, it only brings the data within the definition of personal data and thus requires best practices in handling it.', 'Thank you @Cashkid18 for this detailed response. However, i have some reservations on the mechanism that suggests that frequent wallet address should be avoided. What if the frequency of the wallet address is as a result of honest trading? In the event this mechanism is adopted,  don’t you think it will introduce unnecessary suspicion on wallets that recur frequently and thus impact overall trading negatively?', 'Thanks @f13r for the nice summary, in thisresearch paper, I have just two comments;a. the legality of wash trading with NFT.b. Do you think that wash trading has really affected the NFT market so far?In customary finance, wash trading is unlawful. The US was the main country to proclaim it unlawful in the 1936 Commodity Exchange Act, and presently, the training is illegal in pretty much all aspects of the world.Nonetheless, in the decentralized universe of NFTs, the lawfulness of wash exchanging is yet to be obviously spelt out. While it is dishonest, there’s no regulation anyplace on the planet enumerating what is correct and what isn’t in that frame of mind as its resource class hasn’t been recognized.Regardless of this noticeable shortfall of NFT guideline and class assurance, a few nations have stood firm on the training. For instance, Bithumb, a South Korean crypto trade, was blamed for permitting wash exchanging worth more than $250 million phony volume in 2018.Despite the fact that digital money wash exchanging may be unlawful in certain nations, it is still difficult to distinguish the culprits on account of the decentralized idea of cryptographic forms of money.In my opinion, Wash trading with NFTs doesn’t seem to have affected the NFT market so far. Regardless, the market is showing improvement over and over.OpenSea, the biggest NFT commercial center on the planet, outperformed $3.5 billion month to month exchanging volume January 2022. While NFT exchange volumes keep on developing in spite of the tricks, Chainalysis’ report uncovers that most wash trading didn’t yield however much the work that went into them.', 'Hi there,I think what this scenario is trying to explain is that if you notice the same wallet address have purchased a unique NFT multiple times, then you should take it as red flag.Notice that I was specific with the same wallet address buying a particular NFT many times.A case of the same wallet address buying different NFTs many times shouldn’t be taken as a sign or a red flag for NFT wash trading.I’m still open to suggestions and corrections though.', 'great summary work @fl3r really learned a lot about wash trading, it even led me to other illicit practices with NFT’S .i have a question are there ways though, will like to know if there are ways to detect wash trading ?', 'Exactly, the act in itself doesn’t cause the breach but the means.', 'Hi @GloriaOkoba, pleasant perception, I believe that where there is a wrong, there is a remedy. I think scour could be a solution to wash trading. Scour is a unique product created by BitsCrunch, an AI-powered analytics company that can actively remove wash traders from exchanges. I think bitsCrunch is the Guardian of the NFT ecosystem.The Scour index compiles transactions, wallet addresses, and distributions of reward tokens. Using knowledge graphs and AI technology, Scour can identify bogus orders and complex patterns of wash trades. AI-Enhanced Safety Feature (SCOUR),I don’t know whether my remark has addressed your inquiry. if not, i believe @f13r would do justice to your question.', 'Thank @Henry for this wonderful suggestion. AI are known to be biased sometimes especially where it is so programmed and no human intervention.  Is there a way to detect if such AI is programmed to be bias to certain wallets addresses?', 'Hi @Samuel94 Nice observation, yes, I agree with you that occasionally AI could be biased I think the explanation is on the grounds that External Audits are Challenging because of Privacy Regulations. Don’t worry let me give a context.in scenario where AI applications are used in high-stakes environments, many believe that external audits should be used to systematically vet algorithms to detect potential biases. This may be an excellent idea but often privacy is an issue. So, to thoroughly evaluate an algorithm 1 one needs not only access to the model but also to the training data. But companies cannot share the customer data they use to develop models, as they need to comply with GDPR, CCPA, and other privacy regulations.I believe that the EU’s AI Act (still in draft form) will help, as it requires organizations to use fair training data and ensure that their AI algorithms don’t discriminate. More also, the Equal Credit Opportunity Act stops any creditor from discriminating against any applicant from any type of credit transaction based on protected characteristics. I equally hold the view that companies and other operators should guard against violating these statutory guardrails in the design of algorithms.In addition, Public pressure can play a role in persuading companies to make AI fairness a priority.I am of the view that algorithm could be a strategy for detecting and possibly curing intended and unintentional biases in specific wallet addresses. I hope I am able to address your concern? I am open to learn if there is contrary view on this.', 'In summary @Henry you believe AI should at all time have whom interventions to check its biases?', 'As part of today’s coffee house discussion hosted by @jringo it came to my attention that an implication can be drawn from the conclusions of the paper that is simply too big to ignore.A very interesting point was that the final percentage of wash-trading (~2% of total Txs) seems suspiciously low given people’s experience around NFT project communities and the overall permissionless and trustless nature of web3 with the motto “Don’t Trust, Verify”. There seems to be a suspicion about the final numbers which goes against the common narratives, journalistic work, and first-hand experiences of web3 participants. It is very interesting to investigate this sentiment because it hints at the fact that the conclusions of this paper might be critical for howe we think about the social dimension of web3.Interpreting the resultsThe conclusions lead to the following possibilities:False negative: The number of wash-trading is misleading and is an artifact of a methodological fluke in the heuristics used for representing wash-trading (or other methodological parameters).Folk-psychological bias: The number of wash-trading represents the underlying reality and the discrepancy is due to erroneous prior intuitions and underlying assumptions about human behavior on web3.False Negatives and Methodological HurdlesIf 1 is the case, (as the authors are careful to point out as a possibility) it means that additional studies are needed to form clear hypotheses based on the statements and conclusions of the current paper and find where the contention lies. The most obvious culprit is that the heuristics used such as closed loops and high transaction frequency might be either too constraining or not suitable. There is no reasoning in this paper as to why these two heuristics were chosen in the first place although it seems that the quantified representation originates from (Das et al, 2021). But as mentioned in that original literature, they were the first to construct a quantified model of malicious behavior in the context of NFTs and not enough research exists on the specific ways in which wash-trading actually takes place in this context. I would like to ask @f13r whether they know if any of the quantitative work is based on ethnographic analyses on how this behavior actually takes place and if such ethnographic studies have been specifically conducted on some of the communities behind the NFT projects and marketplaces chosen for the current study. If not, then that presents a very important methodological hurdle because the current methods of quantification might be failing to account for real behavior in NFT communities. NFT communities in the context of degen culture have unique coalition dynamics and collusion patterns and probably engage in wash-trading in ways that have not previously been considered. For example, there is no clear explanation as to why round-trip trades modeled as closed-loop graphs might be a suitable heuristic for identifying wash trades in NFT communities. If @f13r or any of the original authors would provide more in-depth reasoning, as to why this is it would be much appreciated.Folk-psychological Bias, Bad Faith and Views of Human NatureIf 2 is indeed the case, then this hints at a very interesting sociological implication. On the one hand, the default stance in web3 seems to be that fraudulent behavior is expected and doesn’t constitute that big of a problem since culturally, a lot of strain is placed on individual responsibility in terms of security, vigilance and risk. The default stance in terms of the question of human nature and behavior in a social setting seems to be one of acceptance of greed and opportunism. The design of blockchain validation systems in PoW systems that use greed and personal profit as incentives to maintain the integrity and truthfulness of transactions seems to presuppose such an underlying philosophical presupposition.Tactically building systems that utilize the bad faith and opportunism of users for producing outcomes that serve a greater good (from securing a network to reducing carbon footprint) is the underlying presupposition that also informs a great deal of innovation in Ethereum and other blockchains with smart contract functionality (Cosmos, Celo etc.). But if the conclusion of the paper is legitimate, it means that the pessimistic intuitions about human nature that inform the game-theoretic design of the infrastructure of web3 are based on a wrong assumption. This is big and cannot be overstated.It seems that because of the way bad faith behavior seems to occupy the forefront of the space’s attention and because of the inherent cognitive bias to overestimate the negative consequences of certain actions, our underlying perception of how humans act in the anarchic and permissionless environment of web3 is clouded.The possibility that such an interpretation of the conclusions is even possible means that the study cannot stop here. Having been trained in the history of science, simple implications such as this are often enough to append whole established ways of thinking and follow-up research will be needed to address the issues raised. The original researchers such as @f13r will need to address the possibility of methodological errors or replicate given an updated data set including additional sources and heuristics and if no statistically important discrepancy is observed then a follow-up to address the philosophical issues of morality and design based on the empirical evidence collected would be important.']"
                Research Summary: What is a DAO? Conceptual Foundations              ,https://www.smartcontractresearch.org/t/research-summary-what-is-a-dao-conceptual-foundations/1645,Governance and Coordination,124,['https://mirror.xyz/0x8B580433568E521ad351b92b98150c0C65ce69B7/1zGqbsh1YZNi3I9yvtk_2VMcpyg_dvHF1GlZ_LAO3p4'],"['dao', 'summary', 'governance']","['TLDR:DAOs have become a popular concept, albeit with divergent and often conflicting definitions.To address this concern, the authors reviewed the pre-history and early history of DAOs to frame central themes in understanding DAOs. We then identified five conceptual lenses that allow us to conceptualise the telos of DAOs;conducted a study with 155 participants to identify the key ‘qualities’ associated with DAOs;and arrived at a definition of DAOs that satisfies our research purposes.Core Research QuestionHow can we conceptualise DAOs to map the common paths towards becoming a DAO and the problems that block that progress?CitationOspina, D. and Bohle Carbonell, K. (2022). What is a DAO? Conceptual Foundations. Mirror. What is a DAO? Conceptual Foundations — RnDAO 53SummaryDAOs have become a meme, one used to describe everything and nothing. We focus our research on elucidating the telos of DAOsMultiple ideas associated with DAOs have been present in narratives that preceded them.DAOs were originally conceived as non-humane organisations, but influential definitions of the early 2010s included a human element and emphasised censorship resistance. The definition has continued to evolve and diverge since.Across different definitions, DAOs have come to be associated with a broader exploration of coordination between (autonomous) agents who seek to satisfy certain needs and aspirations.Fluid boundaries mean DAOs fit poorly within the definition of Organisations, but a newer concept, Organisationality, proves more appropriate. Organisationality frames three key criteria: characterised by interconnected instances of decision-making, these instances of decision-making are attributed to a collective entity or actor, and collective identity is accomplished through speech acts that aim to delineate what the entity or actor is or does.There’s a set of ideas and values that further qualify DAOs; we’ll refer to these as the Ethos of DAOs.DAOs exist both as entities at a specific point in time and as a constant process of change. DAO are both entity and process.This dual nature (entity and process) poses a complication to conceptualising DAOs.By framing DAOs as Communication, we can study the patterns of communication in a collective and elucidate both the entity and process dimensions.DAOs, as communication networks, are subject to the principles of complex systems. Ethos directly shapes the emergent patterns and structures that we see in DAOs.A Pol.is survey resulted in 4 statements about what characterises DAOs (Ethos of DAOs) with 64% or more participants agreeing:Decentralised power: no single source of authority.Autonomous: self-sovereign, not bound to an external coercive force.A common goal, vision or set of values that are (being) worked towards.A shared treasury controlled by a decentralised voting mechanism.We conclude that DAOs are collectives that exhibit organisationality, expressed and evolved through communication events and processes, and shaped by an Ethos that highlights the 4 qualities mentioned above.MethodWe conducted a literature review, facilitated workshops with 15 participants (including inviting 4 professors to input into the conceptual development), and conducted a Pol.is Survey.The survey was distributed through our networks on Twitter and LinkedIn as well as 20 other DAO-related communities. 155 participants provided 31 statements and cast 1,829 votes to the question ""What makes a DAO a DAO?”.ResultsA majority grouping (97 participants) converged on 5 key statements with 64% or more agreement (2 of said statements we identified as permutations of the same concept and condensed into 1).See Pol.is report here 3And full paper here 53Discussion and Key TakeawaysWe conclude that while DAOs are not organisations (in a traditional sense), they are collectives exhibiting some degree of organisationality, enacted through communication events and processes.In addition to organisationality, we frame DAOs a both static entities while simultaneously evolving and striving to uphold certain values (an Ethos). Conceptualising DAOs as communication networks enables us to resolve the tension between the current entity and its evolution (between what is and what is yet to be).Finally, we identified a generally shared Ethos of DAOs that highlights decentralisation of power, autonomy, a common goals, vision or set of values that are (being) worked towards, and a shared treasury controlled by a decentralised mechanism.Implications and Follow-upsThis study provides a conceptual foundation to discuss DAOs and bridge previous research on organisational studies and related fields.Further work is needed to delineate the limitations of the proposed conceptualisation of DAOs when it comes to applying insights from previous research that didn’t take into account the conceptual foundations (and distinctions) hereby identified.ApplicabilityThis work is currently being applied to create a tool to map, assess, and plan DAOs. Additionally, this research is being referenced to conceptualise and develop assessments of DAO Community Health.Other applications are yet to be explored, and we’re happy to engage and collaborate on their development (contact us through @rndao__ 2)', 'SourceCred is a DAO & an open-source community tool to measure & reward value creation. Can a DAO die? if yes what can result to the death of a DAO? if no then what happened to sourcecred?', 'Interesting question. From the definition we have, I guess that as long as there’s communication happening, decisions are being made and said decisions/acts are attributed to SourceCred, we still have some degree of organisationality, so technically SourceCred is still ‘alive’ as an organisational collective. Then, whether the above aligns with the Ethos of DAOs (or how much) will determine its degree of DAOness', ' kingdamieth:Can a DAO die? if yes what can result to the death of a DAO? if no then what happened to sourcecred?Wait, what happened to SourceCred? It doesn’t seem dead.', 'Thank you @danielo for this timely summary.We recently discussed DAOs and legal wrapping on the chat so your research summary is just in time. Following your takeaway that DAOs are not organisations in the traditional sense, do you have any thoughts on DAOs attempting to gain legal personhood?Do you think the attempt to give legal personality to a DAO still allows it to remain a DAO, especially in line with the principle of decentralisation in the DAO ethos your survey posits?Also, for DAOs that are incorporated, sometimes, the incorporation tends to disenfranchise some of the DAO members who are outside the jurisdiction of the regulatory authority. Would you agree that this is an infringement of DAO ethos?', 'I clearly understand you and thanks for your conclusion with this danielo:Then, whether the above aligns with the Ethos of DAOs (or how much) will determine its degree of DAOnessI think their level of organisationality is Low compared to the past and looking at this from my own perspective, it might affect the level of technicality of their product.', 'It’s not dead but the ‘organisation’ stopped operating in the way it was (no further attempts to pay contributors, anyone who contributes does so as part of an open-source project) and they’re maintaining Twitter and doing some minimal discord moderation.You could argue that it’s still a DAO except for the ‘shared treasury controlled by a decentralised voting mechanism’ part of the Ethos', 'IMO, ideally the whole legal wrappers can be avoided/reinvented, as whether DAO-Ethos-aligned or not, nation-state law is a problematic legacy system. In practice, each individual and collective will have to negotiate their way through it.I see legal wrappers as a necessary evil for limited liability (given the legal uncertainties of the industry) but in an ideal world we can stop giving personhood to legal entities (it is suicidal that fictional entities can have rights like humans and more rights than say animals or the biosphere at large).Also, since DAOs are born global, it’s weird to attach them to the local (a nation state). But again, we haven’t got the right structures in place here.A legal wrapper does put the DAO under the authority of a Nation State (it’s debatable whether without one the DAO is still controlled by a nation state or several as the individuals still are exposed to it/them.) So the Autonomous part is put into question here.Then, if the legal wrapper allows for the decentralisation of power (eg permissionless joining, decentralised governance, etc.) it might not be fully anti-DAO. So we can still try to apply (some) DAO ideals to it.But then, for me, the goal is not to be more DAO, the goal is better human coordination. As long as we don’t call everything that’s not a DAO a DAO, then we can still have a shared language to advance the discussion. And that’s the contribution this piece aims to make', 'I would caution against defining that too black and white. Indeed most DAOs are quite chaotic but they’re also very early stage experiments on a new form of coordination. And six months or 2 years from now we could be looking at a very different picture.Already there are some more ‘organised’ outliers', "" Mr.Nobody: kingdamieth:Can a DAO die? if yes what can result to the death of a DAO? if no then what happened to sourcecred?Wait, what happened to SourceCred? It doesn’t seem dead.As a long-time SourceCred (SC) contributor dedicated to keeping the dream alive, my response here is biased, but hopefully useful in testing some of the conceptualizations presented. danielo:Interesting question. From the definition we have, I guess that as long as there’s communication happening, decisions are being made and said decisions/acts are attributed to SourceCred, we still have some degree of organisationality, so technically SourceCred is still ‘alive’ as an organisational collective.This definition feels validating, as it articulates a vague sense I’ve had since SourceCred (the organization) began winding down a couple months ago. While the number of contributors has declined, there still exists a core group that have continued to meet and make decisions (e.g. the community call is still happening every Wed), engaging on Twitter (including Twitter spaces), participating in convos on discords, forum threads like this one, etc.There are also presumably lots of conversations about SourceCred I’m not aware of. SC has ‘broken through’ as a ‘brand’, reaching relatively broad awareness. I’ve heard stories of people knowing about SC in non-crypto/web3 OSS projects, non-tech people, etc. It’s also being actively used by a decent number of projects. Some of which are deeply invested at this point. For instance, MetaGame, the first community to integrate SourceCred, has expressed interest in being ‘one of the SourceCred stewards’.image1192×212 53.9 KBI should also add here that the SourceCred community decided to release the SourceCred trademark to the public nearly a year ago, in the aftermath of an unsuccessful hard fork. And that the code is MIT licenced. So there doesn’t appear to be a legal barrier to ‘being’ SourceCred (I am not a lawyer this is not legal advice).image1462×830 77.2 KB danielo:It’s not dead but the ‘organisation’ stopped operating in the way it was (no further attempts to pay contributors, anyone who contributes does so as part of an open-source project) and they’re maintaining Twitter and doing some minimal discord moderation.You could argue that it’s still a DAO except for the ‘shared treasury controlled by a decentralised voting mechanism’ part of the EthosBasically agree, but if we zoom out, things get more complicated? For one, a number of projects are still distributing meaningful amounts of money using SourceCred. And SourceCred contributors can earn rewards in those instances, often by supporting said SC instances. For instance, I’m earning from at least five instances: Maker, MetaGame, Giveth, Token Engineering Commons (TEC), and (if eligible) SCRF.SCRF is thinking about potentially increasing payouts 2. Would a core SC contributor (me) writing a long post (this one) increase SCRF’s confidence in their ability to maintain their SC instance, leading to a greater probability of increased payouts?  I’m not writing this post for the money (at least consciously). But feasibly I am coordinating with organizationality with other SC contributors around SCRF (talked about SCRF at last SC community call, have RT’d SCRF Tweets about SC, etc.). May RT this post from the SC Twitter if people are cool with it.Additionally, on a purely technical level, likes on this post will increase my Cred score, giving me more DAI and giving my likes greater influence in the Cred graph.image1340×552 58.1 KBSo technically, I am directing some non-zero amount of SCRF’s budget with my words  It’s a stretch, but if we expand our definition of ‘shared treasury’ to all SC budgets in the ecosystem, and ‘decentralized voting mechanism’ to likes (each post is a mini election), SC meets this requirement in a diffuse, fuzzy way? kingdamieth:I think their level of organisationality is Low compared to the past and looking at this from my own perspective, it might affect the level of technicality of their product.Agreed. We’ve got a pool of devs that have contributed in the past, some knowledgable devs supporting instances, and our recently departed lead dev is around to review the occasional PR. But active development is halted for now. I think this is OK for the near to medium term, as the core components (CredRank + main plugins) are relatively mature and hardened. Which I go into in my recent talk at MetaFest (which for disclosure I earned Cred/XP → SEED). The basic functionality of the product hasn’t changed really in ~2 yrs. In that time numerous projects have battle tested a good chunk of the solution space, found bugs, built features, etc. I think it’s relatively low risk to explore if using existing plugins and not building novel functionality or radically new use cases. However all codebases do require at bare minimum basic maintenance over time, in particular the data fetchers calling external APIs. We shall see I will throw out an interesting possibility: that SourceCred is already a DAO, in the Bitcoin sense. A decentralized protocol. While Bitcoin gave censorship-resistant value transfer, SC gives (if governed correctly) censorship-resistant value reward. Monetary sovereignty → Valuation sovereignty. This is actually how I’ve viewed SC since the beginning, and perhaps why my view has often diverged. Where others see a failing SaaS startup and leave, I see a hardening protocol giving me increasing SC income, expanding possibilities for projects needing stability. If curious, below is an early more creative expression of that vision, where I explore SC as a 'Community-Store-of-Value (CSoV).SourceCred – 18 Jul 19SourceCred as Store-of-Community Value (SoCV) 1SourceCred is, in a literal sense, Proof-of-Work (POW). What would happen if we just applied the Bitcoin blockchain algorithm (proven game theory) to SourceCred, swapping out mining for contributions? A thought exercise.  Bitcoin miners get paid...Reading time: 5 mins 🕑Likes: 6 ❤I realize this may be a stretch conceptually, but AI (and PageRank is considered pseudo-AI by some), has a way of reifying concepts  And the possibility of an anti-fragile income via SC is very motivating for me.DALL·E_scrf_meme1024×1024 147 KBDALLE-2 ‘group of researchers sitting in a circle watching a field of possibilities erupting from black hole’ + SC logoPhew! Ok sorry for long thread, lot to say on this apparently Really liking the proposed DAO conceptualization as well. Resonates with my experience in DAOs and feels like the most useful definition I’ve seen to date."", 'I’d propose a distinction between the output of an organisation/ organisational collective (e.g. the SourceCred protocol/product) and the collective itself. e.g. a hammer can keep existing even if the group that manufactured hammers dissipates. And hammer owners can keep talking about hammers and using hammers but that doesn’t imply a Hammers DAO exists.So while SCRF, MetaGame, etc. can continue using the SourceCred protocol, the SourceCred org could be alive or dead.In this case, to determine whether SOurceCRed is still a DAO, one key criterion of the organisational collective definition is the shared instances of decision making (and the attribution of said instances to the collective entity).Applying this, SCRF using SourceCred does not entail any collective decision of SourceCred users at large, nor is the way SCRF is using the protocol attributed as a decision taken by SourceCred itself.I understand there is still a Github repo with centralised control i.e. one or a few people decide whether to merge open source contributions (not certain this is the case but for the sake of argument I’m running with it).If decisions are made that affect the collective (i.e. the code used by the different SourceCred users), then there are collective instances of decision making, which would validate SourceCred as an Organisational Collective.But is it a DAO?If there’s an aspiration to decentralise power in managing the github repo, then we could probably say that SourceCred has some degree of DAOness (decentralised contributions and an aspiration to decentralise approval of which ones get made “official”). But if the ability to make contributions to the centralised repo ceased and there was no intention to decentralise power, then it wouldn’t be a DAO.', ' danielo:I understand there is still a Github repo with centralised control i.e. one or a few people decide whether to merge open source contributions (not certain this is the case but for the sake of argument I’m running with it).There are I believe like a half dozen owners of the repo. Can’t verify, as GitHub doesn’t make that info available to regular contributors like myself. But iirc it’s basically the main devs over the course of the project. danielo:If there’s an aspiration to decentralise power in managing the github repo, then we could probably say that SourceCred has some degree of DAOness (decentralised contributions and an aspiration to decentralise approval of which ones get made “official”).A couple months(ish) back, when Thena was making their exit, I made it a point to raise this issue in a governance call (who controls GH access), and pushed for rough consensus. Note: there were more formal governance mechanisms in place, but their legitimacy was arguably questionable at that point, and in any case we lacked the bandwidth for formal governance. Rough consensus was the most legitimacy we were going to get. Thena agreed to give ownership to whoever the community decided on. I raised hammad (METADREAMER) as the only candidate I could think of with the ability and potential interest. Those on the call agreed he would be acceptable. Thena made METADREAMER an owner on the call. We discussed how this could enable a ‘coup’ scenario, where any one of the owners could revoke permissions of the other owners. We agreed that wasn’t ideal, but that it was better to decentralize as much as possible. So arguably SC has aspirations to decentralize power. If not the means. Indeed, all formal governance has ceased for now.I should note that last I heard METADREAMER is working on MetaCred 1, which broke off from SourceCred nearly a year ago to pursue its own, more crypto-native vision of the product. But these days appears to just be integrating SC output with Coordinape and other reputation systems to create a “pattern language”. Note: I’m not a part of this project (which is not public) and this may be out of date  .There are also 122 forks of /sourcecred. While there may or may not be any forks being actively developed, the ability to fork definitely shapes the decision landscape.Could someone fork SC and hire devs in Bangladesh on Upwork to update the data scrapers and patch the occasional security fix for a couple grand a year? For what it’s worth, I also think non-code contributions are important here. The ability to change the code is the clearest ‘line in the sand’. Without that it’s hard to argue SC can “manufacture the hammers”. However, decisions around comms are impactful as well. For instance, what @sourcecred tweets to its 3,621 followers. Or the decision I’m making now to share a bunch of knowledge and information in this post. To what extent are comms deciding how people spend their valuable attention? Perhaps I’m delusional here (entirely possible), but on a personal note, my decisions and those of others in the community feel impactful.image955×934 85.8 KB danielo:Applying this, SCRF using SourceCred does not entail any collective decision of SourceCred users at large, nor is the way SCRF is using the protocol attributed as a decision taken by SourceCred itself.Agreed. What SCRF decides  is its own decision. I’m writing this post in part to give it as much information as possible, so that SCRF and others can make informed decisions around SC.I will add that ‘attribution’ is a very blurry concept for me. Presumably scientists are better at this, but observing narratives emerge around SC over the last several months, many seem more about desired (or feared) futures than an objective account of causality. E.g. seeing some leadership attribution bias probably.Right now, I would say SC organizationality is likely low enough to not meet this definition of a DAO. Though we could see something with more organizationality emerge ', 'DAOs were executed to be trustless and for a period being, They worked effectively without need of trust, hence bringing up real issues about the worthiness of current administration hypotheses. The limitations of DAO’s doesn’t discredit trustless associations/organizations. What are the difficulties that should be tackled assuming trustless associations are to succeed? While The DAO’s administration might have fizzled, Do we predict the chance of “CAOs” that are represented by centralized, hierarchically-organized smart contracts?', 'I have a piece with a colleague on “lifecycles” of DAOs here (including resurrection): The Lifecycle of a DAO: Inside a Cultural Phenomenon 2', 'And DAOs :) feedback welcome: DAO Design Patterns:. Components that constitute… | by Kelsie Nabben | BlockScience | Medium', 'Thank you for sharing this interesting article.I love the explanation about the cultural aspect of “gm” and what it represents.It also provides a great answer to @kingdamieth’s question on whether DAOs can die. DAOs die when their mission comes to an end or they have fulfilled their mission.', 'It’s very interesting to read different perspective of participants objective views on what DAOs are. Invariably, all these still corroborate some key characteristics of a DAO.Would you like to elucidate how the “qualifies” were generated? Was is from body of knowledge or from individuals?', ' danielo:But then, for me, the goal is not to be more DAO, the goal is better human coordination. As long as we don’t call everything that’s not a DAO a DAO, then we can still have a shared language to advance the discussion. And that’s the contribution this piece aims to makeAbsolutely, this speaks to Owocki posture on “it’s all about coordination challenge”The “livingness” is fuelled by emmergent factors of a complx system which is a characteristics of a DAO.', 'you can find the full article (including the explanation of the methodology) following the link above.TLDR, we surveyed 150+ individuals who voted on statements and could also add statements for other participants to vote on.', 'Hi, thank you so much for this all-round explanation on What is a DAO!It’s a new idea for me to see DAO as a process. You mentioned that DAO could also be considered as a process. If we take the assumption that any organization with a group people is constantly evolving with people coming and doing things (thus a process), I wonder what aspect of the DAO points this out?If this is a process, then there are series of actions and steps, do we have a clear set of what these steps and actions are, like proposals and transactions, or it’s border than that.']"
"                Research Summary: Ethereum Name Service: the Good, the Bad, and the Ugly              ",https://www.smartcontractresearch.org/t/research-summary-ethereum-name-service-the-good-the-bad-and-the-ugly/1587,Mechanism Design and Game Theory,64,['https://arxiv.org/pdf/2104.05185.pdf'],['summary'],"['TLDRTraditional domain names are controlled by centralised entities and possess several design flaws that make them less secure.Although the Ethereum Name Service (ENS) offers blockchain based alternatives to traditional domain names, this study is the first targeted research on the service.This large-scale study reveals that ENS is indeed popular with unique uses but still possesses a number of security risks that should be addressed by the community.Core Research QuestionWhat is the adoption level of ENS in the community and in what ways is it being used? What underlying security issues prevail in the system?CitationXia, P., Wang, H., Yu, Z., Liu, X., Luo, X., & Xu, G. (2021). Ethereum name service: the good, the bad, and the ugly. arXiv preprint arXiv:2104.05185.arxiv.org2104.05185.pdf 14.58 MBBackgroundDomain Name: A domain name for a website can be likened to the address of a house. Domain names allow a website to be easily found and accessed by users and networks on the Internet. Examples include google.com and medium.com. Domain names were introduced as a user-friendly replacement to the Internet Protocol (IP) address.Domain Name System (DNS): A network protocol that helps map domain names with network information such as IP addresses. Mapping is operated in a hierarchical manner with top-level domains like .com, .org, .edu, and several others.Top-level domains (TLDs): The last part of a domain name represented after the dot. In the domain name google.com, .com is the TLD. They are managed by organisations such as the Internet Corporation for Assigned Names and Numbers (ICANN).Second level domains (2LDs): The part of a domain name that comes before the TLD. In google.com, google is the 2LD.Zooko’s Triangle: This triangle proposed by Zooko Wilcox-O’Hearn states three essential properties an ideal name system should possess. These include that a name system should be human-meaningful, secure, and decentralised. Zooko believed that a name system could not possess all three properties and must compromise on a third.Blockchain Name Service (BNS): A blockchain based alternative to DNS that claims to be a solution to Zooko’s triangle. Early BNS models like Namecoin and Handshake seek to totally replace the traditional DNS.Ethereum Name Service (ENS):  A BNS model built on the Ethereum blockchain. ENS uses smart contracts to perform the function of traditional domain name registrars and map human readable names to machine-readable identifiers.Resolver: A kind of smart contract that stores the mapping of domain names to records and is responsible for the actual process of translating names into Ethereum addresses. These records may also include content hashes and text records.Registry: A single smart contract used to maintain a list of all the domains and subdomains on ENS. In this contract, three pieces of information about each of these domains are stored: the owner of the domain, the resolver for the domain, and the caching time-to-live for all records under the domain.Registrar: Another smart contract on ENS that owns a name, and can automatically assign subdomain names to other users based on certain rules and conditions.Labelhash: The output of the keccak-256 function applied to a label. A label is a single part of a domain name e.g In the bob.eth domain name, the labels would be bob or eth.Namehash: A function that creates a fixed length 256-bit cryptographic hash for any complete human-readable domain name on ENS e.g bob.eth. The output of this function is referred to as a node hash (not to be confused with node client). ENS works with these node hashes to uniquely identify domain names on the system instead of human-readable names.Content Hash: A unique hash identifier for content which can be obtained when files are uploaded to storage systems like the InterPlanetary File System (IPFS). ENS introduced this field for revolver contracts to map content to a specific domain name.Text Record: Managed by the resolver, this field is used to attach any random data to an ENS name. This may include email addresses, URLs, social media profiles, description of the name owner, and any other metadata the user wishes to add.Event Logs: Used to describe an event within a smart contract. In the case of ENS, this could be name registration, name expiry, subdomain name creation, and a variety of other activities carried out by the ENS smart contracts.Geth: One of the three original implementations of the Ethereum protocol used for implementing a node. It is written in Go, fully open source and licensed.Alexa: Top website popularity ranking site owned by Amazon. To be shut down by 1 May 2022.Application Binary Interface (ABI): An ABI is a contract between pieces of binary code. It defines how functions would be called and how these unrelated code must work together.Vickrey Auction: A type of auction in which the highest bidder pays the second highest bid for the item auctioned. It is also known as a sealed-bid second-price auction (SBSPA) because the bidders submit written bids and the bids of others are not shared publicly.Permanent Registrar: Deals with the registration and annual rent payment of names over 6 characters in length. Was put in place on May 4th 2019.Short Name Claim: The period in July 2019 when ENS allowed the reservation of .eth names with the length of 3 - 6 by persons who possessed eligible equivalents in the traditional DNS system.Short Name Auction: The auction process conducted on OpenSea in September 2019 where ENS sold names using an English auction. The winning bid was the registration fee for the first year of the lifespan of the domain name.The Great Renewal: The renewal period in August 2020 for all names registered during the Vickrey auction period.Domain Squatting: Purchasing a domain name, usually a popular or generic one, to prevent others including the rightful owners from purchasing or profiting from its use.Record Persistence: The maintenance of ENS name and subdomain name records even after the expiry of the name itself.SummaryWith its strength in immutability and decentralisation, blockchain technology has been applied to improve the traditional DNS. Some of these BNS solutions like Handshake, Namecoin, EmerDNS and UnstoppableDomains aim to totally replace DNS.ENS, a BNS built on the Ethereum blockchain, is different because it seeks to complement and not replace the traditional DNS with its features. At the time of the writing of the paper, ENS had clocked four years since its launch but had no significant research done on its benefits, use, and security risks. This study aims to correct this.ENS was launched in March 2017 but was shut down after two malfunctions were discovered in the code. It was relaunched in May that same year with 192,471 registered in the first 7 months using a Vickrey auction.Darkmarket.eth was the most valuable name with a price of over 20,000ETH at the time. The winning bidder got the name at the second highest bid price while the losers would receive a refund of 0.5% less than their original bid deposits.The Vickrey auction was replaced in 2019 with the permanent registrar and the registrar controller. Annual renewal fees for names with more than 6 characters started at $5 every year. The registrar controller introduced the possibility of delegating name management to another Ethereum address.The short name claim period in July 2019 gave owners of traditional TLDs and 2LDs the opportunity to pay advance rent for ENS name equivalents or variants. Famous traditional websites like NBA and Ebay applied for .eth names during this period. Prices for names were adjusted to $640 for 3 character names, $160 for 4 character names, and $5 for 5 - 6 character names.Names relating to popular brands like ‘apple’ and ‘google’ and names relating to terms like ‘sex’ and ‘porn’ received much attention during the short name auction on OpenSea in Septermber 2019.Today, ENS has evolved and now shares similarities with traditional domain names. Prices for name registration and annual renewal are now dependent on the length of the name. Anyone can renew a name during the 90-day grace period after its expiration.The most widespread use of ENS has been proven to be as an alternative to blockchain addresses. Other uses include to set content hashes, public key records, descriptions and text records.There are obvious signs of domain squatting. These include explicit squatting by claiming names of known brands and typo-squatting.Bad actors are also exploiting ENS for illegal and malicious purposes like linking to gambling  websites, adult content, and scam activities.Record persistence attacks also pose more security risks to users. ENS maintains records on names even after their expiry dates.ENS shows a promising future but still needs to be properly studied and monitored.MethodThe authors employed a thorough tripartite quantitative approach to collect the primary dataset used for the research.The first step was to collect from Etherscan all ENS official smart contracts related to name registration and name renewal which are the core functions of ENS. These contracts include registry contracts, resolver contracts, registrar contracts, registrar controller contracts and a short name claim contract.Then, Geth was used to synchronise the Ethereum ledger and extract event logs. Each contract’s ABI was fetched and used to decode the event logs. Through this, it was possible to get name-owner mappings, name resolver mappings from registry contracts, name records history from resolver contracts, and auction/registration history from registrar contracts. Extra open source revolvers were added with their event logs fetched and decoded based on their ABIs.Last, the hash values of the ENS names were restored to readable names. This was done by first accessing the name-hash dictionary of ENS developers uploaded on Dune Analytics. Then the labelhashes of over 460,000 English words and Alexa’s top 100,000 domain list were compared to the hashes in the registry event logs to obtain their readable values.Non-ETH addresses, content hashes, and text records were also decoded based on the rules in their ENS documentations; EIP-2304, EIP-1577, and EIP-634.Data on the Short Name Auction was obtained by analysing the data shared by OpenSea in the ENS blog. This was because the auction was conducted on OpenSea and ENS contracts’ event logs did not have details of the auction.As arbitrary text records are set in the form of key-value records with predefined keys, an analysis was performed on the keys of these text records without the empty values to obtain their content.To check for explicit squatting, the labelhashes of each 2LD in the Alexa list was matched with its corresponding labelhash in an ENS name. The test is whether an Ethereum address has more than one famous brand’s domain name; if so, it must be a squatter as these brands are not owned by the same person.To detect typosquatting, dnstwist, is used to generate typo-squatting variants of Alexa top 100,000 names. Dnstwist can create different typo-squatting variants through methods like addition, bitsquatting, homoglyphs, hyphenation, insertion, omission, repetition, replacement, transposition, vowel-swap, and various. All Alexa top 100,000 domains are imputed in dnstwist and the labelhashes of their 2LDs are calculated to check if the squatting names have been registered on ENS.To check the possibility of bad actors exploiting ENS functionalities to deliver malicious or illegal web contents, all URLs obtained from the text records and content hashes are first uploaded to VirusTotal. Then, Eyewitness is used to get the screenshots and source codes of these websites.This data is subsequently uploaded to Google Cloud Natural Language API and Vision API to check if the URLs contain censored content. Suspicious URLs are all manually inspected to reduce false positives.To check if blockchain addresses stored in ENS are used for malicious purposes, a scam address list is compiled from sources like Etherscan, Bloxy, BitcoinAbuse, and CryptoScam. The list is then matched for similarities to get results.ResultsBy employing the method above, the authors obtained ledger information up to block 10, 746, 639 (i.e., 2020-08-28 03:03:42 UTC) on Ethereum. Therefore, all results discussed were obtained in this time frame. A total of 2 million registry logs, 3.4 million registrar logs, 200 thousand resolver logs, and over 3, 000 transactions related to text records.Since the launch of ENS, 107,617 addresses have participated in the registration of 465, 827 ENS names. 183,000 of these names were still active at the time of the study. 2,254 traditional DNS names have also been integrated on ENS.Over 35% of active addresses own more than one ENS name. An address 0xbcbd4885ee8b2b74249c5ad9b8b668fb256a51b1 had registered up to 2,262 names including common words and names of famous brands.ENS names with more than 6 characters are more popular due to the reduced costs of purchase. 54% of active names are those with 5 - 8 characters.A total of 361,751 names were bid on during the Vickrey auction with 274,052 registered. 17,625 addresses took part in the auction with 45% of bids placed at 0.1ETH.7,670 names were sold for a total of 5,697ETH during the Short Name Auction. Decentraland went on ENS in February 2020. Over 12,000 subdomains were created from its own domain name.Users are taking advantage of the ENS feature of assigning records to a name. 140,000 names have set records over 170,000 times. Most records contain blockchain addresses with Ethereum being the most preferred at 114, 542 setting records. BTC comes closely after with other variations like LTC, BNB, XRP, and BCH.Content hashes records were discovered in 5,300 names with 98% of them set for IPFS and Swarm. Text records are mainly used to store URLs. 50% of these URLs are set to subdomains of OpenSea.Text records containing descriptions of the name, links to Twitter accounts, and customised key words were also found. There are 44 customised keywords in 214 record settings of ENS names.15,179 ENS .eth explicit squatting names controlled by 1,532 Ethereum addresses were found. The address topping the list of top 10 holders of these kinds of squatting names holds up to 933.85% of the 3,775 squatting names set to records were set to blockchain address records. Ethereum tops the list with OpenSea links and IPFS websites following closely. Some addresses have transferred their squatting names.18,483 ENS typo-squatting names have been identified. These names target 13,450 Alexa names. The most popular typo-squatting variant is bitsquatting with 5,000 variants found. More than 52% of these names were active at the time of the study.Addresses have been registering suspicious ENS squatting names since the initial Vickrey auction period. This trend maintains a steady rise and fall from then to the time of the study.Three scam addresses have been registered in the ENS system at the time of the study. These include airdrop scams and Ponzi schemes.19 malicious websites involved in gambling, adult content, and scam activities are found linked to ENS name records.ENS name records persist even after name expiration. 16,017 expired .eth names still have records within them alongside their 3,116 subdomains.Discussion and Key TakeawaysENS has built steady popularity since its launch in 2017.It is proving to be a complementary tool to the existing DNS service.ENS names are now being used for dWebs and traditional websites.It inherited the attributes of blockchain technology; immutability, transparency and decentralisation.Rare names and names with popular words are in high demand. Users are trying to get as many names as they can.The most common use of ENS name records is to link blockchain addresses. Other uses include to store content hashes, and text records.Several security issues like domain name squatting and malicious behaviours found in traditional DNS still plague the ENS system.New security issues posed by the use of smart contracts also exist. ENS names are highly prone to record persistence attacks. An attacker can renew a name after expiry and edit the records. Innocent buyers who are unaware of this change may still associate the .eth name with the old owner and use it in transactions.Given the fact that ENS names are ERC 721 tokens, could their acquisitions be based on their market profit in the future?What are the privacy implications for the user of this service? ENS allows users to link blockchain addresses and custom records to human readable names. If a user publishes this human readable name on a platform like Twitter, it could make way for third party surveillance of the user’s address activity and balance.Implications and Follow-upsIt can be inferred from its active users and integrations with dApps and traditional TLDs that ENS has a healthy ecosystem.There need to be new solutions to enhance the security of ENS due to its inherent security risks.Users need to cross-check the addresses under an ENS name before approving any transaction or interaction with them.The ENS team aims to scale the service on Layer 2 and is working towards more integrations with traditional 2LDs. This will help reduce costs and facilitate use.The authors acknowledge that there have been several studies on the designs of BNSs. Hari et al. propose a distributed, tamper-resistant DNS infrastructure as a solution to the limitations of traditional DNS and its dependence on Public Key Infrastructures (PKIs).Guan et al. present a domain authentication scheme, AuthLegder,  to reduce trust in certificate authorities. Other studies like He et al. seek to put forward discussions on how to improve the security of DNS nodes. He et al puts forward a novel decentralised DNS root management architecture based on a permissioned blockchain.Gourley et al. is also cited for their proposal of an improved DNSSEC based on blockchain.Other works relating to the analysis of BNS systems are also cited. The empirical analysis by Kalonder et al. on Namecoin is mentioned. Works that border on the properties of BNS systems like that of Patsakis et al. are also mentioned. Patsakis et al. analyzes security threats to BNS systems such as malware, underlying registrar mechanism, domain market, phishing, motivation and immutability.Liu et al. and Karaarslan et al. compare the designs of several blockchain-based DNSs including ENS.The authors note that there has been no worthy mention of systematic study of ENS besides their work.ApplicabilityThe methods used by the researchers can be employed by the community and ENS developers to conduct more research and improve the system.DApp and Blockchain wallet providers who integrate ENS functionalities should apply the methods in this research paper to detect these security issues and warn users.Methods in this work can be used to study other BNS solutions.', 'Thanks so much for coming to the forum and contributing this – what do you make of ENS’ recent governance struggles 2 and have they recovered from them?', 'Thank you for your question @jmcgirk. The recent situation around the proposal to remove Brantly Millegan as Director of the ENS Foundation was just a clear cut proof of DAOs doing exactly what they were made to do: decentralizing decision making. The majority voted against the proposal. This majority turned out to be Brantly himself who had attained so much voting power because others delegated their ENS tokens to him; a perfectly legitimate occurrence.While the proposal to oust him was being voted on, a subsequent proposal to choose a new Director was also held. The results of this new choice would only be valid if the proposal to oust Brantly received a majority yes vote, which it did not. Currently, Brantly still remains the Director of the ENS Foundation though he is no longer a steward of the DAO or a part of True Names Ltd. All thanks to decentralization.Here’s a link to the two proposals on Snapshot.[EP6.1] [Social] Removal of Brantly Millegan as Director of The ENS Foundation[EP6.2] [Social] Election of a new Director of The ENS FoundationHere’s a link to Brantley’s response to the outcome of the vote.Message to the community on the finalization of the EP6.1 vote', 'Thank you very much – and I agree, it’s definitely proof that decentralized decision-making is really making a difference!', 'Hey @Favvz nice summary.Reading this, I don’t get the sense that ENS or other BNS are particularly better than DNS. It’s like the same issues but on a blockchain. Do you think the same?Also I am curious what is actually the main use of ENS. For me, it is mainly people showing that they have a .eth on Twitter.', 'Could you describe a little more the similarity that you see between BNS and DNS as being the same issue?In some ways, I see where you are going, but doesn’t migrating to being on chain make this fundamentally different from scalability, centralization, and security perspectives?', 'Thank you @jyezie I actually agree with your first point. The authors agree too. The issues DNS have are quite heightened in BNS solutions because of the very same qualities that give blockchain technology its hype. I’m talking about immutability here and transparency. Everyone sees everything. Then, take the record persistence issue that was raised. On DNS, we knew it was alarming that someone could register your name if you were late with your domain name renewal. But on BNS solutions, your domain name as well as your metadata is at risk. You can unregister a name but only while you are still in control of it. And even though the name would be unregistered, the records still show that you owned the name during that time. It is never erased. There’s also more motivation for domain squatting seeing as .eth names are actually NFTs. People want to collect them.In response to your second question, yes, the use of .eth names on Twitter is quite a spectacle. I would insert a laughing emoji if I could. But we cannot deny that the first point of ENS names was to give blockchain addresses human readable names. If I might say, give people in web3 more readable identities. Just like how we now have domain names for the internet after the era of IP addresses. Anyone showing their .eth names on Twitter is just helping champion the cause.However, the main selling point I see with ENS and other BNS solutions is the quality of decentralisation. That elimination of intermediaries. You have full control. Asides being in control of your own domain name, you could also directly create, manage, and sell subdomain names to others. You could reclaim these subdomain names in case of any breach. That is a lot of independence. When you combine ENS and IPFS, the decentralised file storage system, censorship resistant websites actually seem possible.So as always with technology there’s always the good and the bad. It all depends on how we answer and act on these kind of questions.', 'Thank you for your question @zube.paul. The only similarity I identify between DNS and BNS solutions is the fact they are both made for domain name registration. Asides that, there’s really no other similarity. They approach this problem with different techniques and are exact opposites on the grounds of scalability, centralization, and security, just like you identified.', 'Since the traditional name service and DNS have been widely adopted by the developers and administrators worldwide, I wonder what angle of attack could allow ETH Name Service to disrupt the industry in the upcoming decade?', 'Can you give us a sense of the kind of applications we might see ENS being used for on L2?', 'I don’t think we can expect an immediate overhaul but it’s surely growing. At the time this study was conducted in 2021, the number of active names recorded was 183,162. Now, in less than a year, by its 5th anniversary, ENS has grown to over 1.12 million names registered on its platform with 503 integrations. These integrations have been with the likes of Coinbase, Trust Wallet, Brave, Metamask, Etherscan, Snapshot, Aragon, BanklessDAO, Balancer, Chainlink, Mirror, and Opera. More integrations are to come.In a world where web3 products go mainstream, I believe that services like ENS would be more sought after for seamless functionality.Here’s a link to an article on the next steps ENS plans for growth:1 Million names and the next milestone 1', 'Thank you @Favvz for this research summary. I have personally used ENS and I find this study very interesting. This is the only in depth research paper I have seen about ENS as well.I will also try to answer @jyezie’s questions about main use cases. Although, most of it was covered in the OPs Key Takeaway section, I will try to expand on it and give some context to it.First to address the concern Favvz:Given the fact that ENS names are ERC 721 tokens, could their acquisitions be based on their market profit in the future?I think definitely yes. They play a part in the NFT speculation market that is going on right now. The following facts seem to support it. Some of these numbers are just mind boggling.The entire NFT Market Cap is currently at $12 Billion.The highest sale for an ENS is 420 wETH.The market participants also speculate on the ENS token - which is a governance token of the Ethereum Name Service. ENS currently trades at ~$10 with a market cap of ~$200M  While reaching an all time high of ~ $82 with a market cap of $1.6 Billion.This is also equally fueled by celebrities owning ENS and posting them on twitter among others. The current minimum price (floor) for 3 digit numerical ENS (Ex. 00x) is 14.5 ETH and the highest sale at 83.5 ETH for 003.eth.ENS in its peak also generated $1M a day in revenue.These numbers goes to show that there is definitely more monetary interest in ENS trading than in its’ use cases. The question that still remains is - How can we aim for global mass adoption with such squatting (as shown in the OP’s post) of ENS names? Would we have the same problems as we have currently in the regular DNS?ENS would continue to have value as long as the Ethereum L1 exists and continue to scale. To take it a step further, I think the same ENS problems/issues will also be in any other competing naming systems by other smart contract protocols.Having said that, I think ENS is a perfect way for blockchain address both for visualization and memory. Every application and use case built on Ethereum will want to represent themselves with a .eth domain be it on twitter or in any future decentralized social media.The next big use case after using it as a wallet, is representing content hashes on a decentralized file storage system to point to some text or metadata of some sort, this data can also be a website. By having some website on IPFS one can obtain a Content Identifier (CID) which is a sha-256 type hash and the ENS records can be updated to point to that CID and hence your website. As far as I am aware, the website cannot be altered without changing the CID.These websites needs to accessed through  web3 browsers like Metamask or Brave. To overcome the browser limitation, there is something called ETH DNS where normal browsers can be used to access the ENS domain with .link at the end of the ENS like -  https://vitalik.eth.link/ . Here is a reference if you want to dig deeper.I can also imagine some sort of identity related services using ENSs like voting in polls etc. Although I  am not sure how can that actually be implemented.One can also create and rent out subdomains of an organization. I can imagine something like KFC.eth to have a subdomain like NY.KFC.eth. Synonymous to DNS and emails.It is very exciting to see what applications and use cases unfold through ENSs as they currently seem to be limited as far as our imagination goes.  I would like to end this comment by posing a question to the community.  Will we have a universal decentralized domain naming system or is a multichain domain system the future? Given that .ENS now has Multi-coin support.', ""Well it’s a great write up I’d say @Favvz very  I’d also like to add some contributions. Well to me I’d say I prefer the DNS to the ENS and the BNS.Let’s start by discussing DNS. One of the pillars of the internet is the Domain Name System (DNS). It converts human readable language (domain) into numbers used by computers to communicate with one another. Without this systematic conversions, it’d be difficult for man to communicate with computer and get access to technology. When this happens, man encounters a tough situation because almost every element of modern life is impacted by technology, including food and healthcare availability, sociability, and productivity. Therefore, the computer actually uses the DNS we entered, such as Facebook website which is Facebook.com 1 when we visit a website in a browser to acquire a lengthy string of numbers known as an IP address. Without DNS, users would have to enter confusing IP addresses to access websites.Though the DNS system face the threat of getting hacked easily, just like we hear daily complains of Facebook accounts and Instagram.com accounts being hacked daily.images (1)960×320 13.3 KBWell talking about ENS we see some usefulness of course, but certainly not comparable to that of the DNS. The ENS performs the function of a domain name service. It is a name service built on Ethereum, not a naming service exclusive to Ethereum. It provides a safe, decentralized method of addressing resources with names that are human readable, which means you may decide to send ETH or ERC tokens to '“zubepaulsatoshi.eth” rather than sending to his wallet address. zube.paul:Could you describe a little more the similarity that you see between BNS and DNS as being the same issue?The similarity I see in both is that whether it’s an IP address or a crypto wallet address, ENS and DNS both reduce a complicated web address to a short, readable name. Your crypto address is condensed by ENS into your name.Instead of 0x1901456D21F57fF88205C980CBBC137b194p1e47c, you can actually use .ethDNS converts a website URL from something like 540.72.899.78 to www.scrf.com"", 'The fact that both DNS and BNS technologies are designed for domain name registration is the only similarity I can see between them. Other than that, there isn’t much overlap.As long as the Ethereum L1 is active and scales, ENS will be valuable. To go even further, I believe the same ENS difficulties will also arise in any rival name systems used by alternative smart contract protocols.In some respects, I understand where you’re coming from, and I agree that it’s unquestionably an evidence that decentralized decision-making is actually having an impact.', 'great work @Favvz, this summary was for sure needed .@Freakytainment your write up is quite explanatory, great work .got a question for you though.DNS are super important in our search for knowledge, communication and interaction as social beings because, without it, we would have ones, zeros and a couple of numbers to memorize, but DNS most certainly has its issues like regular leaks (IP addresses becoming visible to the public domain after they have taken steps to conceal the IP)the particular one that’s a source of concern to me is ‘Hijacking’ common with DNS systems.Is it possible for ENS to be hijacked due to their similarities in functions and features?There are malicious activities and scam websites, anyway to detect or mitigate such issues?', 'Well @GloriaOkoba the most frequent method of hacking a DNS is through an IP address attack using the widely used DDoS technique. If you wish to check whether or not your DNS has been hijacked, you can use the aforementioned program. A website user’s PC is infected by malware via a local DNS assault. You must avoid opening suspicious emails and installing software from unreliable sources because the malware, which is typically a trojan malware disguised as legitimate software, grants cybercriminals access to users’ network systems, allowing them to steal data and change DNS settings to direct users to malicious websites.  However, VPNs  can occasionally be useful because they can both encrypt and disguise your IP address.', 'That was really a nice summary there@Favvz.But, I would love to get something straight here, I was thinking that because the ENS was developed for Ethereum smart contracts  and is native to the Ethereum ecosystem – it doesn’t suffer from security issues faced by a DNS system. DNS records of domains and names are stored on a centralized server. That means they are prone to hacks unlike ENS that is stored on decentralized server.What’s your take on that.', 'Thank you @Favvz ,that was a great summary, from the research summary, I must say that ENS is becoming a promising framework on route to being complementary to DNS, however the spams and security issues might obstruct its encouraging, honestly this research summary is a plus to the BNS environment and commonsense experiences.', 'thanks for the helpful tip @Freakytainment', 'Yes ENS is native to the Ethereum ecosystem and it was built using smart contracts. And it does face some of the security issues faced by the DNS system such as domain name squatting. Not only does it face these issues, they are amplified as a result of the immutability and transparency of the network. I discussed these security issues in my comment here: Research Summary: Ethereum Name Service: the Good, the Bad, and the Ugly - #7 by Favvz']"
                SCRF Recommends              ,https://www.smartcontractresearch.org/t/scrf-recommends/1964,Community,44,[],['about'],"['Best Resources to learn about Web3In this thread, you will find some recommended papers, resources, and links on Web 3 to help you in your journey into Web 3.The SCRF Recommends thread is a valuable resource for people who are new to web3 and people who have lots of knowledge and experience in web3. There is something for everyone! You can find new articles/papers to refresh your memory about a subject or even introduce you to something new. In this thread, you can share papers, articles, and resources you find interesting with the community and help others discover them.How can you contribute to SCRF recommends?You can contribute to SCRF Recommends by responding to this post. You may follow this format:Hyperlinked titleIndicate category (Beginner or Advanced)About {brief introduction to the recommended paper/article/course, duration, etc}Name of author/organizer/ownerIf you need help or have any questions, you can ask @zube.paul or @TolulopeWhy should you contribute?The web3 community has brought together so many people globally. One thing that everyone has in common is the love for knowledge and the willingness to help each other. You should contribute to SCRF recommends for the following reasons;i. help people discover new topics and resourcesii. contribute to the growth of the web3 communityiii. You can earn some SourceCred by doing soiv. You can also discover new things while doing so.BeginnersHere, you will find links to online courses, white papers, and articles that provide an introductory overview of web3.Bitcoin: A Peer-to-Peer Electronic Cash System1The original Bitcoin whitepaper. This is your first stop.Satoshi NakamotoAn Introduction To Terminologies And Layers In Web31A deep dive into common words used in the web3 ecosystem. It serves as a glossary for newcomers into web3.Ruchika Gupta, Geekyants.Unit Master’s Program3A free six-week blockchain literacy program. Topics include Decentralization, Blockchain and incentive alignment, and Stakeholder capitalism and sustainability. This basic requires no prior knowledge or learning.Unithttps://www.web3.university/A community-driven platform offering free programming-focused blockchain courses. Students learn to create smart contracts and build NFTs. It is simplified enough for a person without a technical background to learn.Web3 Universityhttps://www.learnweb3.io/1Level-based free courses for programmers in web3. Courses begin from basics of web3 to building dApps to Security and Hacking. The courses progress from basic to intermediate to difficult, building on the lessons for each level.Learn web3https://www.linkedin.com/learning/what-is-web3Introduces web3 and the metaverse and blockchain and web3 basics. A simplified approach to web3.LinkedInEthereum Whitepaper | ethereum.org 1The original Ethereum whitepaper that accelerated non-financial blockchain use cases.Vitalik ButerinMOOC: Introduction to Digital Currencies.A free introductory course by the University of Nicosia. It introduces digital currencies and the need for them. No previous knowledge or learning is required to enroll in this course.University of NicosiaBitcoin and Cryptocurrency Technologiesfrom Princeton University, lectured by Joseph Bonneau, Ed Felten, Arvind Narayanan, and Andrew Miller, has been useful in providing an overview. The course was published around 2016, but the foundation it helps build are timeless.Vitalik Buterin’s websiteA blog by Vitalik Buterin that consists of several topics relating to Blockchain.*Bitcoin TalkA Bitcoin forum with discussions on Bitcoin and related issues.AdvancedHere, you will find resources that are of a more technical nature. They mostly require a previous understanding of web3, blockchain, DeFi or any other relevant concept.DeFiMOOC: Introduction to Decentralized Finance (DeFi). This course introduces concepts of DeFi and TradFi. A basic understanding of cryptocurrencies (like Bitcoin), Ethereum-based smart contracts and fundamental blockchain concepts is required.University of NicosiaToken Engineering Fundamentals 1The course teaches how to design crypto-economic systems from scratch and how to enhance token utility. Previous knowledge is required.TE AcademySmart ContractsSolidity DocsIntroduction to solidity, installing solidity compiler, and others. This is more suitable for people who understand smart contracts and how they work.https://cryptozombies.io/An interactive platform for learning basic concepts about smart contracts.CleverflareDecentralized Autonomous Organisations (DAOs)DAOs, DACs, DAs and More: An Incomplete Terminology Guide 1This paper explains concepts such as smart contracts, autonomous agents, decentralized applications, and decentralized organisations, among others. Knowledge of Blockchain and Decentralization is required.Vitalik ButerinDAOs - The New Coordination Frontier 1.A report curated by individuals from Gitcoin and BanklessDAO. It provides in-depth information and statistics about DAOs.Gitcoin and BanklessDAO.The DAO Landscape 1This article breaks down DAOs and explores the relationship between social and financial capital.CoopahtroopaCryptographyIntro to CryptographyThis course introduces cryptography and dives into discrete probability, stream ciphers, block ciphers, and message integrity among others. Watch on YouTube. Access the accompanying free textbook on applied cryptography here.Dan Boneh (Stanford University).Decentralized ThoughtsThe webpage consists of useful resources on Blockchains and Distributed Computing as well as cryptography.Useful Cryptographic resourcesA website on Cryptographic Engineering managed by Matthew Green, a cryptographer and professor at Johns Hopkins University. Start with his curation of valuable cryptography resources.Matthew GreenOracles and DataBlockScienceA firm that seeks to integrate academic-grade research with advanced mathematical and computational engineering. Check their website and navigate to the resources or blog page.What Is a Blockchain Oracle?This article explains oracles, the oracle problem, types of oracles, decentralized oracles, oracle reputation, and oracle use cases.ChainlinkPrivacy and SecuritySecureum BootcampA three-month bootcamp focusing on smart contract security and audits.Check their Twitter for discord and next cohort.Securem Web3 Security: Attack Types and Lessons Learned 1This article presents common themes and projections in security software trends to help people and businesses better guard their wallets and undertakings.Riyaz Faizullabhoy and Matt Gleason.ZK Whiteboard SessionsThe first few lectures are also by Prof. Dan Boneh, and they have been fun and helpful. Some interest in math is required to appreciate the course.ScalingAn Incomplete Guide to RollupsThis article explains how channels, plasmas and rollups work. Tradeoffs between two flavours of rollups and some yet-not-fully-solved challenges in rollups.Vitalik ButerinThe Complete Guide to RollupsA very long blog post with a whooping estimated read time of 83 minutes.Quote: Vitalik gave us the amazing Incomplete Guide to Rollups. I present to you The Complete Guide to Rollups. Ok it’s not actually complete, but it’s a great meme so I’m stealing it. This report only analyzes the design space of rollups on Ethereum and Celestia. I strongly recommend my recent Ethereum report for background.– Jon Charbonneau', '@Tolulope thank you for curating list.', '@Chrisarch if you haven’t seen this yet, it would probably interest you.Very helpful links @Tolulope.', 'Thank you @Ulysses for calling my attention to this amazing resource links by @Tolulope.', 'Thanks for the curation. Happy to share part of my reading list too:BeginnerBitcoin and Cryptocurrency Technologies 2 from Princeton University, lectured by Joseph Bonneau, Ed Felten, Arvind Narayanan, and Andrew Miller, has been useful in providing an overview. The course was published around 2016, but the foundation it helps build are timeless.On the deckI’m currently reading Decentralized Thoughts 2 to learn more about consensus and cryptography. Quoting from the blog: this page contains material for a graduate course on Blockchains and Distributed Computing with a dash of Cryptography.To learn ZK, I’m watching ZK Whiteboard Sessions 3. The first few lectures are also by Prof. Dan Boneh, and they have been fun and helpful. Some interest in math is required to appreciate the course.There are also other resources I think everyone should at least be aware of, such as Vitalik’s blog 2 and Bitcoin talk 1, and certainly many more I have missed.', 'Thank you @Twan for enlisting further links. Decentralised thoughts interests me and I will be taking steps to study the course. Be rest assured ,I will come back to you with some questions or clarification oncesi commence the studies.', 'I am happy you found this helpful @Samuel94, please feel free to add any resources you may know to the list.', 'You are welcome @Chrisarch, I hope they are helpful to you!', 'Thank you for sharing @Twan. Very helpful!', 'Thank you @Tolulope  for sharing this information. I have been looking forward to this especially the links for the online courses. It’s helpful to me.', '@Tolulope thank you for a well curated list.This is highly rich from beginner to advanced. Totally recommend.Will like to add:Scaling SolutionsAn Incomplete Guide to Rollups 3This article explains how channels, plasmas and roll ups work. Tradeoffs between two flavours of roll ups and some yet-not-fully-solved challenges in roll ups.Vitalik Buterin', 'Seeing you single out this post, I’d also add this to the list (just came out this month):The Complete Guide to Rollups 1A very long blog post with a whooping estimated read time of 83 minutes.Quote: Vitalik gave us the amazing Incomplete Guide to Rollups. I present to you The Complete Guide to Rollups. Ok it’s not actually complete, but it’s a great meme so I’m stealing it. This report only analyzes the design space of rollups on Ethereum and Celestia. I strongly recommend my recent Ethereum report for background.– Jon Charbonneau', 'Thanks to both @Twan and @Dansmage for their suggestions. These all seem great and worthy of inclusion!This seems correctly categorized as beginner, so that makes sense to add there. Twan:BeginnerBitcoin and Cryptocurrency Technologies  from Princeton University, lectured by Joseph Bonneau, Ed Felten, Arvind Narayanan, and Andrew Miller, has been useful in providing an overview. The course was published around 2016, but the foundation it helps build are timeless.Do we think that these two should also be Beginner Twan:There are also other resources I think everyone should at least be aware of, such as Vitalik’s blog  and Bitcoin talk , and certainly many more I have missed.For the following, would this make sense in Cryptography? Twan:I’m currently reading Decentralized Thoughts  to learn more about consensus and cryptography. Quoting from the blog: this page contains material for a graduate course on Blockchains and Distributed Computing with a dash of Cryptography.I definitely think that the ZK work makes sense in Privacy Twan:To learn ZK, I’m watching ZK Whiteboard Sessions . The first few lectures are also by Prof. Dan Boneh, and they have been fun and helpful. Some interest in math is required to appreciate the course.I also wonder if we should add a Scaling categorization to the original list to accommodate the rollup recommendations. Thoughts?', ' zube.paul:Do we think that these two should also be BeginnerThis was the first course that introduced me into Bitcoin and Blockchain. Even though it is technical to an extent, which is unavoidable, it played a good role in bringing me up to speed as a beginner. It focuses on the foundations of blockchains and cryptocurrency which I believe any beginner should start from for a solid foundation.', 'Thank you @Twan for adding this to the line up. The memetic theme of writing will make the reading fun, I suppose. Can’t wait to get up to speed on it.', 'Thanks for the curation', 'Thank you @zube.paul. I totally agree that we should add Scaling to the original list.', 'I hope you find them helpful @Lisayanky', 'Wow !!  Thanks a lot for the recommedations @Tolulope', 'Please feel free to incorporate from this A Collection of Blockchain Syllabi and Course Material 4']"
                Research Summary: On the Economic Design of Stablecoins              ,https://www.smartcontractresearch.org/t/research-summary-on-the-economic-design-of-stablecoins/817,Mechanism Design and Game Theory,75,['https://ssrn.com/abstract=3899499'],"['defi', 'summary']","['TLDR:Without reliable economic design, under stressful conditions stablecoins are at risk of entering a death spiral when they are unable to defend their peg and win back market confidence.This is especially severe if the stablecoins are only backed by themselves or other cryptocurrencies.While fiat-backed stablecoins with appropriate capital buffers have low volatility, only CBDCs have zero risk.Core Research QuestionAre the economic designs of existing stablecoins reliable? What are effective ways to avoid economic risks?CitationCatalini, Christian and de Gortari, Alonso, On the Economic Design of Stablecoins (August 5, 2021). Available at SSRN: On the Economic Design of Stablecoins by Christian Catalini, Alonso de Gortari :: SSRN 17 or http://dx.doi.org/10.2139/ssrn.3899499 3BackgroundWhen well designed, the technology behind stablecoins may lower the cost of payments and increase interoperability. This is because they are able to trade on par with a reference asset (usually the US dollar).Stablecoins differ from each other by the following qualities:economic designquality of the backing assetstability assumptionslegal protections for coin holdersThe economic design of stablecoins has two critical dimensions:The volatility of the reserve assets against the reference assetThe degree of risk exposure to a death spiralLack of robust economic design can undermine a stablecoin’s usefulness. In the worst cases, it can even turn into a threat to financial stability.High-quality, liquid assets perform better in stressed market conditions, making it easier for stablecoins to maintain their redeemability. This is especially true when combined with capital buffers.The volatility of stablecoins refers to how much reserve is needed to support the peg at any point in time.The current economic risks of stablecoins:Given the size of their issuing amounts, it is unclear how many stablecoins would be redeemable at par in a crisis (the top five USD stablecoins account for $110B of worth)Stablecoins that rely on investment tokens to back them suffer from this problem even more severely.If expectations change these stablecoins would enter a death spiral, making them worthless.Decentralized stablecoins rely on algorithms by design. They have a big challenge maintaining stability against a real dollar.Summary1 IntroductionAs seen in the previous section (Background).2 Key Design Choice: The Reserve Risk ProfileThe key challenge stablecoins face is to ensure they trade close to par with their reference asset.When a stablecoin’s price trades above par, it can restore the peg by issuing new coins.When the price trades below par, it needs to liquidate reserve assets and buy back coins.However, if the reserve assets have dropped in value after coins have been minted, the issuer cannot buy coins back and defend the peg.2.1 Volatility against the reference assetAt one extreme, the value of the reserve and the reference asset move in perfect synchrony. This kind of reserve has zero risks, and is only true for CDBCs, as the banks control both the digital assets and its reference assets.Though fiat-backed stablecoins have low volatility, they still need appropriate capital buffers to offset potential losses coming from credit risks, market risks, liquidity risks, and operational risks.At the other extreme, stablecoins are backed by non-fiat-like assets, such as risky types of commercial paper or cryptocurrencies. Their required amount of reserve assets for buying back a given amount of stablecoin supply will fluctuate dramatically over time. In this case, over-collateralization is expensive and cannot protect coin holders from extreme events.2.2 Self-fulfilling prophecies and stablecoin death spiralsDeath spirals are likely to occur whenever the value of a stablecoin’s reserve is tied to the future success of the stablecoin itself. This is true for stablecoins reserved by their investment coinsThey can fail even when initially over collateralized.Stress is sufficient to cause loss of confidence and more redemptions, which can then lead to death spirals even in a healthy market.During a death spiral, a stablecoin backed by its investment coins needs to be shut down. The reserve’s value will fall to zero easily and cannot be recovered.Features that are intended to stave off death spiralsSolution group 1Partially winding down the stablecoin as the likelihood of a death spiral increases.This is effective while the stablecoin is fully backed, and there will be no need to sell its investment coins.Solution group 2Stopping a run after it materializes.Has not been tested in the context of stablecoins, but likely to make matters worse based on historical evidence on bank runs in the United States.The risk of death spirals is inherently tied to the first design choice, which is the volatility of the reserve assets to the reference asset.3 ConclusionDecentralized stablecoins are either exposed to death spirals or capitally inefficient due to a lack of reliable on-chain, fiat-pegged assets.Decentralized protocols rely on centralized stablecoins as part of their reserve, and/or over-collateralization to maintain their market price at par. This shows that fiat-backed stablecoins may be the only way to ensure long-run stability.MethodThe paper first goes through the natures of stablecoins and arrives at its conclusion by logical reasoning and comparison with historical events in a particular case. Some explanations are supported by diagrams which can be found in the next section.Results2048×1357 209 KB1956×1156 166 KBThe authors include an appendix and Figure.5 to discuss why algorithmic stablecoins find it hard to defend their peg in a crisis by reducing stablecoin supply.Suppose the current price of the stablecoin is p < $1, and there is S amount of excess coins in circulationThe protocol needs to raise $(1+p)S/2 funds to burn S coins (Because the average price is (1+p)/2, and the quantity is S)The additional coins to be issued is Q, with P as the price of the investment token, P’ as the price of investment token of the Qth coin is sold, and (P+P’)Q/2 = (1+p)S/2During a death spiral, the amount of new investment tokens needed to be minted and coins to be burned rises dramatically. The cost of defending the peg quickly becomes infinitive.1394×671 122 KBImplications and Follow-upsAdequate reserve assets (high quality, liquid, embedded within a legal framework) opens stablecoins to an upgradable path for CBDCs.By building the peg with their investment coins, algorithm stablecoins are extremely volatile.ApplicabilityTo better understand the economic risks associated with various types of stablecoins and make better design choices for future tokens.Stablecoin owners need to take caution in the implications of economic designs for different stablecoins', '@Twan thank you so much for contributing a fascinating summary. A couple of quick questions: Do you know of any recent examples of stablecoins failing to maintain their peg and entering the death spiral? Are there ways that stablecoin projects can mitigate the risk by conducting audits?I’m also curious whether there is a point at which a stablecoin has so much money behind it that it becomes “too big to fail.” It also seems interesting to consider what might happen if more countries like El Salvador adopt Bitcoin as legal tender. Seems like it would blur the difference between CBDCs and crypto-backed assets a bit.', 'That’s interesting to think if stablecoins become “too big to fail.”[Fitch Ratings (2021). Stablecoins Could Pose New Short-Term Credit Market Risks. Retrieve from https://www.fitchratings.com/research/fund-asset-managers/stablecoins-could-pose-new-short-term-credit-market-risks-01-07-2021 1] In a recent report, Fitch Ratings seems to regard the reserve of Tether as a high-risk asset allocation. “A sudden mass redemption of USDT could affect the stability of short-term credit markets if it occurred during a period of wider selling pressure in the CP market, particularly if associated with wider redemptions of other stablecoins that hold reserves in similar assets.” (Fitch Ratings, 2021) However, it also points out that “We believe authorities are unlikely to intervene to save stablecoins in the event of a disruptive event, partly owing to moral hazard”. (Fitch Ratings, 2021) From this perspective, they don’t think stablecoins reach the quality of “too big to fail” so far. Maybe stablecoins need more capital and breakthrough moral concerns by some approach such as FIDF to reach it?', 'Thanks, James. Here’s my take on your questions.For a real example, Tether is at the center of attention. Let’s hope the stablecoin stays stableGrant Thornton LLP issues attestations each month for USDC. You can find the reports here: Centre | USD CoinAs cryptocurrencies are decentralized, it is less destructive than traditional institutesPreferably, legal tenders should have a stable price. Yet this quality is not essential to payment instruments', 'I think there will not be an international trend for countries to adopt Bitcoin as legal tender as El Salvador in the recent cryptocurrencies development phase. If they do so, it will be hard to adjust the exchange rate and currency supply. This may cause a severe problem on loan liquidity, then affect the economic activities in the country. So I think most countries may prefer to preserve their currency sovereignty rather than use this kind of non-sovereign, decentralized cryptocurrencies, which may also be influenced by unknown third parties. However, if there come more countries to do so, the power of the USD may be weakened. I also wonder what will happen in other aspects.On the other hand, as I know, many people are expecting to see a whole new financial system can be built, and this is prospective to begin from the cryptocurrencies-world. I think this is an interesting issue too.', 'It seems that exchange rates and currency supply are less pressing problems in their country, which is why they adopted USD in the first place. President Bukele of El Salvador says he believes this will encourage investors with cryptocurrency to spend more of it in his country, but I wonder if people holding Bitcoin are looking to spend them at all.', 'Thank you for contributing. Interesting and clarifying read!I am curious. What is the reason that over-collateralization still occurs in the status quo if it is such an expensive option? And, why have protocols utilizing this option not switched over to fiat-backed stable coins if this is the most stable option?', 'Thanks, shoule. Although over-collateralization could further ensure stability, switching to fiat-backed stablecoins could be financially devastating and thus is not an available option for everyone.', 'Thanks Twan, really great writeup. Would love to hear your thoughts on the below in regards to the users of CBDCsHave you seen any literature/insight on what goes into the consumer choice of which stablecoin to use? Of course there are some specific points of differentiation (ex. why someone would use a crypto-backed stablecoin vs. a Fiat-backed one)The authors state that fiat-backed stablecoins appear to be the only way to ensure long-run stability - with the eventual introduction of CBDCs what might be the use cases / demand for fiat-backed stablecoins that are not CBDCs?', 'Thanks for appreciating the work put into the summary. To your questions:The first one sounds more like a topic that belongs to business or marketing, not economics. The resources that may be helpful to gain “insights” are no more likely to show in academic papers than industry reports or user studies.The second one is asking for a prediction of the future. So far my favorite author on this topic is Nassim Taleb. If you know what he advocates, then you probably can understand that I think in a world dominated by unpredicted black swans, it is risky to assume CDBCs would take over. Needless to say what role stablecoins would play at that time.Hope this helps ', '@Twan and @Astrid_CH do you see any connections between your two recent summaries (Wildcat Stablecoins discussion post and this one)? There’s also a bit of anxiety in the market right now about a Chinese real estate company called Evergrande, which has liabilities of about $300B and seems to be on the brink of failure. The company has issued many low-quality bonds, which are similar to the ones said to be backing some the larger stablecoins. Is this the sort of volatility trigger that could trigger a death spiral (or more legislation from regulators)?', 'Evergrande is concerning to investors for many reasons. Analysts also have diverging opinions on how things may turn out. Yet I would say that the threat is different from the threat of a death spiral.Death spiral happens when the market loses confidence in the promise from the institution, and starts panic selling. Panic selling is something that happens during the death spiral, but it can also happen somewhere else, such as a market crash, which could also lead to panic selling.One of the main concerns with Evergrande’s impact on crypto is that investors that lost money may need to sell coins to raise funds. If bulk selling happens, prices may drop very quickly because of the surplus in supply. Price drops could also lead to problems with leveraging, hitting the market even harder.In this case, the price drop from panic selling is only part of the process. Other factors play significant roles too.So far issuers of some stablecoins have assured that they are not backed by the bond. However, if a stablecoin backs itself with bad bonds, and loses the value of the reserve due to that, then yes, it could enter a death spiral. That would be very relevant to this paper.', 'Twan, this is a great research summary!Stablecoin design and mechanisms is a constant work-in-progress, and having writing such as this provides such valuable insight and clarity over the topic.', 'Why thank you. There is plenty of interesting work in the forum. I hope you stay around and find more summaries that you would enjoy.If you’re particularly interested in stablecoins, you can check out Discussion Post: Taming Wildcat Stablecoins by @Astrid_CH.', 'Thank you so much for pointing out the Taming Wildcat Stablecoins discussion post. @Twan can you tell us a little bit about how some of the regulations being considered in the US (such as Taiwan) might affect the design of future stablecoins? And how might that shape decentralized finance?', 'Regulations in Taiwan and the US are quite different, and they are going to change as the market is still in an early stage of development. But if cryptocurrency continues to go mainstream, the government is likely to set up regulations that protect general investors against large risks.I’d also like to follow up with your question on the similarities between the Taming Wildcat Stablecoins post and this one.I think that post might be a more general overview of how stablecoins could develop over time, and this post is about a specific type of tail risk.', 'Thank you for sharing that discussion post, which was also very insightful!I’m curious, you write in your conclusion, “fiat-backed stablecoins may be the only way to ensure long-run stability”. Can you expand on your thoughts pertaining to how crypto-backed and algorithmic stablecoins can be a long-run solution with significant risk mitigation against a death spiral scenario? Or if you don’t ever see these types of stablecoins being a viable option, why that is so? You write that stablecoins backed by high quality assets perform better under stressed market conditions, and I would love to hear more about what you determine as the criteria for high quality.Lastly, if given the perfect scenario, what are your thoughts on the ideal economic design of a stablecoin?', 'IMAO crypto-backed and algorithmic stablecoins are less reliable because of the nature of that type of asset.Because the crypto doesn’t pay dividends, the only price gain comes from the expected value in the future.This easily leads to a vicious cycle when a death spiral happens, because the cause of that is exactly a significant drop of the expected value in the future.Ideally, a stablecoin should be collateralized by fiat or highly liquid assets. The reserve should also be higher than 1:1 to buffer other financial risks.', 'I keep thinking about the discussion we had about the digital eNaira and how struggling governments might use CBDCs to enforce spending controls or prevent withdrawals during a bank run. Smart Contract Summit 2021: Central Bank Digital Currency (CBDCs) & Blockchain Panel. Do you think stablecoins based on hard currencies might be used as a ‘safer’ substitute for a CBDC in some cases? Perhaps instead of using Bitcoin or pegging a currency to the dollar.', 'If it is a legal tender, rather than a means to store value in the digital world, then instead of issuing stablecoins backed by the dollar, they might as well just used the dollar.But what are the safety issues on CBDCs are you referring to?As for Bitcoin, I think the country that adopts that as their legal tender usually have different goals than what expectations we have from more traditional options.']"
"                Deep Diving into PRBMath, a Library for Advanced Fixed-Point Math              ",https://www.smartcontractresearch.org/t/deep-diving-into-prbmath-a-library-for-advanced-fixed-point-math/686,Tooling and Languages,55,[],"['testing', 'summary', 'oracles', 'zero-knowledge', 'dex', 'flash-loans', 'arbitrage-bots', 'discussion', 'dao']","['Deep Diving into PRBMathThere are severals reasons why doing math is hard in Solidity, with the chief one being the lack of support for fixed-point types. As per the documentation 46, fixed-point numbers can be declared, but not assigned to or from. Developers are expected to figure out on their own how to implement advanced math functions like exponentials and logarithms.PRBMath is a smart contract library that purports to solve the aforementioned difficulties and make math in Solidity easy and fun.OverviewPRBMath comes in four flavors:PRBMathSD59x18PRBMathSD59x18TypedPRBMathUD60x18PRBMathUD60x18TypedThe first two work with signed 59.18-decimal fixed-point numbers, while the latter two work with unsigned 60.18-decimal fixed-point numbers. The first and the third operate on basic integers, while the second and the fourth operate on structs. There is no qualitative difference between the non-typed and the typed versions - you use one over the other when you want to demarcate between basic integers and fixed-point numbers in your contracts.The values the numbers can take are bound by the minimum and the maximum values permitted by the Solidity types int256 and uint256, while the name of the number formats stems from the fact that there can be up to 59/60 digits in the integer part and up to 18 decimals in the fractional part.These are the functions that you are getting out of the box:AbsoluteArithmetic and geometric averageExponentials (binary and natural)Floor and ceilFractionalInverseLogarithms (binary, common and natural)Powers (fractional number and basic integers as exponents)Multiplication and divisionSquare rootIn addition, there are getters for mathematical constants:Euler’s numberPiScale (1e18, which is 1 in fixed-point representation)Finally, there are functions to convert from and to the fixed-point representation.How to UseThe README 51 on GitHub offers comprehensive instructions for how to use PRBMath, but here’s a sample:// SPDX-License-Identifier: UNLICENSEDpragma solidity >=0.8.0;import ""prb-math/contracts/PRBMathSD59x18.sol"";contract SignedConsumer {    using PRBMathSD59x18 for int256;    function foo(int256 x, int256 y) external pure {        int256 result;        result = x.abs();        result = x.avg(y);        result = x.ceil();        result = x.div(y);        result = x.e();        result = x.exp();        result = x.exp2();        result = x.floor();        result = x.frac();        result = x.gm(y);        result = x.inv();        result = x.ln();        result = x.log10();        result = x.log2();        result = x.pi();        result = x.mul(y);        result = x.pow(2);        result = x.scale();        result = x.sqrt();    }}To make the code compile, you will need to use either yarn or npm to install the prb-math 8 package.Target AudienceThere’s not much I can predict in terms of what the library is going to be used for, specifically. If anything, we will make good use of it in the next release of our Hifi 4 protocol.What I can think of though are the following use cases:Compounding interest rates.TWAP oracles.Pricing derivatives like options.Standalone AMMs.Custom price curves for Uniswap v3 and Balancer v2.Parts and ParcelsPRBMath is the result of a month-long tinkering with math puzzles and computer science algorithms. My goal was to write a math library that is at the same time practical, intuitive and efficient.To obtain the best possible gas results, I resorted to assembly code and unchecked 1 arithmetic. I was surprised to notice that in many cases it’s cheaper to check for overflow manually than let the Solidity compiler do it via checked arithmetic.Let’s delve into each function. Note that I will use the non-typed SD59x18 flavour throughout this document, but you can substite it for UD60x18 in all cases and the meaning remains the same. Further note that I will make frequent use of the “phantom overflow” notion, which refers to certain mathematical operations whereby the intermediary result overflows but the final result doesn’t.AbsSignaturefunction abs(int256 x) internal pure returns (int256 result);ExplanationThis is a simple function. The first thing I do is check that xxx is not min sd59x18, since in this case the absolute value couldn’t be represented in Solidity (read about two’s complement 1). Then I use the unary operator - to flip the value if xxx is negative or simply return the value if xxx is positive.AvgSignaturefunction avg(int256 x, int256 y) internal pure returns (int256 result);ExplanationThe naive way of calculating the average of two numbers is to add them together, then divide by two. The issue with this approach is that it’s susceptible to phantom overflow.To go around this, I divide each number by two and then add the result. But I must not forget that Solidity truncates the result when performing division. Thus, if both inputs are odd, I have to add one to the result to account for the doubly truncated halves.Bonus: the chronicle 1 of ideas that I iterated through before arriving at the fastest solution.CeilSignaturefunction ceil(int256 x) internal pure returns (int256 result);ExplanationI confess that I didn’t expect this function to be as subtle to implement as it was. This fact that ethers handles the modulo function differently to Solidity only made matters worse.The first thing I do is check that xxx is less than or equal to the value dubbed “max whole sd59x18”. This is basically max sd59x18 but with all 181818 decimals set to zero. This requirement is in place because the ceil couldn’t otherwise be represented in Solidity.Then, the idea is to calculate the remainder of xxx divided by the scale number (1e18), and:If the remainder is zero, return xxx.If the remainder is non-zero, I set the result to the difference between xxx and the remainder calculated above. And finally, if xxx is positive, I also add the scale number to the result.DivSignaturefunction div(int256 x, int256 y) internal pure returns (int256 result);ExplanationDividing two fixed-point numbers is done by multiplying the first fixed-point number by the scale number 1e181e181e18, and then dividing the product by second fixed-point number.Naturally, this creates a risk of phantom overflow. I prevent this by using Remco Bloemen’s mathematical mulDiv 11 function. It’s called mathemagical for a reason - you almost can’t believe that it works.But mulDiv works only with uint256 numbers. To go around this, I compute the signs and the absolute values.ExpSignaturefunction exp(int256 x) internal pure returns (int256 result);ExplanationI use the following mathematical trick:ex=2x∗log\u20612ee^x = 2^{x * \\log_2{e}}ex=2x∗log2\u200beI already have an implementation for 2x2^x2x, and the second factor I can pre-compute offchain and use as a run-time constant. The binary logarithm of eee is 1.4426950408889634071.4426950408889634071.442695040888963407.Caveats:xxx must be less than 88.72283911167299962888.72283911167299962888.722839111672999628.For any xxx less than −41.446531673892822322-41.446531673892822322−41.446531673892822322, the result is zero.Exp2Signaturefunction exp2(int256 x) internal pure returns (int256 result);ExplanationThe key insight here is to write the binary exponential as a binary fraction expansion.Any fractional number can be written as the sum of the integer part and the fractional part:x=n+f,0<=f<1x = n + f, 0 <= f < 1x=n+f,0<=f<1Then, using xxx as an exponent:y=2x=2n+f=2n∗2fy = 2^x = 2^{n + f} = 2^n * 2^fy=2x=2n+f=2n∗2fWriting fff as a binary fraction:f=f1∗2−1+f2∗2−2+...f = f1 * 2^{-1} + f2 * 2^{-2} + ...f=f1∗2−1+f2∗2−2+...Where fif_ifi\u200b can be either 000 or 111. Then:y=2n∗2f1∗2−1∗2f2∗2−2...y = 2^n * 2^{f1*2^{-1}} * 2^{f2*2^{-2}} ...y=2n∗2f1∗2−1∗2f2∗2−2...Note that:2fi∗2−1=(22−i)fi=2fi2i={1,if\xa0fi\xa0is\xa0022i,if\xa0fi\xa0is\xa012^{f^i*2^{-1}} = (2^{2^{-i}})^{f_i} = \\sqrt[2^i]{2^{f_i}} =\\begin{cases}1,              & \\text{if $f_i$ is 0} \\\\\\sqrt[2^i]{2},  & \\text{if $f_i$ is 1}\\end{cases}2fi∗2−1=(22−i)fi\u200b=2i2fi\u200b\u200b={1,2i2\u200b,\u200bif\xa0fi\u200b\xa0is\xa00if\xa0fi\u200b\xa0is\xa01\u200bI pre-compute these magic factors 22i\\sqrt[2^i]{2}2i2\u200b off-chain use them as run-time constants. The iii stands for the position of the bit in the fractional binary representation. E.g. if I were to match the very first bit, I would multiply by the square root of 222, i.e. ~1.41421.41421.4142.Caveats:xxx must be lower than 128128128, because 21282^1282128 doesn’t fit within the 128.128-bit fixed-point representation used internally in this function.When xxx is negative, I return the inverse of the binary exponential of the absolute value.For any xxx less than −59.794705707972522261-59.794705707972522261−59.794705707972522261, the result is zero.FloorSignaturefunction floor(int256 x) internal pure returns (int256 result);ExplanationThis function can be thought of an analogue of ceil.The first thing I do is check that xxx is greater than or equal to the value dubbed “min whole sd59x18”. This is min sd59x18 with all 181818 decimals set to zero and the last digit incremented. This requirement is in place because the floor couldn’t otherwise be represented in Solidity.Then, I calculate the remainder of xxx divided by the scale number 1e181e181e18, and:If the remainder is zero, return xxx.If the remainder is non-zero, I set the result to the difference between xxx and the remainder calculated above. And finally, if xxx is negative, I also subtract the scale number from the result.FracSignaturefunction frac(int256 x) internal pure returns (int256 result);ExplanationThis is as simple as taking the modulo of xxx with respect to the scale number 1e181e181e18. The implementation is based on the odd function 2 definition of the fractional part.GmSignaturefunction gm(int256 x, int256 y) internal pure returns (int256 result);ExplanationI first ensure that the product of the two inputs:Does not overflow.Is not negative.Then, I calculate the square root of the product.InvSignaturefunction inv(int256 x) internal pure returns (int256 result);ExplanationI divide 1e361e361e36 (the square of the scale number) by the input.LnSignaturefunction ln(int256 x) internal pure returns (int256 result);ExplanationI perform some boundary checks and then I use this mathematical trick:ln\u2061x=log\u20612xlog\u20612e\\ln_x = \\frac{\\log_2{x}}{\\log_2{e}}lnx\u200b=log2\u200belog2\u200bx\u200bThe first factor is the log\u20612\\log_2log2\u200b function for which I already have an implementation, and second factor I pre-compute off-chain and I use as a run-time constant. The binary logarithm of eee is 1.4426950408889634071.4426950408889634071.442695040888963407.CaveatsDoesn’t return exactly 111 for 2.7182818284590452352.7182818284590452352.718281828459045235, due to the lossy precision of the iterative approximation.Log10Signaturefunction log10(int256 x) internal pure returns (int256 result);ExplanationDue to the lossy precision of the iterative approximation (explained below under the log2 section), the last decimal of the result, that is, the 18th decimal, may not be the same as when calculating the logarith with an advanced calculator like Wolfram.This is an incovenience that can be glossed over in most cases, except when passing a power of ten as the input x. In this case, the function should return a whole number. The algorithm goes something like this:If the number is a power of ten, return the power.If the number is not a power of ten, use this mathematical identity:log\u206110x=log\u20612xlog\u2061210\\log_{10}{x} = \\frac{\\log_2{x}}{\\log_2{10}}log10\u200bx=log2\u200b10log2\u200bx\u200bChecking that the input is a power of ten was easier said than done (I documented 3) my efforts toward finding the fastest possible implementation). The gist is that there are quite a few ways to verify that a number is power of ten, but most are achingly slow in Solidity. The most efficient one is to use assembly and the switch control operator and compare xxx against all hardcoded powers of ten that fit within the given type.Log2Signaturefunction log2(int256 x) internal pure returns (int256 result);ExplanationI begin by looking at the sign of xxx:If it’s positive, do nothing.If it’s negative, take the inverse of xxx.I can do this because of this mathematical identity:log\u20612x=−log\u206121x\\log_2{x} = -\\log_2{\\frac{1}{x}}log2\u200bx=−log2\u200bx1\u200bNow I begin the iterative approximation algorithm. I will give a succinct description below, but do check the Wikipedia article I linked for a thorough explanation.Let nnn be the integer part of the logarithm. Then, the fractional part of the logarithm, fff, can be calculated like this:f=log\u20612x−nf = \\log_2{x} - nf=log2\u200bx−nf=log\u20612x−log22nf = \\log_2{x} - log_2{2^n}f=log2\u200bx−log2\u200b2nf=log\u20612x2nf = \\frac{\\log_2{x}}{2^n}f=2nlog2\u200bx\u200bNow, given the equality:2n<=x<2n+12^n <= x < 2^{n+1}2n<=x<2n+1It follows that:1<=x2n<21 <= \\frac{x}{2^n} < 21<=2nx\u200b<2Thus, calculating the fractional part of a binary logarithm can be reduced to calculating the binary logarithm of a number between 1 (inclusive) and 2 (exclusive). To do this I use the following two rules:log\u20612x=log\u20612x22\\log_2{x} = \\frac{\\log_2{x^2}}{2}log2\u200bx=2log2\u200bx2\u200blog\u20612x=1+log\u20612x2\\log_2{x} = 1 + \\log_2{\\frac{x}{2}}log2\u200bx=1+log2\u200b2x\u200bThe idea is to continuously apply the first rule until x2n\\frac{x}{2^n}2nx\u200b crosses the value of two. Then, I apply the second rule and halve xxx.CaveatsThe results are not perfectly accurate to the last decimal, due to the lossy precision of the iterative approximation.MulSignaturefunction mul(int256 x, int256 y) internal pure returns (int256 result);ExplanationMultiplying two fixed-point numbers is done by multiplying the two fixed-point numbers together, and then dividing the product by the scale number 1e181e181e18.Just like in div, I use Remco Bloemen’s mulDiv 11 to fend off phantom overflow, and I compute the signs and the absolute values separately. But there’s two subtle things that I make differently here.I am not using mulDiv, but a variant that I wrote, mulDivFixedPoint. The latter is twice more efficient than the former, because I replaced many runtime operations with constants pre-computed off-chain. The cool kids would call this constant folding 2.I add 111 to the result if this property holds:(x∗y)\u200amod\u200ascale>=scale2(x * y) \\bmod scale >= \\frac{scale}{2}(x∗y)modscale>=2scale\u200bThe goal with this is to increase the accuracy of the result in certain cases where Solidity would incorrectly truncate the result down. For example, without my optimization, a result of 6.6e−196.6e-196.6e−19 would be truncated to 000 instead of rounded up to 111 (recall that PRBMath works with maximum 18 decimals). For more details, see “listing 6” and text above it here 2.If there’s anything that might be called innovative about PRBMath, it’s this mul function. I basically took Remco’s function and refined it for fixed-point number multiplication.PowSignaturefunction pow(int256 x, int256 y) internal pure returns (int256 result);ExplanationThis is an emergent function - it is an integration of other functions already defined in the library:xy=2log2(x)∗yx^y = 2^{log2(x)*y}xy=2log2(x)∗yThat is, exp2, log2 and mul.PowuSignaturefunction powu(int256 x, uint256 y) internal pure returns (int256 result);ExplanationThis is an alternative to pow that works strictly with basic integer exponents, and it is slightly more precise.I implemented this function using the famous algorithm exponentiation by squaring 1. Given the popularity of the algorithm, I’m sure there is already a much better explanation somewhere else on the Internet; I’ll nonetheless give a succinct description here.Suppose nnn is the exponent. When nnn is odd, write xnx^nxn as:x∗(x2)n−12x*(x^2)^{\\frac{n-1}{2}}x∗(x2)2n−1\u200bAnd when nnn is even, write xnx^nxn as:(x2)n2(x^2)^{\\frac{n}{2}}(x2)2n\u200bI begin by setting n=yn = yn=y and proceed with halfing nnn and applying one of the functions above, depending upon the parity of nnn.CaveatsThis is the only function in PRBMath that doesn’t work only with fixed-point numbers. The second argument yyy is a basic unsigned integer.I assume that 000^000 is equal to 111.SqrtSignaturefunction sqrt(int256 x) internal pure returns (int256 result);ExplanationThe idea here is to employ the Babylonian method 2 and set the initial guess to the closest power of two that is higher than xxx.xn+1=xn+Sxn2x_{n+1} = \\frac{x_n+\\frac{S}{x_n}}{2}xn+1\u200b=2xn\u200b+xn\u200bS\u200b\u200bI do seven iterations and I stop.Gas EfficiencyYou can find gas efficiency tables on GitHub 7. I won’t replicate them here to avoid having to make revisions in multiple locations.Gas-wise, PRBMath is comparable to ABDKMath 4, which is reputed 1 to be the fastest advanced math library. But given that PRBMath is faster 2 than ABDKMath among 7 out of all 12 common functions, I think it’s safe to assert that PRBMath has replaced ABDKMath as the most efficient advanced math library for Solidity.Testing and SecurityThere are 1,422 test cases and test coverage is around 96%, although in reality I think that the number is lower because solidity-coverage doesn’t cover unchecked blocks. Nonetheless, the percentage should be pretty high.To increase security guarantees, I plan to:Write fuzzing tests with Echidna 3.Get a security audit.User DocumentationI don’t have a documentation website, but I went above and beyond to ensure that every single block of code is properly commented. In fact, I think that there are more commented lines than actual lines of code.All functions are preceded by comments that adhere to the NatSpec Format 3, which makes the code clear, intuitive and sensible. There are even sections for requirements and caveats, some of which are replicated above.You don’t need an undergrad in mathematics or computer science to understand the inner workings of PRBMath.End NotesPRBMath is released under Unlicense 1, but I don’t give any warranties and I won’t be liable for any loss, direct or indirect, through continued use.Issues and PRs are welcome on GitHub 11. Questions I’d like to address the SCR community:Generally speaking, what do you think of this? Would you use PRBMath in your projects?Can you find any critical flaw anywhere?Do you know how to make any of my function implementations simpler, more efficient, or more elegant?What functions are missing? Why?', 'Thanks for this great library and your explanations behind each function. Given all the benefits, if someone was building a financial application in solidity what reasons could they have for not using such a library?', 'If someone was building a financial application in solidity what reasons could they have for not using such a library?If you’re using Solidity v0.7 or below, you can’t use PRBMath. But for v0.8, indeed, this is a solid choice for a fixed-point math library in a Solidity project.There is one edge case though. If you care about every single gas unit, and you happen to only need to use either powu or sqrt, it might be cheaper to use ABDKMath. Albeit you will have to work with 128.128-bit numbers, it has faster implementations for these specific functions.', '@paulrberg Thanks for this post. This is a joy to read.A couple of points to clarify:How close is the current implementation from production usage? Are there any projects using it?Looking at coveralls report on Github, while the overall test coverage is high (line coverage at is 95% and branch coverage at almost 97%), I noticed that for contracts/PRBMath.sol, the overall coverage is at 83%. Any particular reason for that drop in comparison to the other files?As you pointed out, solidity coverage is not counting unchecked blocks, as their content does not appear green in the coveralls report. Is your test suite indeed stressing those paths?Was the entire code written from scratch (seems like), or was it partially cloned from another library? If so, having this sort of traceability documented in the code would help towards a future audit.Please let me know of your thoughts.', ' lnrdpss:How close is the current implementation from production usage? Are there any projects using it?GitHub reports that there are currently 21 projects using PRBMath:What portion of these are serious projects, and what are short-lived hackathon experiments, I can’t know for sure. What I do know is that we at Hifi 2 will use the library in our v1 release. Also the Pods team, who I met in person, told me that they are looking into PRBMath for their next release as well. lnrdpss:I noticed that for contracts/PRBMath.sol , the overall coverage is at 83% . Any particular reason for that drop in comparison to the other files?Note that the core PRBMath.sol file is not meant to be used directly by end-users. I wrote it with the specific purpose of avoiding duplicate code.Coverage is ~83% because the mulDivSigned function is not tested. But this function is not used in SD59x18, SD59x18Typed, UD60x18 or UD60x18Typed. I added it an  “alpha” feature that can be used by experienced users who know what they are doing (none of the instructions in the README contain an import for PRBMath.sol, so in theory users should not know about it). lnrdpss:Is your test suite indeed stressing those paths?It certainly does. The vast majority of my usage of unchecked is “exhaustive”, meaning that the entire function is wrapped within an unchecked block.Side note: this is when I realised that it would be great if Solidity added an “unchecked” keyword that we could add as a modifier to functions that should have disabled the usual overflow/ underflow checks. lnrdpss:Was the entire code written from scratch (seems like), or was it partially cloned from another library?From scratch, but I need to expound on this.While some of the features are entirely novel, like the avg function or the different mulDiv variants (mulDivFixedPoint, mulDivSigned), I sourced the mathematical logic behind some functions (e.g. exp2, pow) from Mikhail Vladimirov’s Math in Solidity 3 series. Mikhail is the author of ABDKMath, but his article series does not share snippets from the library; it just explains the math.This is conjecture on my part, but I suspect that he also sourced a part of his implementations from Wikipedia or other places. Some mathematical functions can most efficiently be implemented with certain known algorithms, which are timeless. E.g. the Babylonian method for the square root.', '@paulrberg Thanks for the clarification and effort put in the explanation. All is clear :)Still, if you allow me, I would like to follow-up on the first  question you had in your post (questions 2 & 3 are sort of by-products): paulrberg:Generally speaking, what do you think of this? Would you use PRBMath in your projects?I think this is a great project and the value is there. Not only it brings more functionality than ABDK, but it is also more gas-efficient for certain operations. However, ABDK’s library has been extensively used by now, and since ABDK is itself an auditing company, there is some level of assumed trust.  Hence, as you acknowledge yourself, the next step is to make sure your code is secure as best as you can. Consider setting up a crowd-funding arrangement to get an audit for this library. Without such an audit in place, adoption could be hindered.Also, you mentioned  using Echidna to fuzz test and bring more security guarantees/confidence that the implementation is not buggy: paulrberg:To increase security guarantees, I plan to:Write fuzzing tests with Echidna.Note: Scribble could potentially be an alternative; @maurelian wrote a nice post on SCRF about it 3. In a nutshell, “Scribble is a specification language and runtime verification tool that translates high-level specifications into Solidity code” (ref: Scribble | ConsenSys Diligence 1). Following their website, “[…] after writing properties, developers can use tools such as Diligence Fuzzing to automatically test smart contracts and ensure all is working as planned!”. Their fuzzing as a service tool is currently being released as an early access style, but should be something to be considered.Hope that helps. Once again thanks for your post and welcome to SCRF ', 'I thank you! This was fantastic feedback. lnrdpss:Consider setting up a crowd-funding arrangement to get an audit for this library. Without such an audit in place, adoption could be hindered.Yes, I actually have an idea in mind for how to run the crowdfund. I agree that an audit would go a long way. lnrdpss:Note: Scribble could potentially be an alternativInteresting, I didn’t know about it, thanks. I will check it out!', 'I just saw this Solidity rounding error in Mean Finance 4. @paulrberg  do you avoid this kind of bugs?More generally, how do you guarantee that a rounding error somewhere won’t snowball into a bigger error somewhere else?', ' jyezie:I just saw this Solidity rounding error in Mean Finance . @paulrberg do you avoid this kind of bugs?I did all I could to make the library as precise as possible.Firstly, take a look at mulDivFixedPoint 5, which contains some hardcore assembly code. What that does is to implement the logic explained in listing 6 of this journal article, i.e. to increase the accuracy of the mulDiv result in cases where Solidity would otherwise truncate the result down. Without my adjustment, a value like 6.6e−196.6e-196.6e−19 would be rounded down 000, instead of being rounded up to 111, which is numerically closer.Secondly, most PRBMath functions have a “Caveats” section in their NaSpec that explain what shortcomings they have. Here’s an example for the ln (natural logarithm) function:This doesn’t return exactly 1 for 2718281828459045235, for that we would need more fine-grained precision.Now, about this: jyezie:how do you guarantee that a rounding error somewhere won’t snowball into a bigger error somewhere else?There’s so much precision you can get in Solidity.Any Solidity library developer who guarantees that there aren’t any rounding errors is either a fool or a fraud. The onus is on the library user to ensure that their protocol is safe to use.', 'Thank you for the library and the writeup, Paul!I am to create a vault that gives out rewards based off of the geometric sequence. To do this, i need to perform operations like 0.9993^10000I am having trouble making this happen with your library for base values less then 1. Here is an example that tries to calculate 0.9993^3, which reverts. I also tried with powu, which returns 0.  function doTheMath() external pure returns (uint256 result) {    uint256 x = 9993*10**14;    uint256 y = 3*10**18;    result = (x).pow(y);  }', 'Hi @djrthree, I’m glad that you like it!The best channel to ask for support is the Discussions 4 section on GitHub.I’d like to not offer any answer to your question here, because the behavior of the pow function might change in the future for sub-unitary bases.']"
                Flying the SourceCred Plane              ,https://www.smartcontractresearch.org/t/flying-the-sourcecred-plane/1873,Meta,58,[],"['discussion', 'governance']","['After the last SourceCred Guild meeting 3 I attended, I was reflecting on SCRF’s SourceCred instance, and was struck with an image. @zube.paul and @brian.alexakis flying a plane into a storm.1600×1598 464 KBDon’t worry! I’m projecting. As a long-time SourceCred contributor, who watched SourceCred (the project) rocket into the stratosphere, paying over $1M/year to contributors via Cred scores, only to see it crash to earth 9, I have a new respect for the power of these systems, as well as the challenges of ‘permissionless’ orgs generally. I often feel like a survivor, crawling from the rubble, shaking off trauma, muttering to myself about plane designs…I also see some contributors expressing some common concerns around SourceCred (SC). This is expected. In addition to the novel challenges of algorithmic governance, reward systems have a tendency to surface all our issues.1308×992 124 KBI’ve spent the last five years working in permissionless systems with novel reward mechanisms (Decred, SourceCred). These systems have enabled me to work with the most amazing, diverse groups of humans I’ve ever encountered. And allowed me the freedom to work on what I find most meaningful. I think they’re the future. I’ve also seen some plane crashes.Using sophisticated machine learning algorithms to reward contributors, governing the algorithms in a transparent and decentralized way, is like building a plane while flying it. The planes do fly. But I wanted to share some observations from the failures I’ve seen, which will hopefully enable the SCRF community (and others using SC) to govern their SC instances more effectively.The SCRF PlaneSo the SCRF plane (SC instance) looks solid. It appears to be one of the safest, smartest designs I’ve seen actually. The parameters make sense. The payout policies are well designed. The pilots (SC guild members) seem generally skilled, like they have some experience implementing complex systems. And I can see them putting in the hard work of proper governance: engaging the community, creating reports, incorporating feedback, addressing concerns, etc.The rollout feels responsible. By starting with relatively small amounts (5,000 DAI/month), and systematically iterating parameters, the stakes feel relatively low. The org is diversified with other reward mechanisms (salaries, grants, contests, etc.). The community seems well-moderated, and the culture open to experimentation.SCRF’s SC instance right now feels like a party bus with wings, flying low to the ground. A flying open bar for academics funded by crypto whales and VCs (or wherever SCRF’s funding comes from). At this stage, the plane would presumably be easy to land. Even a crash landing would probably go unnoticed by most.1600×1573 582 KBFlight RisksShort-term, typically the main risks for a SC instance are:MaintenanceSince SC (the org) has winded down 9, the project no longer has paid plane mechanics (maintainers). It’s just a regular OSS project. The SCRF community has discussed this on the forum and decided the risks acceptable for now. I agree. SC has ‘broken through’ as a technology. It has some large users incentivized to make repairs (submit PRs with bug fixes), we recently found a dev to maintain the codebase and do critical bug fixes (woot!), and some core contributors are exploring ways to secure more sustainable funding (hope to have some news soon  ).SCRF can minimize these risks by:Being transparent about the risks (e.g. this post, updates, etc.)Not becoming too reliant on unique SC functionality until it has more organized support (i.e. keep it so that SC can be swapped for another reputation/valuation system if need be), or winded down without causing too much disruption. I have recommended that SCRF keep SC to Discourse for now, as the Discourse plugin is IMO the most robust and least likely to need fixes in the near-to-medium term.Keep up the governance work that makes it responsive to community input. As expressed by the SC Guild on a recent call, “keep SC something that the community is doing, not something that is happening to the community”. Similar to early feedback I’m hearing about Coordinape, where people don’t mind so much when Coordinape payouts aren’t very accurate because at least they had a voice, I find that when people have a voice in changing SC parameters, they tend to be happier with the results. Even in cases where I personally didn’t see that much improvement in the scores, as long as the results are perceived as directionally better, the act of changing the parameters is often meaningful to participants. This seems especially true when shared values can be translated directly into parameter choices (e.g. I think reviewing/commenting is generally undervalued, let’s increase the Cred that flows to post replies). Personally, I tend to lean towards governance minimization strategies, but have also seen communities get really into granular parameter choices (e.g. the Token Engineering Commons (TEC) instance has tons of semantic choices baked into parameter values). I suspect this will be a function of culture and implementation resources. A warning: people often want more control in theory but don’t want to do the tedious governance work, or don’t have the technical skills or resources. This can lead to disappointment if expectations aren’t managed.BurnoutAs I’ve experienced myself maintaining SC instances, proper maintenance can be a lot of work. Especially if taking it seriously, as SCRF has so far. Engaging the community for feedback, generating reports, meetings, documenting everything on GH, uploading to YouTube, etc. is a lot of work. Having maintained MakerDAO’s SC instance for 1.5 yrs, I have firsthand experience with how stressful this type of work can be. Especially if navigating DAO politics, exploring new parameter configurations, doing policy work around payouts, etc. Maybe I’m projecting again. But want to say I see you SC Guild, and make a couple suggestions:Relax: you’re doing great and the stakes are still low (flying the party bus)Prioritize automation and efficiency where possible: e.g. having everyone opt in every month makes sense to me conceptually and values-wise, but it adds admin overhead? I think it would probably be OK to leave people opted in, in part because a) inactive contributors will see their Cred scores diluted over time, and b) it appears that ‘Cred whales’ aren’t opting in anyway. This advice may be off if the guild has a more efficient way of updating the instance state (opt-ins/outs, payouts, balances). But if doing it the default way by updating the static site on GitHub, the process can be tedious, time-consuming and error prone for some, especially for non-devs. The admin load also generally increases linearly with the number of opted in contributors.Medium term risksMedium-term, risks around SC tend to be tied to raising reward amounts. When rewards increase significantly, approaching what people might make via traditional salaries, I’ve observed that there tends to be a psychological ‘flip’ in contributors. With the stakes higher, some become more reliant on the income (potentially even quitting their Trad job), and expectations can shift. This can be mitigated to an extend by clear communication and expectation setting. But humans are gonna human. Especially if deeply invested in established systems or trying to overthrow them. People can (consciously or subconsciously) start comparing SC to a job. Which obviously provides greater perceived income stability and guarantees (legal protections, benefits, etc.) than a typical SC instance. The plane is flying at 30,000 ft now. And people start asking questions about why the fuselage is shaking, why the fuel gauge is low, and when the plane is gonna land (it is landing, right? Right??).Personally, having experienced the volatility of several tech startups, I prefer the volatility of SC to a job. A salary is flat, until it abruptly goes to zero. And the relationships you invest in turn into LinkedIn connections. In reality, the volatility of a flat salary is simply hidden. Usually by a centralized authority incentivized to withhold information from you and shift financial risk down the hierarchy. SC income is volatile, but payouts are determined by a distributed consensus on the value you create. It provides a different type of stability, which is not legible at first to people without experience in these systems. But apparently I’m an outlier here, presumably due to some combination of privilege, high risk tolerance, and experience with these systems. Most people I’ve noticed want more stability in their pay than I do. And creating stability via the algorithm is doable, but still relatively unexplored territory. Also worth noting that if SC payouts are high enough, they can start to compete with other reward systems (e.g. salaries), which can create governance labor.I should also say here that, if the stakes are high enough, even scores the community generally thinks are fair are likely to be disruptive. If someone asks why their pay is X, and the answer is ‘because the pseudo-AI in the black box says so’, that may not be an acceptable answer, especially if someone is taking a pay cut from their traditional job. Data analysis and understanding of Cred flows is still nascent, so answering hard questions about how Cred scores are calculated can be difficult.Ideological differencesI hesitate to even bring this up. I’m wary of stirring up ideological debates generally, and don’t want to impress my personal views on SCRF (WARNING: this only my opinion, and not that of the SC community or ecosystem). However, reward systems are viewed by many as inherently political. Indeed, for many in the space, building is an explicitely political act, prefigurative politics. And SC does afford an unprecedented opportunity to encode your values, in a real, reifying way. So it should not come as a surprise that SC often raises moral, political and ideological questions.This can lead to some common pitfalls:Proxy wars: crypto protocols have a history of ideological holy wars. Typically over technical changes to the protocol (e.g. the Bitcoin block size wars of 2017). Predictably, the conflict becomes about much more than the proposed technical change (e.g. increasing the Bitcoin block size by 1M). Often to the detriment of the discourse around the actual change, much like IRL proxy wars are never good for the country it happens in. If the stakes are high enough, and participants have differing incentives, discussions around even minor changes can spiral into polarized, black and white narratives detached from reality. For instance, in retrospect, we can look back at the block size debate and see that catastrophic predictions around increasing the block size were overblown. Several Bitcoin forks used larger block sizes and the sky didn’t fall. They’re still producing blocks as we speak. If this dynamic happens predictably with decentralized lending protocols, it would be strange for it not to show up in a system valuing contributions, where ideological and value-based arguments may actually have more relevance to the technical change.Inflexibility: if you have based your identity on a particular political philosophy or ideology, and have taken an ideological stance on a particular change to the protocol, are you capable of continuing to participate if you lose? If you win, and things are getting noticeably worse, are you capable of seeing and acknowledging it? This is particularly problematic in sufficiently complex systems, where causation can be difficult to impossible to determine from correlation. I have myself fallen into this trap. Perhaps even as I write this  ?dalle_cargo_cult604×1469 106 KBScapegoating:  If the community has a lot of conflict or disagreement, that tension has to surface somewhere… If there’s no way to resolve that conflict, some will look to blame SC as the source of their problems. I’m not saying SC won’t legit be the source of the problem in some cases. Just that the only situations I’ve seen SC blamed for a community’s problems, there also happened to be lots of unresolved conflict and other pressures not unique to that community. And the claims against the algorithm seemed weak. Though one potentially predictable issue is that conflict does make it more difficult to reach consensus on governing the algorithm, which can lead to more conflict…Personally, the more experience I have, in all different types of orgs, the more I suspect scapegoating will always be present to some degree. I would just urge caution around claims that SC is the ‘root cause’, as that could be obscuring deeper issues.Long distance flightsLonger-term risks are more difficult to assess. Technology-driven social change is so fast now (and accelerating) that nobody can really predict where this is all going. Any system powerful enough to create alternatives that can rival the current system contains both utopian and dystopian potential. I personally advocate for keeping an open mind and having an emergent strategy 1, as expressed by adrienne maree brown in her book of the same name. Realize that larger structural issues may not be solved in our lifetime, or even several generations. Do the hard work to create the psychological safety necessary to have real conversations, know that most experiments will fail but are necessary to move forward, iterate based on actual outcomes and don’t become so attached to ideological/conceptual commitments that they bind you to bad choices.I also suspect an ethics of care 1 may be more promising than coding of explicit rule-based ethics, as it provides more flexibility.  Though have not yet had the opportunity to test that.On leaderboardsAs an example of a contentious issue, I’ve heard a couple people express concerns about the leaderboard in the SC instance. That it could introduce unwanted competitive dynamics.image1203×830 61.7 KBPersonally, I generally agree with this sentiment. I think we’ve overemphasized competition to the point it’s harmful in many contexts. I’ve been in corporate environments where leaderbaords seemed to drive toxic culture; I worked at a startup with leaderboards literally hanging over the heads of overworked sales and customer service reps, and was not surprised when it was sued for $145M 2 for highly unethical business practices, rendering my stock options worthless In practice, I’ve seen communities generally like the leaderboard. It seems to generate playful comments and healthy competition. For instance, Maker does bi-weekly reports on its SC instance that features two leaderboards: top 10 posts and top 10 Cred earners over the last two weeks. The comments on it are generally positive (e.g. the latest report).  New people in particular seem to like it, as it gives them recognition they may not get anywhere else. Perhaps a bigger problem in DAOs today is the tyranny of the structurelessness 1, and visualizing invisible power structures helps promote more equality than flatly rejecting any structure that facilitates competition?I honestly don’t know the answer for SCRF, or any community. But I will point out a couple options that don’t involve a leaderboard, should the community not want one:Don’t use the SC default instance: one can display the scores however they want (or not at all). The administrators can simply not turn on the creation of the public site, and only view the instance locally when making changes.Alternative visualizations: there are nearly infinite ways to visualize the basic SC data structure (a graph of nodes that represent users and contributions). Fields such as social graph analysis have produced many visualization templates to choose from.Planes in flightFortunately, there are now a number of SC planes in the wild. While the governance surface presented by SC is large and relatively unexplored, a swath of the possibilities have been explored, and SCRF can go there in relative safety, should it choose.The most comparable SC instance would be MakerDAO, which has been paying significant rewards (~20,000 DAI/mo) for governance contributions on its Discourse for nearly two years. Contributors are generally happy with the instance. It has served decentralization, and appears to be proving itself particularly useful as a recruitment mechanism. A number DAO contributors with full-time positions in Maker Core Units (CU)) have credited SC for bringing them into the project. Maker recently adjusted its elevation down a bit (decreasing total payouts from 20,000 DAI/mo to 14,000), and created a second plane, specifically designed to compensate delegates in its governance system. The main instance pays out ~10,000 DAI/mo, and the delegate instance ~4,000 DAI/mo.I would characterize Maker’s SC instances as the first DC-10s, regular ferrying contributors to more conventional DAO roles and outcomes.1008×996 167 KBSCRF could do something similar, customized to its goals, and operationalize without too much risk IMO. A sort of top of funnel for the DeSci revolution  ?On MoonshotsGoogle’s PageRank algorithm (what SC uses to score contributions) was inspired by academia’s citation system. SourceCred was founded by an AI engineer in Google’s Deep Brain group, who quit Google to create a credit attribution system based on PageRank that would be free of corporate capture. Which subsequently escaped the lab and was used by a DeSci project (SCRF) to create a new cryptoeconomic system so powerful it replaced centralized academic institutions within a decade. Like Wikipedia replacing encyclopedias, or cryptocurrencies and DeFi replacing the Fed and banks, in the span of a few years, the ivory tower was burning, as academics ran free into the greenfields of knowledge. Scholars were finally paid for knowledge they create (peer reviewers too this time!), not just based on where they were born, or how well they play the politics of top-down hierarchies captured by monied interests. The algorithms were governed in a decentralized manner where people had a genuine voice. Viva la Revolution!Yeah…I should warn you, when people discover SC for the first time they tend to get a little… manic. The basic primitive (arguably, a credibly-neutral intersubjective valuation by a community) provides new technological affordances that make possible many dreams previously deemed infeasible. People often get very excited. In SC, we call this initial excitement being ‘sourcepilled’.1478×1452 290 KBHere’s the thing about sourcepills though…they wear off. The SC technology is complex, and often unwieldy. It works well for a number of use cases. But bending it to your will to do bigger, more precise things is often difficult. To the point many give up. This has left more than a few with a crushing hangovers and high opportunity costs, swearing off sourcepills forever.While I believe many ambitious dreams are made possible by this technology, many of those will require considerable technical resources (developers, data analysts, governance experts, etc.), as well as work in fields such as algorithmic governance, cryptoeconomics, machine learning, sociology, org science, etc, etc… I generally advise projects to view SC as a source of ‘signal’, to be used as input into other mechanisms. And the signal can be noisy.The good news is, the core CredRank algorithm (the engine) is solid tech, the existing plugins (the plane models) (Discourse, Discord, GH) provide much room for experimentation. The algorithm can be programmed to an extent at the level of configuration and surrounding policy. It’s also possible to create new plugins (aircraft) with the right resources. The plane flies. But it can be a bumpy ride.It’s important that communities not view SC as the cure for all its problems, but as one piece of a larger puzzle. Cred scores, viewed as ‘signal’, can be useful input into any number of mechanisms. For instance, even a noisy measurement of value can be used to create good-enough binary outputs like Sybil detection, making feasible experiments in more democratic, human-centric governance.I myself have just enough knowledge and experience of the algorithm and codebase to see a fuzzy, shifting view of the possibilities. And the possibilities are real. But you should know, I’m largely doing it by feel, without being able to make precise predictions or guarantees. I may have been on sourcepills when I wrote this.image619×610 37.8 KBDeSci does seem particularly exciting, because the scientific process and professional norms of academia constrain the scope of the problem in a way that could curtail some of the problems SC has seen in other contexts. And the internalized norms will survive a long time, even when more freedom is created…You are FlyingI want to leave you with a suggestion: you are flying the plane. Yes, the larger plane (SC instance) is collectively governed. However, you can also think of your SC (Discourse) account like a plane of its own. Unlike a typical institution, where you contribute value in a thousand ways, hoping a person at the top will eventually give you a binary outcome (grant, job), each contribution in SC is like a micro grant. But with the payout determined by a rolling vote of your peers.Like Bitcoin, the distributed consensus algorithm is probabilistic. But in SC, you receive rewards in proportion not to computing power but the value you deliver, as determined by the community, not a centralized authority. You no longer need to ask permission to get rewarded for your work! Once you realize that mind shift, it can be a very empowering and fun experience 1546×1518 448 KBOf course, the jet fuel (money) will run out eventually, unless you can figure out how to coordinate with the other pilots to produce revenue. But that is for another post.', 'Hey @s_ben, beautiful and insightful analysis of SCRF’s SourcCred (SC). I must say that the analogy and imagery are the USP of this post, great job.To clarify things out, I have a few questions:In a system like SCRF’s SC, where there is a constant inflow of subscribers, can you share actionable ways to create stability in pay via the algorithm?In talking about the “Medium term risks” you raised, leaning on your experience as a SourceCred contributor and your first hand experience on how SCRF’s SC works, what would be your maximum monthly SC rollout recommendation to keep things sustainable on SCRF’s SC?', 'Good content @s_ben , this is a topic i’ve really been trying to put up, now i’m really happy to see it at last. Well, a reward for contribution is cool if the platform/organization has got some good source of funding and it’s ready to make some sacrifices and able to manage the system, cause i’ve heard and read on some platforms that went into bankruptcy due to bad management and exploitation by some.Now talking about SCRF and it’s 5000 DAI reward for contribution, well i feel there can be an increase if the probability of it not affecting the system is checked and deemed to produce a positive result rather than a negative one. s_ben:if SC payouts are high enough, they can start to compete with other reward systems (e.g. salaries), which can create governance labori’m agreeing to this view that an increase in payout/reward propel an increase in labor, cause everyman wants to see a reward for whatever he’s participating in and the time he spends “cause time is money” even when the reward is little there would be an urge to participate and everyone would want to put in his their best, but when there’s a higher reward on SC there would be a higher level of participation and this would attract more participators and increase the labor force to produce a higher yield, cause an increase in efficiency and thereby project the organization/platform into a higher level of stability. Ulysses:In a system like SCRF’s SC where there is a constant inflow of subscribers, can you share an actionable idea(s) on ways to create stability in pay via the algorithm?Well @Ulysses this is a good question. This is just like an economic situation, paying compensation that are over the market rate (known inside financial matters as “efficiency wages”) can be an imperative spurring drive for your existing worker base. The instinct is direct: higher compensation makes a work more alluring. Since there’s an inflow in the number of subscribers, there should be an increase in the funding so reward can be tangible even if not to the highest level but at least average. Because there’s a daily increase in the inflow of SC subscribers, pay will continue to drop and remain unstable if it’s same 5000 DAI amount, and if not increased, there would be a lesser governance labor when compared to an increased pay situation.RepresentationIncrease in Inflow of  Members ==> Increase in Total Pay Amount ==> Increase in Participation = GrowthUnder normal circumstances, they should be directly proportional to one another, and so there’s a forward flow, but there’s going to be a backward flow & a decline in labor/efficient participation when there is a continuous inflow of subscribers and compensation amount/ Pay is left constant.', ' Ulysses:Hey @s_ben, beautiful and insightful analysis of SCRF’s SourcCred (SC). I must say that the analogy and imagery are the USP of this post, great job.Danke. Curious, did the DALLE images inform the meaning of the words, or was it more just eye candy? Ulysses:In a system like SCRF’s SC where there is a constant inflow of subscribers, can you share an actionable idea(s) on ways to create stability in pay via the algorithm?Interesting you use the word subscriber. To me subscribers pay, instead of being paid.Income stability could be created in a number of ways. The most obvious mechanism to use would be the payout formula, which is separate from the Cred scores. For instance, the formula could be that everyone is paid the same amount of DAI if their weekly Cred score stays above a certain threshold. You could also base payouts on different functions of the (weekly) Cred scores. For instance you could use lifetime Cred scores (sum of all weekly Cred scores), which are more slow moving.There are also parameters you can tweak to change Cred scores directly. For instance, there is an ‘alpha’ parameter, which controls the ‘leakage’ of Cred from node-to-node. Increasing this parameter effectively allows Cred to ‘spread’ further from where it is created, creating a ‘flatter’ distribution. This could create more ‘equality’ in the sense that less active contributors could see higher Cred scores over time. Here’s an article on how the algorithm works that may be accessible to those with some high-level technical knowledge. There are also more indirect ways to create stability by tweaking Cred flows. For instance, if you can identify activity that is underpaid, you may be able to ‘boost’ it via parameter changes . Ulysses:In talking about the “Medium term risks” you raised, leaning on your experience as a SourceCred contributor and your first hand experience on how SCRF’s SC works, what would be your maximum monthly SC rollout recommendation to keep things sustainable on SCRF’S SC?My gut tells me you could probably go pretty high. Up to 20k/mo like Maker did, perhaps higher. However, I do not trust my gut that far into the future. I would recommend slowly increasing the payouts and changing course if needed based on community feedback. But that is assuming that SCRF has the political will and bandwidth to manage the instance. Which I am not assuming at this point. Increasing payouts could increase labor costs and introduce new issues, such as “Cred farming” (contributors creating posts primarily to make money), gaming, new social dynamics, etc. It’s also not clear to me what the community sentiment is. In the initial post proposing SC, I saw some common concerns, and am curious what people think now after 5k/mo DAI has been distributed for a couple months.', ' Freakytainment:Well, a reward for contribution is cool if the platform/organization has got some good source of funding and it’s ready to make some sacrifices and able to manage the system, cause i’ve heard and read on some platforms that went into bankruptcy due to bad management and exploitation by some.Have any examples you can share? Freakytainment:Now talking about SCRF and it’s 5000 DAI reward for contribution, well i feel there can be an increase if the probability of it not affecting the system is checked and deemed to produce a positive result rather than a negative one.I think it will affect the ‘system’ regardless, especially if payouts are increased. And that ‘positive’ is highly subjective, so hard to measure unforch. Freakytainment:there’s a higher reward on SC there would be a higher level of participation and this would attract more participators and increase the labor force to produce a higher yield, cause an increase in efficiency and thereby project the organization/platform into a higher level of stability.I think it’s pretty obvious by now that money will attract contributors, and that SC would produce a higher ‘yield’ in the form of more contributions (posts, comments, etc.). Efficiency and stability however assume a defined output, as well as a business model presumably if the stability is economic in nature.  I don’t think SC solves this on its own. There have been a few efforts I know of to use SC to direct contributors towards certain outcomes, but I have yet to see any that were demonstrably successful. It can increase engagement, which can indirectly progress a community’s stated objectives. But because valuations are determined largely by the community, this essentially allows contributors to get paid for all value they create, not just the value that advances those objectives. If the payouts are substantial enough, this could even subvert more traditional forms of authority, as you’ve now taken away that authority’s main mechanism for exerting power over (revoking pay). Even worse, by empowering contributors, you have lessened their fear. And most hierarchies rely primarily on fear to maintain power relations.image952×720 40.5 KBOk now I’m drifting into personal politics I fear. And there are other valid interpretations I’m leaving out. But I will just say, it seems clear that decentralizing valuation does not on its own produce efficiency or stability. It’s possible SC is creating more value overall compared to other mechanisms. But to create economic stability, a community must still produce something to trade for fait currency, similar to a company. Or, as some in the DAO space are now postulating, become its own mini nation state, with its own currency, internal economy, etc. Which is a much more ambitious project. Freakytainment:Since there’s an inflow in the number of subscribers, there should be an increase in the funding so reward can be tangible even if not to the highest level but at least average. Because there’s a daily increase in the inflow of SC subscribers, pay will continue to drop and remain unstable if it’s same 5000 DAI amount, and if not increased, there would be a lesser governance labor when compared to an increased pay situation.This is an important choice. I’ve seen commiunities that kept a flat amount (e.g. SourceCred, MakerDAO), and communities that varied payouts based on Cred created (e.g. MetaGame). I wish I had hard data, analysis and conclusions. Alas, I do not. I will say that if payouts are kept flat, I have heard contributors say it introduces a ‘scarcity mindset’. For example, I’ve heard a couple people say they were more hesitant to invite people to a DAO because they didn’t want their payouts decreased. It seems to be less of an issue if the payouts are higher. Presumably because there is a limit to the number of people that want to, say, contribute to governance in a DeFi protocol. I personally don’t feel it alters my behavior either way, but that is partially due to privilege. I’ve heard similar attitudes from other people in the crypto space that I assume are comfortable financially (a common demographic in DAOs).Varying payouts by Cred generated seems like the most sensible option to me generally. People object to it because they fear it will increase the incentive to game/farm. But from what I’ve observed (again, from a limited/biased perspective), I’ve not seen that fear justified in practice. It does however introduce variability into an org’s budgeting. Which could create additional governance labor if adjustments need to be made, and possibly execution risks due to less acurate financial modeling. Freakytainment:Increase in Inflow of Members ==> Increase in Total Pay Amount ==> Increase in Participation = Growthtldr; yes. Though unless DAOs can turn that growth into revenue (or some other form of sustainability), then Growth => run out of money faster.', ' s_ben:Danke. Curious, did the DALLE images inform the meaning of the words, or was it more just eye candy?To feed your curiosity , the images made the words more potent, nothing like eye candy here.', ' s_ben:Interesting you use the word subscriber. To me subscribers pay, instead of being paidMy bad, the word should be opt-in. That’s what I meant.', ' s_ben:I think it’s pretty obvious by now that money will attract contributors, and that SC would produce a higher ‘yield’ in the form of more contributions (posts, comments, etc.).yes it should, but that’s if the system is properly managed to utilize all available opportunities like the constant inflow of contributors.', 'You welcome…I’m always ready to dedicate my time, energy and knowledge to the crypto world in general', 'This is a great post @s_ben I really agree with the point of maintaining the initial stake amount, I feel when the stakes get increased, a few gotten to be more dependent on the fund and thereby take it as probably a job, and then when there’s this type of mentality, exploitation, extortion and greed comes into play and desires can move. This could be moderated to an expand by clear communication and desire setting. But people are gonna human. Particularly on the off chance that deeply invested in set up frameworks or attempting to oust them. So I’d simply stick with the option of maintaining the initial stakes and reward for the main time since SCRF isn’t an employing company that’s mandated to pay workers salary. s_ben:I think it will affect the ‘system’ regardless, especially if payouts are increased. And that ‘positive’ is highly subjective, so hard to measure unforch.@Freakytainment whatever has a positivity value must also be weighed with it’s negative side. When SCRF members are committed to the building and expansion of the system, they’d do it willing regardless of what ever is sent to their wallets. If there’s a continual increase in the stakes and the 5000 DAI is topped up everytime because there’s a constant inflow of members/contributors, there would also be a decline/crash in the system’s finance someday.We should know that sustainability still does not fit flawlessly into the trade case. Companies have trouble segregating between the foremost imperative vital openings and dangers on the skyline.A sustainable lifestyle is generally considered more expensive, unless everything is balanced and sustainability isn’t placed as a priority over other necessary aspects.images694×442 26.7 KB', 'This is nice write up @s_ben , wish to give my iwn point of view. I think a reward formula, which is independent of the Cred scores, would be the most obvious technique to use. Numerous strategies could be used to establish income stability. For instance, if each person’s weekly Cred score remains over a predetermined threshold, everyone receives the same amount of DAI.', ' s_ben:The most comparable SC instance would be MakerDAO, which has been paying significant rewards (~20,000 DAI/mo) for governance contributions on its Discourse for nearly two years. Contributors are generally happy with the instance.@s_ben I am curious, especially considering other responses referencing sustainability and an increase in payout and contributors, did you notice and positive or negative change with the change/reduction in payout?']"
                Research Summary: Design and Implementation of Blockchain-based E-Voting              ,https://www.smartcontractresearch.org/t/research-summary-design-and-implementation-of-blockchain-based-e-voting/1885,Mechanism Design and Game Theory,23,['https://cdn.jsdelivr.net/npm/@meck93/evote-crypto@0.1.10/report.pdf'],"['summary', 'governance', 'game-theory', 'oracles', 'flash-loans', 'defi']","['TLDRDeveloped and developing countries have adopted electronic voting (E-voting) to replace the traditional ballot box or postal method of voting. However, the requirement to be met to achieve voters’ privacy and the process verifiability is high.Research has been conducted into applying cryptographic techniques to achieve privacy and verifiability, but the solutions are usually hard to implement.The researchers propose an easy-to-use, secure, and verifiable design for E-voting on the Blockchain that requires minimal setup.Core Research QuestionHow can privacy and verifiability in E-voting systems be achieved without relying on a centralized server?CitationMoritz Eck, Alex Scheitlin, Nik Zaugg, “Design and Implementation of Blockchain-based E-Voting.” University of Zurich, Feb. 2020. https://cdn.jsdelivr.net/npm/@meck93/evote-crypto@0.1.10/report.pdf 1.BackgroundAccess Provider: An authorization service run by or on behalf of the voting authority that grants only eligible users access to the E-Voting system.Ballot Secrecy: A casted vote must remain private and unlinkable to the voter.E-Identity: A digital certificate issued by the government containing the unique ID of a voter.Identity Provider:  A trusted third-party responsible for validating and verifying potential voters’ eligibility and electronic identity.Individual verifiability (IV): An IV is assigned when a voter can ascertain that their vote has reached its intended destination without any alterations.Limited Votes: A voter can vote for a certain number of candidates out of a total number of candidates. For instance, a voter can choose to vote for two (2) candidates (the first and third candidates) out of a total number of three (3) candidates.Multi-way Elections: A voter can select one out of multiple candidates in an election.Proof of Work: A form of cryptographic mechanism primarily used by permissionless Blockchains such as Bitcoin or Ethereum to come to a consensus on the state of information recorded on the Blockchain or verify the accuracy of transactions added to the Blockchain.Proof of Authority: A consensus mechanism that gives some Blockchain actors power to validate transactions or interactions across the Blockchain.Receipt-Freeness: A voter must not be able to prove to a third party that they have cast a particular vote.Sealer: A service in the E-voting system to run a blockchain validator, participate in the distributed key generation, and tally votes once voting has ended.Secure Public Bulletin Board (SPBB): A public board used to broadcast a publicly verifiable log of communication of an ongoing election or vote and to store the final result.Universal Verifiability (UV): A concept that allows a third party to verify the outcome of a vote.Voter: Any eligible person who wants to participate in voting.Voting authority: The administrator and coordinator of the E-voting system. They are responsible for coordinating with the sealer, setting voting questions, opening and closing the votes, and deploying a smart contract.SummaryTraditional voting systems are frequently faced with voter manipulation, vote-buying, and human errors, which has led to governments seeking alternatives in E-voting systems.E-voting makes it easy for people with disabilities, people living abroad, or people in remote areas to exercise their right to vote.There is a general distrust and lack of acceptance of E-voting by voters. It is challenging to balance the protection of a voter’s privacy while also verifying that the vote was included in the ballot as intended by the voter.A common feature of existing E-voting systems is that trust is embedded in central authorities who control the system even though it is distributed or replicated.Research into cryptographic techniques has yielded fruitful results toward achieving a balance between privacy and verifiability. However, there is still a heavy reliance on centralized servers.Using centralized servers for encryption and proof verification requires the voters to trust the central authority, thereby defeating the purpose of decentralization on the Blockchain.Centralized servers also provide no ability for voters and third parties to verify votes since all the cryptographic operations occur on a central server.The researchers seek to tackle the shortcomings of E-voting systems on a centralized server by designing and implementing a new proof-of-concept E-voting system.The system consists of various stakeholders such as the sealers, the voting authority, voter(s), identity provider, Blockchain, and access provider who constitute the E-voting architecture.Interaction between stakeholders in the E-voting architecture:MethodUsing a proof-of-authority blockchain to act as a secure public bulletin board, client-side vote encryption and proof generation are employed and all votes and proofs are verified and stored inside a smart contract on the Blockchain.To create an E-voting prototype that is easy to use and requires minimal setup, the researchers chose a browser-based implementation. In addition, typescript was chosen to enhance developer experience and a type-safe implementation.For the prototype’s frontend, the researchers used React and MaterialUI, while the backend was built with Typescript and Node.js to avoid context switches.The prototype is then built on the Ethereum blockchain using the Parity client.To ensure fast setup and no installation, all the services were containerized using Docker which can be started through a single script.ResultsVote encryption and proof generation is performed by the client’s server instead of a central server.The E-voting system ensured voters’ ballot secrecy since voters encrypt their votes using the public key of the system created using each sealer’s public key as a share.A third party cannot gain information about a voter’s choice once the system has generated the encrypted vote and proofs, ensuring receipt freeness.Coercion resistance is not guaranteed if the voter and the coercer are in the same physical location, as the voter can be forced or manipulated to abstain from voting or give up their credentials.Fairness is guaranteed as the results of the voting exercise can only be accessed at the end of the exercise when all the sealers must have submitted their decrypted share.The system also fulfills the requirements for individual and universal verifiability.The eligibility property of the E-voting system ensures that only eligible people can vote and the voter’s privacy is private.The system is reliable as it is built on a blockchain that ensures redundancy and protects against data loss. Therefore, where one of the sealers malfunctions, the system will still be operational.Discussion and Key TakeawaysThe identity of a voter could be discovered by tracing their IP address to their exact location and the system cannot guarantee the privacy of the voter’s identity in this situation.Where the identity provider goes rogue, they can create their blockchain wallet and gain access to the voting system by trying all generated one-time tokens. Unfortunately, this activity may go unnoticed as not all eligible voters will eventually vote.If the identity provider and access provider collude, it would be possible to link a voter’s wallet with their E-identity, thereby removing the receipt-freeness property of the E-voting system.Implications and Follow-upsThe researchers did not enforce communication over secure channels due to time constraints.Due to the limitations of the Ethereum virtual machine, only 256-bit integers could be used which weakens the security of the system. Therefore, it is recommended to use 2048-bit integers for future work on production settings.The ability to trace a voter’s location by tracing their IP address can be avoided by incorporating onion routing between the voter and blockchain network.A scheme involving blinded voter tokens could be deployed to solve the problem of possible collusion by the identity and access provider.The E-voting system designed only supports elections with two options; multi-way elections and limited votes could be implemented in future systems.ApplicabilityPrivate and public entities looking to conduct free and fair elections devoid of common problems associated with traditional voting systems while maintaining a balance of privacy and verifiability.Private and public entities seeking to ensure that vote encryption and proof generation are done by the client-server instead of a centralized server in their electronic voting systems.', 'Hi @Tolulope. Thanks for an excellent contribution to the Forum. Here’s something I recently read about that made me think of your research summary:David Chaum, the legendary cryptographer and inventor of digital cash, has completed preliminary work on a system called VoteXX. It claims to solve the problems of remote voting.“VoteXX is a joint effort by xx network and an international team of academics to create a massively scalable, end-to-end voter verifiable, coercion-resistant and remote election system.”https://chaum.com/votexx/ 3https://votexx.org 1If this project is as described, will it change our reading of this paper?', 'Hello @Tolulope,Weldon for the research summary. I am thinking that Voting receipts could be  another transparency measure that can be used in E voting solutions to help assure voters that their ballots have been received by the voting server. Do you think it is necessary.', 'Thank you @rlombreglia for sharing David Chaum’s great work.The project will definitely change our reading of the paper and the entire e-voting landscape when finalised.While the architecture seems quite similar to the one designed by the researchers in the paper, it introduces ‘hedgehogs’ who can nullify votes with the keys provided by voters. The Votexx project addresses the problems of voter coercion and vote buying while using a fully decentralised model. The coercion resistance function is equipped to ensure that voters cannot prove how they voted, and the ability of both the voter and ‘hedgehogs’ to nullify votes would really go a long way in ensuring coercion resistance.However, I am curious about how the model would work with multi-way elections since it currently allows two options. Also, I feel the project focuses a lot on online coercion. I wonder how the model will mitigate coercion from physical forces present at the voter’s location and can watch the voter select a candidate and vote for them. However, that may not be a great problem since the voter is able to change their votes with the passphrase using the hedgehogs. I wonder if there would be a limit to the number of times a voter can nullify/change their votes and if the system will be able to flag such votes and set measures to ensure there is no ongoing manipulation.Overall, Votexx proposes solutions to many of the issues currently faced by e-voting systems while being inclusive of traditional systems.', 'Thanks for the comment @Henry. Could you provide more context into what you mean by voting receipts?', 'Nice summary @Tolulope.Despite the cutting-edge technology utilized in the implementation in this summary, I still believe that there are always security problems intrinsic to blockchain technology, thus does the blockchain system examined in this research  satisfy the fundamental security requirements of internet voting?I think the most significant limitationfor building secure internet voting systems is thatRegardless of how secure the voting system is, voting equipment could still be compromised or hacked into.', 'Voting receipt here could mean the identification code which proves that a vote is casted by the registered personLet me assume that a voter realizes that his vote is not counted correctly? How will he be able to prove it?', 'I love the focus of the summary, Tolu…I’ve always been curious about blockchain’s applicability in a country’s elections.Unfortunately, the system does not meet the criteria for practical use. I hope the researchers continue and subsequently improve the work soon.I believe then it could be used in elections, especially for African countries.', 'Thanks for clarifying @Henry. I think voting receipts would beat the point of the paper. The purpose of a blockchain-based e-voting system is to ensure ballot secrecy, among others. A voting receipt would negate that. It would also encourage coercion, voter manipulation and vote buying, which is what the design aims to achieve.', 'Fantastic summary @TolulopeSome of the problems that currently plague election systems may be resolved by blockchain systems. On the other hand, privacy protection and transaction speed are the problems with blockchain applications that are most frequently raised.I believe that remote voting security must be practical for a blockchain-based electronic voting system to be scalable. Additionally, transaction speed issues must be resolved. These issues make me believe that the frameworks need to be improved in order to be used in voting systems.', 'Thank you @Tolulope  for this wonderful summary. It came at time when my country,Nigeria just passed an electoral law wherein section 84 approved electronic transmission and registration of vote. While the law doesn’t forsee e voting , i believe it it will only take time before that happens and who knows, you might be called as one of the experts to advice the federal government on this.Considering the fact that election is a very serious and delicate issue capable of dislodging the unity of a country, especially  multi-national country, does this research or any other research known to you, provide mechanism to prevent identity provider from from accessing the e voting system when the identity provider goes rogue as pointed out in the research?Secondly, although secondary to the context and content of the research, to how extent does the research consider user friendly design of the system as to not widen the already existing digital divides in some countries, especially African Countries which has long history of election and voter manipulation.Finally,  deploying Blockchain in election will have implications to privacy of data subjects and thus conflict with data protection law viz: its immutability and the fact that the design proposed by the research has potential of revealing the identity of the voter and vote cast. Is there any available mechanism to remedy this?Thank you']"
                Research Summary - Understanding Security Issues in the NFT Ecosystem              ,https://www.smartcontractresearch.org/t/research-summary-understanding-security-issues-in-the-nft-ecosystem/1350,Privacy,54,[],"['summary', 'ethereum', 'privacy', 'network-security', 'iot', 'zero-knowledge']","['TLDRThe study investigated security in the NFT marketplaces and the broader NFT ecosystem through comparative, in-depth analysis of top NFT marketplaces.Thirteen critical security, privacy, and usability issues were discovered.These issues include counterfeiting, lack of seller/buyer verification, and a lack of transparency among many others.Core Research QuestionWhat are general security, privacy, and usability issues within the NFT ecosystem, and how do they affect the industry?CitationDas, D., Bose, P., Ruaro, N., Kruegel, C., & Vigna, G. (2021). Understanding Security Issues in the NFT Ecosystem. arXiv preprint arXiv:2111.08893. Available at: 2111.08893v1.pdf (arxiv.org) 13BackgroundNon-Fungible Tokens (NFTs): Unique, non-interchangeable digital assets on blockchains. NFTs have various use-cases. Typically they represent assets such as art, collectibles, land, and others.Fungible Tokens: Digital assets that can be easily interchanged. These include digital currencies like Bitcoin, and fiat currencies like US dollar banknotes.Executing an NFT: The exchange of an NFT between a buyer and seller.Smart Contracts: A program constituting ‘if’ and ‘then’ commands executed on a blockchain, for example, the transaction framework with which NFTs are bought and sold.Decentralized applications (DApps): Blockchain program(s) designed for the end user primarily on the Ethereum blockchain, or any other network capable of launching Turing-complete programs.Decentralized autonomous organizations (DAO): A group or community registered on a blockchain, bound by an agreement to be guided by rules encoded into a smart contract.The Ethereum blockchain: A blockchain network that is Turing-complete, allowing for the creation of smart contracts and DAppsERC-20: The first standard interface launched on the Ethereum blockchain, which allows for the creation and exchange of fungible tokens.ERC-721: The first accepted interface for the creation of NFTs on the Ethereum blockchain , that allows for the creation and exchange of tokens that are usually non-interchangeable.Off-Chain storage: The storage of an NFT off of a blockchain network, usually on a centralized server.On-Chain storage: The storage of an NFT on a blockchain network (within the storage capabilities of said network).Decentralized Finance (DeFi): A distributed and decentralized equivalent of the traditional finance system consisting of trading, lending, payments, and other financial services existing on a blockchain network.Non-Fungible Token Marketplaces (NFTMs): Digital platforms designed to create, buy, and sell NFTs through digital transactions. The transactions are made by the buyer with virtual currencies; thereafter authenticated by the NFTMs; and then completed via smart contracts.InterPlanetary File System (IPFS): A permission-less, distributed, and decentralized file system providing storage, and access to several documents, images, websites, links, and other data. The system is available “off-chain”; however, it allows for immutable permanent links in the blockchain.Hardware Wallets (HW): A device that stores digital assets like cryptocurrency, and hosts private keys permitting access to those digital assets. Common examples are the Ledger or Trezor wallets.Software Wallets (SW): A program that holds digital assets in storage in a downloadable format on a hardware device such as a computer or mobile phone. Consequently, SWs are easier to access than HWs but not as secure. MetaMask is an example of an SW.Two-Factor Authentication (2FA): 2FA secures transactions by forcing multi-factor identity proof and verification, usually consisting of a digital certificate security and a one-time password (OTP).Know Your Customer (KYC): The process and the policies used to identify and verify customers to prevent fraud or fraudulent activities.Anti-Money Laundering or Combating the Financing of Terrorism (AML/CFT): Sustaining the integrity and stability of the international financial system through specific processes designed to prevent money laundering and funding of terrorism such as KYC.Minting: The process of converting a file into a digital asset or signature, consequently becoming part of a blockchain network.Levenshtein Distance: Also known as “edit distance,” a string metric for measuring the distance between two sequences.Wash trading: Falsifying market activity. This is usually illegal and achieved by buying and selling (in this case) NFTs by the NFTM to create the perception of high sales and demand.Shill bidding: Influencing the bidding process of an NFT by creating a different account solely for the purpose of submitting inflated bids to influence and spur other buyers to bid higher.Bid shielding: Submitting artificially high bids to make an NFT unattractive to other bidders, and subsequently withdrawing the bid so that a fellow colluder can win the NFT at a lower price in a later auction.Broken chains: URLs that lead to IPFS gateways breaking when the gateway becomes unavailable.SummaryThe study provides a systematic overview of the NFT ecosystem, identifying the emergence of NFTs, their protocols, and prominent actors. It covers privacy, security and usability issues in NFT ecosystems.The multi-billion dollar ecosystem for the purchase and sale of NFTs has garnered attention not only from art collectors and gamers, but also from bad actors seeking to exploit security vulnerabilities.The study identifies 8 prominent NFTMs: Opensea, Rarible, Nifty, Axie, Cryptopunks, Sorare, SupeRare, and Foundation.They find security issues involving privacy, usability, and security are prevalent across the ecosystem.The study focuses on the 3 most prominent NFTMs, OpenSea, Rarible, and Sorare.First, they identify 13 security, privacy, and usability issues in the NFTMs (5 of which the study states were previously unknown). They also discover irregularities in NFTM implementations, revealing 5 security bugs in the three largest NFTMs: OpenSea, Rarible, and Sorare.These irregularities consist of two different types of NFT implementation contracts, which are: marketplace contracts and token contracts.The marketplace contracts are an interface between the user and the blockchain, while token contracts are implemented directly.The study also included the discovery and highlight of ways external entities could pose a threat to NTFM users.Finally, the study explains and measures malicious user behaviors such as wash trading, shill bidding, and bid shielding.MethodThe study segments the NFT ecosystem into prominent actors: NFTMs, external entities, and users.The analysis of NFTMs consisted of a qualitative and quantitative study collecting 3 types of data across the NFT ecosystem (metadata of NFTs, NFT-related events, and Discord chat messages in corresponding channels) between June 15, 2021, and August 15, 2021.The NFTMs were listed and data retrieved through API access, web scraping, and blockchain parsing while ensuring compliance with market restrictions. The analysis included inspection of Discord servers using specific keywords. A total of 31,000 messages were inspected across 9 channels. Information gathering for the security issues also included Discord investigations, reviewing NFTMs, reviewing previous public security incidents from news sources like blogs and technical reports, and other official NFTM documentation.Identification of issues with external entities involved a comparative analysis of NFTM platforms. The comparative analysis was essential to discover how many NFTs had broken links between their metadata URL and the metadata record (which comprises the image URL of the NFT). Only one third of the more than 12 million digital assets on OpenSea had valid metadata.A measurement study was employed to identify fraudulent user behaviors in NFT transactions. The process to decipher counterfeit NFT creation involved identifying the collections, images, and URLs and conducting computational analysis by measuring the Levenshtein distance (with a shorter distance indicating more significant similarity and a longer distance less similarity). The final stage required a perceptual algorithm and image hashing tool to compare images and detect similar and counterfeit images.Heuristic data modeling revealed trading malpractice such as wash trading, shill bidding, and bid shielding. The model applied to 13,556,332 assets and 353,629,018 events, creating four graphs: a sales graph, a bidding graph, a payment graph, and an asset transfer graph, connecting and revealing relationships and malpractice paths.ResultsThe study identified 13 issues as security, privacy, and usability concerns in the NFTMs.The study identifies 5 security bugs in 3 of the largest NFTMs (OpenSea, Rarible, and Sorare), three of which the identified parties had identified and remedied. The remedied security bugs remain undisclosed due to non-disclosure agreements signed by the authors.The analysis of images and metadata reveals that many old tokens are invalid and do not contain images; consequently, a high number of NFTs have broken chains. This conclusion was reached after reviewing 12,215,650 assets from OpenSea, which returned only 4,393,566 assets with a valid metadata URL.The findings reveal that 98.14% of wash-trade transactions reported point to Rarible, which the authors attribute to malicious users attempting to capture the platform’s $RARI tokens. OpenSea represented 1.71% of its transactions, with Sorare making up the rest of the total. OpenSea and Sorare showed 3,395 instances of shill bidding, while 492 instances of bid shielding involved 745 users across 113 collections on OpenSea (OpenSea and Sorare being the only evidenced platforms).Discussion and Key TakeawaysImportance of Industry Analysis: Analyzing issues in the ecosystem is necessary to prevent loss and maintain industry growth.Pertinence of Issue Resolution: The highlighted security, privacy, and usability issues of NTFMs will only continue to grow if not resolved.Increased Security Standard Across NFTMs: A higher security standard across NFTMs is emphasized in the study.Implications and Follow-upsThe study reveals several issues about the security of transactions and exchanges taking place between creators, buyers, and sellers on NFTMs. Considering the volume of transactions conducted in the NFT ecosystem, understanding security issues will make the ecosystem a safer and more environmentally friendly industry for investment.The notion of making a trustless environment more trustworthy is ironic in this study. History has shown that standardization follows the growth of almost every industry but has moved slowly in NFTMs and the broader blockchain industry.The influx of billions of dollars worth of capital and investments into the NFT system requires stringent compliance and standards, especially across platforms.The study is missing a compliance angle to proffering a solution to the underlying issues of the ecosystem within the main article. Where not entirely legal, security compliance and quality or operating standards should be interoperable within platforms. There is a massive gap in the ecosystem regarding this. The authors did not recommend solutions to the security and ecosystem issues uncovered.A further analysis of the top 15 NFT sales also reveals tax and legal issues.The lack of interoperability across NFTMs is highlighted and can resolve where regulation mandates compliance with standard measures.Accordingly, the discoveries made are essential for corrective measures in the future, especially considering the numerous possibilities NFTs offer.ApplicabilityThis comparative study can help develop patterns and metrics for the NFT ecosystem. Their examination of the NFTMs provides insight into how the ecosystem works and the underlying protocols of the markets.Buyers, sellers, makers, and marketplaces will benefit from paying attention to the security vulnerabilities outlined in this article, so they can develop ways to counter them and avoid financial loss.Data retrieval is needed to compare protocols, platforms, and marketplaces. The comparison allows metrics to be gained, stored, and used to create indices and parameters for future reference and correction.The taxes and regulations around NFTs are unclear. This study can be useful for regulators seeking to better understand the fraudulent behaviors and insider trading of bad actors, and the dangers to the public involved in NFT transactions.', '@LTTOguns Congratulations on this very timely research summary. I enjoyed reading it, but it also brought up a question that’s been on my mind for awhile.You clearly state that “security issues involving privacy, usability, and security are prevalent across the [NFT] ecosystem.” And yet we also learn that the top three NFTMs had a trading volume in excess of $10B USD in the month of September 2021 alone.How do you account for the willingness of investors to accept insecure systems and fraudulent behavior? Is it simply greed and FOMO ratcheted up by NFTM marketing, or are there other issues in the ecosystem itself? Do we need governmental regulation and oversight to prevent people from literally being robbed by high-tech charlatans?', 'I agree with Ralph, this was a great read and for me an important introduction to the NFT community’s issues with security. It seems like the wash trading they identified was primarily a means for generating tokens from the NTFMs, was there a sense of how much of a problem wash trading is for pricing? I’m not sure I’m wording that correctly, I’m curious about how inflated NFT prices are. Reading this, it doesn’t seem as crooked as I might have thought although they did look only seem to look at “blue chip” NFTs, in so much as there is such a thing, which might have a little more scrutiny on them as a category. I wonder how much of a problem wash trading and pumping and dumping is for NFTs from less reputable communities.', '@rlombreglia Thank you so much for your question. The introduction of Bitcoin back in 2008 was beleaguered with major controversy (even till this day). Early adopters of Bitcoin and the underlying technology, like other advancements, have always initially appealed to the illegal community with the likes of Silk Road and the other drug users and activities happening through the dark web[i]. Despite the controversy and challenges, Bitcoin beat all the odds, becoming a high-value digital asset, creating sudden millionaires, consequently creating a crypto mad rush due to FOMO - the fear of missing out on the next big thing. Therefore, the high trading volumes are a direct consequence of this FOMO, notwithstanding the prevalent issues. I do not believe this indicates investors accepting insecure systems and fraudulent behavior. Instead, I think such investors hope governance and regulation will catch up to the process; they invest early, hold, and hope to get significant ROI on their initial investments in this ‘economic drive’ cycle.[i] Why is bitcoin so controversial?. Bitcoin is a peer to peer… | by Unocoin | Unocoin’s Blog', 'Thank you so much @jmcgirk you raise an important limitation of the research paper. In their revision, the authors recently included the minting, listing, and trading of tokens, including other malpractices such as counterfeiting of NFTs. Another study conducted around the same period indicated that wash trading could be less prominent than initially estimated. However, we do recognize the enormity of these issues.Wash trading and ‘pumping and dumping’ of NFTs could be a significant issue on less prominent communities NFTMs. The authors, as you mentioned, focused primarily on the Top 3 NFTMs given community attraction and high sales on such platforms. Wash trading, for instance, would typically occur between colluding agents to inflate apparent price/volume[i]. Investigating the NFTMs with the highest volumes could consequently be indicative. A deeper analysis of related NFT wash trades in less prominent communities would be enlightening.[i] Arash Aloosh and Jiasun Li. “Direct Evidence of Bitcoin Wash Trading”. In: SSRN Electronic Journal (2019). doi: 10.2139/ssrn.3362153. url: Direct Evidence of Bitcoin Wash Trading by Arash Aloosh, Jiasun Li :: SSRN.', 'Thank you for this fantastic summary!  Considering the inherent risks that come with arbitrage, did the study point at any successful regulatory sandboxes or experiments that would be useful as examples in how to approach attempting to regulate (or not to regulate) the risk?I was fortunate enough to participate in helping the Mauritian government research and design their regulatory sandbox and by proxy the licensing that came after.  One of the most recent developments is that Mauritius passed the Virtual Asset & Initial Token Offering Services Act which aims to provide “a comprehensive legislative framework to regulate the new and developing business activities of virtual assets and initial token offerings.”With all that said, I am not advocating for “regulation” as a panacea for reducing market risk; on the other hand there does seem to be a benefit to creating more welcoming market conditions when there is clear legislation and regulation within a given market.  Considering all of the legal issues that have already surfaced with the NFT market with a lack of clarity on how much responsibility an NFT issuing platform has in ensuring no copyrights are being infringed; did the authors make not of the potential for some type of “scam fatigue” to emerge in which a lack of regulation within the market creates conditions in which market abuses kill the inertia in the market?image958×512 108 KBConsidering the available data suggests that interest in NFTs is waning, does the author give any indication as to how regulation could possibly help or hurt interest in NFTs?', '@Larry_Bates Thank you so much for your insightful contribution and sharing of your previous projects to shed more light on the summary. Your highlight of a limitation of this paper is also refreshing. The authors, unfortunately, did not recommend any successful regulatory sandboxes or experiments to mitigate the security risks despite the discoveries made or their in-depth perspective of the NFT ecosystem. However, this can be juxtaposed with the restriction encountered by the non-disclosure agreement signed by the authors vis-à-vis potential solutions employed by certain NFTMs. The entire examination of the NFTMs, external entities, and users inadvertently invites the discussion around standardization and regulation (holding all parties responsible for their actions).The NFT system, like all other decentralized systems, revolves around decentralization. The interference of a central system or method consequently defeats the whole concept. When individuals or parties conversely encounter the security, privacy, or technical issues accompanying the decentralized system, appeal then builds for regulation to mitigate risk. The crux of the matter then becomes how to resolve such issues in a decentralized manner. Meanwhile, the abuses could gravely affect the ecosystem, but we hope that with more research and development, clarity will begin to unfold for all stakeholders.', '“The NFT system, like all other decentralized systems, revolves around decentralization. The interference of a central system or method consequently defeats the whole concept.”I think this statement is internally consistent.  The issue becomes, the statement presumes an “all or nothing” state concerning decentralization; which is only necessary when a specific type of decentralization is not alluded to.  One of the running conversations on the forum is “what types of decentralization can occur”?In this context, with the understanding that NFTs seem to be unable to cross the mainstream adoption threshold without some sort of regulatory clarity which area do you think would be most pragmatic for decentralization?The contracts?  The network security layer?  The application layer?  The regulatory oversight?It doesn’t seem feasible to try to decentralize every single aspect of a system (unless I am missing something).', 'This is a brilliant summary. Well done @LTTOguns. I now understand terms like wash trading, shill bidding, et cetera.There is a lot of practical use cases of NFT, especially as a bridge between developing economies and developed economies. It is thus quite sad that the NFTs ecosystem is now attracting bad actors. Would you say that the ecosystem being susceptible to bad actors is a result of the flaw in its design? Is there a need for governance mechanisms to punish bad actors?I wonder if smart contracts could be created to enable the closing of the accounts of notorious actors.There is a popular Nigerian saying that goes like this, “Prevention is better than cure”. Hence, I will read the study to identify patterns of security issues in the NFT Ecosystem and try to avoid being a victim.', 'How many of these issues would apply for a blockchain like Chia, which has offers, essentially smart contracts that automatically connect a buyer to a seller (so long as they meet the agreed-on price)? It seems like a direct connection between buyer and seller might mitigate many of the trading risks.', 'Thank you so much @Austin_jul You raise valid questions. To answer your initial question, No… the ecosystems are not necessarily susceptible to bad actors as a result of a flaw in the design. As with the introduction of any new tech, bad actors generally have leeway to take advantage of users due to the initial lack of initial regulation and efficient governance structures. Despite existing regulations bad actors still ravage several tech ecosystems, and so without laws/governance what we are beholding now is the usurping of the gap created. Consequently, the creation of governance mechanisms is pertinent. With regards to Smart Contracts (SCs), I believe they will become more sophisticated as time goes on; especially with cohesion with artificial intelligence patterns and practices. At that point SCs could be able to identify and shut down a percentage of identifiable notorious actors.Thank you, and Yes! I concur with the Nigerian saying. As many Nigerians will say, it is ‘very necessary’.', '@jmcgirk Such automatic connections would likely mitigate trading risks as it would evidently cut out many of the manual manipulative processes. I believe that as smart contracts become smarter we will see a rebirth of them in a new sophisticated way. The convergence of smart contracts with AI and machine learning will be able to connect transactions in new ways including detection and consolidation of transactions automatically. The issue then will be to ensure automatic governance mechanism are ahead of machinations of bad actors.', 'What a beautiful summary at @LTTOgunsI’m particularly interested in the decentralization aspect of the NFT ecosystem. It is sad but true that NFT marketplaces are not fully decentralized, therefore giving marketplace custodians an edge to things like wash trading, front running, etc.Even though one can argue that NFTs are implemented on public blockchains and that the tokens are stored on chain, the files and documents associated with these tokens are stored off chain in centralized databases. These databases are a single point of failure amongst several other things that could go wrong.It is alarming that these security issues are numerous and multiply everyday. It is possible to mitigate some of these issues by implementing fully decentralized NFT marketplaces. Most of the NFT marketplaces present are not fully decentralized. A research into how NFT marketplaces store data by Jonty reveals and confirms that Beeple’s “Everyday: the First 5000 Days” is stored in a centralized database.', 'Thank you @Ulysses the single point of failure of centralized storage systems is actually quite rampant and notorious. The light weightiness’ necessity for decentralized storage systems continually fuels the need for off-chain storage. The ongoing innovations within the ecosystem will reveal sustainable solutions over time. Quite interesting to follow as you stated.', '@LTTOguns , the summary was really expository and informing especially on issues concerning NFT and the NFT marketplace.There have also been several severe security incidents as a result of the non-fungible token (NFT) surge. For instance, in March 2021 there were over 300% more domain registrations with the names of NFT retailers that had a questionable appearance. You need to have an active cryptocurrency wallet in order to participate in an NFT marketplace.Because attackers can access your crypto wallet through your marketplace account, this exposes NFT holders to new vulnerabilities.It is already believed that threat actors have even gained access to the OpenSea Discord server, an NFT marketplace, by impersonating support personnel and tricking targets into revealing account access.Some attempt to trick NFT holders into transferring money or divulging passwords by using traditional phishing techniques.I believe that the The NFT industry is still in its infancy, and as a result, both the opportunities and the risks are expanding.It is beneficial that NFT participants stay informed about security issues if they invest in NFT.', 'I appreciate the excellent summary @LTTOguns.The NFT market is reportedly still in its infancy, but demand growth will ensure its expansion.The future of NFTs is looking more promising as time goes on, but so are the growing security worries.From experience, by frequently updating to the most recent version of the software available for your device, you can make sure the NFT software functions without any issues. After the new update, it will fix the faults in the driver software, this will in turn reduce cyberthreats are the result.For the purpose of sale an purchase of NFTs, those who are new to it as well as those who wish to participate in it should be careful to choose the appropriate wallet in order to protect their NFTS.This is fundamentally one of the first steps towards addressing the security issue around NFTS in the market place. Looking forward to further research around this topic.', 'Thank you for your comment @Cashkid18 The opportunities and risks are truly expanding, hence the need for extensive research, and industry collaboration to discover and mitigate risks as well as expound the opportunities. The researchers in this paper push this further with their analysis of the top 8 Marketplaces in the NFT ecosystem, revealing underlying issues in these existing systems. The industry needs more awareness, education, and research by participants, investors, and organizations.', 'Thank you @Idara_Effiong. The security issues of NFTs will always be a worry. Same way traditional web1/web2 assets currently are. The key considerations should be regulatory intervention and establishment of penalties, punishment, and other deterrents. Other factors like new parties conducting their own research is of course pertinent as well.']"
                Discussion Post: Are Soulbound Tokens the best way to create a trusted layer on the blockchain?              ,https://www.smartcontractresearch.org/t/discussion-post-are-soulbound-tokens-the-best-way-to-create-a-trusted-layer-on-the-blockchain/1705,Governance and Coordination,72,['https://ssrn.com/abstract=4105763'],"['dao', 'defi', 'discussion', 'summary', 'governance']","[""TLDR:Soulbound Tokens (SBTs) are reputation primitives, allowing for the creation of decentralized applications that incorporate reputation, and would allow for developments such as partially secured loans, community-wallet recovery, or transportable credentials.Ohlhaver et al suggest a combination of SBTs and governance could eventually function as a bulwark against the hyper-financialization of web3 (warned against by Nathan Schneider) by creating a “decentralized society.”There is some dissent and controversy over the proposed method. Kate Sills, for one, argues that the permanence of SBTs and the subjectivity of their projected use cases (which she calls “claims”) make them an inadequate candidate for a trusted layer on the blockchain.At IssueAre Soulbound NFTs the best way to create a decentralized society capable of resisting the hyper-financialization of web3?CitationWeyl, Eric Glen and Ohlhaver, Puja and Buterin, Vitalik, “Decentralized Society: Finding Web3’s Soul” (May 10, 2022). Available at SSRN: Decentralized Society: Finding Web3's Soul by E. Glen Weyl, Puja Ohlhaver, Vitalik Buterin :: SSRN 3 or http://dx.doi.org/10.2139/ssrn.4105763 4Sills, Katelyn: “Soulbound Tokens (SBTs) Should Be Signed Claims” (June 06, 2022). Available at: Soulbound Tokens (SBTs) Should Be Signed Claims 10.DiscussionWe’ll need a trusted layer on the blockchain to grow much larger than we have. Without some index of counterparty risk, for example, it’s impossible to create a decentralized finance (DeFi) product that accurately prices an unsecured loan; without them, DeFi can’t compete with commercial paper or consumer finance. There are countless applications and widespread utility waiting to be unlocked by the adoption of a reputation primitive—in other words, a data object that can accurately represent social proof or a credit score in a smart contract.In May 2022, E. Glen Weyl, Puja Ohlhaver, and Vitalik Buterin released “Decentralized Society: Finding Web3’s Soul,” which introduced “soulbound tokens” (SBTs) to the Ethereum community. Soulbound tokens build on earlier work Buterin had done on a concept he had adapted from the game World of Warcraft. In that game, certain items are bound to a player and can’t be traded or auctioned off. Soulbound tokens represent memberships, credentials, affiliations or other relationships that are usually publicly visible and non-transferable.SBTs would be bound to a specific wallet, or a Soul, and might hold tokens representing “educational credentials, employment history, or hashes or their writings or works of art.” Souls would be able to issue tokens, each representing a testament to a particular relationship. Institutions holding wallets would also be Souls and could issue SBTs of their own, signaling proof of attendance or completion or acceptance by a whitelist.The authors have a grand vision of a decentralized society (DeSoc), a co-determined sociality “where Souls and communities come together bottom-up, as emergent properties of each other to create plural network goods” that would be enabled by broad adoption of SBTs. Like DeFi, organizations require a layer of trust. Consider how many backchannel communications are needed to arrange a successful vote in a decentralized autonomous organization (DAO) for example, or how much politicking goes into the successful execution of an office project.In “Cryptoeconomics as a Limitation on Governance” (May 2022), Nathan Schneider describes web3 as enabling an economization of everything, forcing every social interaction into a transactional frame and essentially flattening all relationships within a cryptoeconomic system. “To overcome these limitations,” Schneider writes, “designers should envelop cryptoeconomics within a logic of politics capable of seeing beyond economic metrics for human flourishing and the common good.”Ohlhaver et al imagine SBTs as a bulwark against the same tendency. SBT would enable a staircase of increasingly ambitious applications that would unlock provenance, unlock undercollateralized lending markets through reputation, enable decentralized key management, thwart and compensate for coordinated strategic behavior, measure decentralization and create novel markets with decomposable, shared rights. These components would knit together into a new kind of community, one far more equitable than the one we’re in now.Kate Sills, an engineer in the blockchain space, takes issue with using “public non-transferable tokens” for this purpose, and suggests a different approach. She worries that the most important of them would be issued by the large institutions that blockchains are intended to bypass in the first place. Permanent trust tokens would empower the issuers. Given that SBTs would be issued in response to a subjective issue—for example, “Are you qualified for a job/airdrop/loan”—Sills argues that using “a statement digitally signed with Ethereum private keys,” which she calls a “signed Claim,” would be a much safer option than using a permanent token.(0x00931d500eea10DcBD418ea2eBdE3C1DCa86564b)has been a good pet sitter over the past 5 years. By signing, I (0xcf9F9021e2594394b2A9d6115e9cc682d761368f)attest to this statement.Consider the way reputation works in a real community. Information accumulates and is context-dependent. People accumulate different levels of trust in different areas. These tiny vignettes are also relatively private—i.e., you can choose whether or not to share a story with someone.The beauty of a Claim is that it’s also context-dependent and private. Instead of creating a permanent stamp of approval or blackmark, the Claim provides a nugget of information. Rather than a permanent mark akin to credit score or a diploma, Claims would only be useful in the aggregate and would be ephemeral in that other Claims could supersede them. They would be private. You could only read a Claim if the signer let you read it. Web2 giants operated by callously hoovering up people’s personal information and repackaging it to marketers and advertisers. With Claims, web3 probably wouldn’t be able to do this. With SBTs, they might.The problem with Claims as Sill presents them is that they seem like they would drastically limit what could and couldn’t be done in smart contracts. Would a Claim-based dApp be possible or as powerful as a more persistent SBT-based one? Ohlhaver et al’s vision of a DeSoC world might not be possible without persistent and transparent tokens creating a layer of trust underneath.What do you think? Are there other ways around this divide? Might a differently designed chain with a more developed decentralized identity component be a better solution?Discussion QuestionsHow does identity persist with Soulbound Tokens?Should tokens be revocable and how might that best be implemented, particularly with respect to GDPR?What other research should be done when it comes to classifying existing community recovery processes, classifying potential types and parameters of SBTs, etc?How might the use cases differ with a privacy-bound claim and an SBT?What about other chains/dApps/protocols that have decentralized identity (such as Chia/Civic/Proof of Humanity/Bright ID) and peer-to-peer transfers as architecturally built-in components?Which methods/usages must (or must not) be developed to avoid the potential dystopias such as the cold start problem (which could reduce users’ equity and be against the ethos of decentralization)?Further ReadingOSF 4 (Nathan Schneider, “Cryptoeconomics as a limitation on governance”)Soulbound (Vitalik Buterin, “Soulbound”)Scott Kominers and Jad Esber, “Decentralized Identity: Your Reputation Travels With You” (Decentralized Identity: Your Reputation Travels With You | Future 2)"", 'Kate Sills’ conception of claims really maps nicely onto David Graeber’s description of debt and obligation in society, so to me it seems the most natural way of encouraging an emergent trust-based ecosystem. On the other hand, in terms of actual primitives and creating sophisticated debt-based financial instruments using crypto, I prefer something along the lines of Spectral.Finance which analyzes wallet liquidations and other activities to create a dynamic credit score.', '@jmcgirk Thanks for opening up this discussion,The Souls are very interesting model. I love the idea of SBTs as they could become the defaults in all of identity use cases. I am particularly interested in SBTs for educational credentials. I think that would be a game changer.However I would like to understand more about,a. Let us say, an educational entity such as Udemy or Ln Learning - awards an SBT for a successful completion of a course. This solves the problem of fake credentials. But how can we ensure that SBTs actually prove that the course is done by the ‘Soul’ in concern or was it someone else?b. How does someone recover these SBTs in case of wallet losses? or change in ecosystem (from one layer 1 to another)?c. If SBTs or claims, can be hidden from another souls based on need. It would defeat the purpose of souls in the first place. Let us imagine, each review about a restaurant on google is an SBT. We know that it is not fake, but what if the restaurant chooses to hide it?d. Web3 community for some reason, which is beyond me, like to remain Anon. Will the community embrace this?Having said that, as you clearly point out. I think there is immense potential.I also disagree with Kate Sills argument of blockchain needing to bypass large entities in this context. A reputation can be built ground up from a smaller institution as well. I fail to understand why is this relevant here. Can you shed some light @jmcgirk ? I think trust is a property of reputation. I do not think we can separate the two.', ' Yesh:A reputation can be built ground up from a smaller institution as well. I fail to understand why is this relevant here. Can you shed some light @jmcgirkI think she’s concerned with large institutions becoming the equivalent of the centralized institutions they were designed to replace in this context. She’s proposing what would end up being an aggregated contextual reputation of many different claims rather than a binary trust/not-trust which the presence or absence of a token would often represent. W/r/t trust vs reputation, in this context, I think of reputation as describing a persistent quality, the equivalent of a credit score, while trust is binary – approve/disapprove – so maybe trust is a second party’s assessment of reputation?', 'gm, please consider also what we’ve been doing with EIP-4973:What are Account-bound tokens? 1EIP-4973: Account-bound Tokens 3', '@TimDaub we’d be really grateful if you could give us a little summary of what they are, how do they differ from SBTs?', 'Thanks for writing the discussion post @jmcgirk, this is a topic I’m particularly interested in and it’s a good summary. I appreciate the elevation of Kate Sills’ critique, as well as connecting the topic to Nathan Schneider’s recent work.The general idea behind SBTs is sound, but there is a lot more work that needs to be done refining the concept. Reading through the summary, I am reminded of Buterin’s comments at a conference about two ago where he was asked what he thought was most important going forward, and one of the answers given was that a framework for attestations was crucial. I see SBTs playing a significant role in this context, which is aided by other protocols such as Sismo (What is Sismo? - Sismo Docs 1), which is building a framework for ZK blockchain related attestations.In this light, I don’t actually see two distinct proposals here, ie one is SBT, the other is signed claims. Rather, my interpretation is that Sills’ critique is about the side effects of reducing highly subjective contextual dependant interpretations down to objective facts, ie, turning signed claims into tokens to be quantified. Here are some key points she makes:“It’s important to note that an essential part of evaluating a claim is evaluating who made the claim, their expertise, their experiences, and their motivations. The process of evaluating a claim is very subjective and context-dependent. People will naturally disagree about the trustworthiness of other people, and trust depends on the context. For instance, you might not trust your broke younger brother to repay a loan, but he might be a caring and fun babysitter for your five-year-old.Because this analysis is so subjective and context-dependent, we should be extremely skeptical of any proposal to “calculate over” claims to get objective results… The thing that I really want to prevent is representing claims as tokens on blockchains.”****I strongly agree with the points she makes, and the dangers she warns of, though I am not convinced that the authors of the paper are encouraging pursuing this direction. It is, however, highly likely the industry will try to do this regardless, which makes her warnings apt.I think a better way to frame SBTs would be as a profile or account (like a forum account, except sign in with Ethereum that works everywhere) as well as a framework that holds attestations about the owner of the account. The owner should have full control over what is shared, and with who, however the attestations themselves are extrinsic. But attaching attestations to the ‘account’ allows for it to become a key which gives access to certain communities/contexts depending upon the attestations it holds (ie token gated communities).More generally, I think there are some underappreciated side effects. While SBTs reduce the surface area of vulnerability, it also concentrates the vulnerability in another area – i.e., who is signing attestations. If SBTs and signed claims make it harder for someone to lie about themselves, can they then get others to lie about them? A key area of research will be who gets to make what attestations, and what weight should be given to them. Objective facts will be relatively easy to attest to (for example, a University can easily provide a signed claim that attests whether a person has the qualification or not). The trustworthiness of that claim will be dependent on the trustworthiness of the institution, which will then need to be evaluated and that evaluation maintained. Centralised institutions that provide attestations will be an attack vector in this context.Small scale subjective attestations will be more complicated. Attestations about pet sitting was an example provided. On the surface, this seems good, but if an attestation like this is required (quantified), this provides an incentive for a bad pet sitter who wants access to bully someone into providing a false attestation. There are some extreme negative externalities possible in this scenario, especially in different contexts, but good system design can alleviate these problems. For example, when claims are subjective or the attestor is potentially unreliable, you want to show a consistent history of positive attestations in multiple contexts, along with the specific claim, rather than a single attestation about a single context. In this sense, the SBT becomes a proxy for a reputation that is impossible in the scale of the world we currently live in.On the topic of identity, one area where I differ from the authors is that don’t think the soulbound tokens should be bound to an address/wallet, but rather actually be literally ‘soul’ bound (i.e., to a person, or identity). The transferability of the token should be less important than the conviction it belongs to a single (ZK verifiable) individual – though accomplishing this and ensuring no duplicates will be hard.Identity is a core issue in this space, as the paper effectively spells out. However, there are different levels to the requirements for identity. For example, a creditor for unsecured lending needs to know both who they are dealing with for recourse in case of a default, as well as information about their past which will determine their capacity to meet the terms of their arrangement (ie credit rating). This will require a ‘hard’ attestation which includes proof of identity, and credit score. This will almost certainly need to be bridged into crypto from Tradfi, at least in the short to medium term.In the case of gitcoin, this requirement is less hard. Rather, they need to be relatively certain that each participant is a unique human being, to ensure that one person doesn’t just fund a proposal from multiple different wallets to take advantage of the quadratic funding mechanism. These are two different levels of attestation. The former is almost certainly going to require some level of KYC type identity, whereas the gitcoin example can also use an aggregation of smaller unrelated claims to conclude that someone is likely an individual person. And even if they are not, the amount of effort required to pass the checks makes easy abuse of the protocol much more difficult. The current implementation of this system can be found here: https://passport.gitcoin.co/ 1. One of the biggest challenges for the usefulness of SBTs will be figuring out how to securely attach identity.My final thoughts on this topic, for now, are that this design space will open up a lot of challenging tradeoffs that need to be navigated. The primary one will be the centralisation/decentralisation spectrum. Unless we are talking about attestations for data that is all onchain, we’ll be relying on legacy centralised systems to provide attestations, and we’ll need an oracle to bring this onchain. This will spark a lot of debate about what decentralisation means.', ' jmcgirk:reputation primitives jmcgirk:transportable credentialsI am curious how succession planning would evolve with SBTs. In real life, children and relatives sometimes draw on the positive (and even negative) reputation (or social capital) of parents and relatives. Is there a provision for this nuance in SBTs?I think this relates to your question about revocation. When a SBT holder dies, is their token revoked?What would this mean for maintaining the holder’s digital legacy?https://www.jstor.org/stable/256324Liz Lean PR – 2 Sep 21The importance of reputation in family businesses - Liz Lean PRUK PR and reputation management agency, Liz Lean PR (LLPR) talks about one of its specialisms, reputation in the family business sectorEstimated reading time: 10 minutes jmcgirk:Instead of creating a permanent stamp of approval or blackmark, the Claim provides a nugget of information.Isn’t that information permanently on the blockchain? Or how is a claim not a permanent stamp? Unless there is an update with a new signed claim? Further to this, can SBTs be forked? In case there are some irregularities?', ' dwither:is a unique human being, to ensure that one person doesn’t just fund a proposal from multiple different wallets to take advantage of the quadratic funding mechanism.Would instances of a Sybil attack only come up if attestation is decentralized? Example requiring a minimum number of attestations?If attestation is centralized, then this would not be an issue to worry about? @dwither', ' Fizzymidas:Would instances of a Sybil attack only come up if attestation is decentralized? Example requiring a minimum number of attestations?If attestation is centralized, then this would not be an issue to worry about?Attestations are generally a way to avoid sybil attacks. But it depends on the quality of the attestation. This is similar to the oracle problem, it really depends on the reliability of who is providing the data. Centralised entities already provide non blockchain attestations, ie universities with degrees, or governments with identity. Bringing this onchain securely would utilise what already exists. But consider the difference between a degree awarded by a low quality perhaps fraudulent university, compared to an ivy league school. Decentralised attestations would be similar, but with even more questions regarding who is providing the attestation. THe biggest limitation for decentralising attestations, is what they can actually attest to.', 'There are some really interesting “proof of life” / “proof of death” crypto startups that are trying to answer the question. On some level existing legal frameworks would probably apply in most cases, although if codes on a wallet, then any funds might be gone for good!@kelsienabben @quinndupont - what do you think of SBTs generally? I tend to agree with Kelsie and @ntnsndr’s critiques of cryptoeconomics generally, although I’m greatly encouraged from what I’ve learned about Gitcoin and other approaches to governance/economics.', 'Hey @jmcgirk ,I just learned a little bit about soul bound tickets yesterday in a another group I’ve been getting to know. I do see Soulbound Tokens a way to create a trusted layer on the blockchain. Because this is something that would greatly effect a persons wallet. Because once you have it will never leave your wallet. I see many of useful uses for this concept.A good way of use for this would be for attending events or attendance. Which can be used to show that you was present and its something that will always and forever be on your wallet address. It could also be used for verification and many more other applications. As a way to show seniority of your wallet and for employers in web3 space a good way to show case your experience and knowledge.The other way I see soulbound tokens being used is marking bad wallets or wallets that have been used for scamming. So when a wallet connects to a platform they can be banned from the soulbound token used to mark them. Making tagging wallets easier and potentially making a way for others to know the wallet address they are sending too has a been tagged for something mischievous. I do see soulbound tokens creating a trusted layer for a better blockchainOnly bad part I see in this for a soulbound token once its on your wallet. Its on your wallet forever. Which is the main and biggest concept of soulbound tokens.', 'Puja Ohlhaver, one of the papers authors, just posted a detailed twitter thread which expands on how we should see SBTs as ‘community keys’. It’s a very interesting thread which covers topics such as consensus, meaning & power. I summarised this idea briefly in my previous post (quoted below), but this is is a much deeper dive with explicit examples and well worth the read. dwither:I think a better way to frame SBTs would be as a profile or account (like a forum account, except sign in with Ethereum that works everywhere) as well as a framework that holds attestations about the owner of the account. The owner should have full control over what is shared, and with who, however the attestations themselves are extrinsic. But attaching attestations to the ‘account’ allows for it to become a key which gives access to certain communities/contexts depending upon the attestations it holds (ie token gated communities).There has also been further discussion 2 between Puja and Kate Sills on the topic of privacy. Sills argues that the papers authors are just “hand-waving away privacy as future work”, while she sees this as a show stopper for the idea. While I am in complete agreement with Sills on the importance of privacy, I do this see this as a solvable problem.For example, an SBT or profile might hold private information that one does not generally want to share with the public (we could use basic demographic data such as age, gender or address as an example). At the same time, there might be significant value in understanding the demographic profile of a community. It should be technically possible (either now or very soon) to build a ZK system where this data can be securely transmitted from an SBT with a verified identity that holds this information, and then aggregated with all other members of the community. The only data publicly available would be the aggregation, which is made up from the private data of individuals. I’d describe this as micro level privacy, and macro level transparency.This is just a simplistic example, however, there are a lot of complexities and nuances that would need to be explored to ensure privacy. For example, if a group of individuals is too small, some info might be individually identifiable if their data is enough of an outlier. There’s a whole field of academic research which explores these problems in real world research, which should be applicable in this context as well.', ' jmcgirk:We’ll need a trusted layer on the blockchain to grow much larger than we haveI have a basic question here. Why do we need an trusted layer when the blockchain itself is supposed to bring some trust?', 'That is a great philosophical question.  It would seem to be contradictory, however there is a very simple explanation: The blockchain itself can only validate whether the input into a transaction is valid within itself as having a consensus among validators.  There is no mechanism to validate if the INPUT is correct.  That is a seemingly minuscule, yet extremely important nuance of recording data. The only thing a blockchain can do is preserve the input.  There is no blockchain that can confirm real-world input as being “correct,” and that may never occur at the base blockchain level unless there is hardware developed in tandem.  Ultimately, the need to validate the input is why a Layer 2 solution becomes necessary.  Any given blockchain can only verify the input has having had a consensus that the input is “valid,” but not necessarily “accurate”.  It is this simple aspect as to why people can accidentally send the wrong amount in a transaction.', 'Thanks so much for this @jmcgirk for this, I’m always intrigued by your contents, well I have some little contributions/thoughts on this.Soul bound tokens are the future of Ethereumaccording to the founder Vitalik Buretin so I think much consideration should be put into considering issues related to it Soul bound entirely.Web3 and its biological system of decentralized arrangements have recorded significant development in a brief time, presenting a few developments that are challenging the standards in numerous businesses of the world nowadays. But indeed with such critical development, web3 has numerous impediments that are avoiding it from coming to the following level.A proper manner of soul bound tickets will be for attending occasions or attendance. Which may be used to expose which you changed into gift and it’s something with a purpose to usually and all the time be in your pockets to deal with. It may also be used for verification and plenty of extra different applications. As a manner to expose seniority of your pockets and for employers in web3 area a terrific manner to expose case you enjoy and knowledge. The different manner I see soul bound tokens getting used is marking wallets which have been used for scam job transactions. So while a pocket connects to a platform, they may be banned from the soul bound token used to mark them, this is better for controlling stuffs like that. Making tagging wallets less complicated, easy to use doubtlessly and securely, and creating a manner for others to recognize the pockets deal with they’re sending too has a tagged for something mischievous.', 'Thanks @jmcgirk  for the content you posted. Weldon job.I like the idea of SBT, it is really a big development because  Soulbound tokens or SBTs could unlock benefits that can transform how we currently view social identities in real life.For example, NFTs serve as a way to prove what a person owns, and the things they can afford, support content creators, and even improve crowdfunding efforts. However, beyond the incentive of money, SBTs improve upon the NFT concept to serve as a way for people to prove who they are and confirm their reputation.With SBTs, users can observe the immutable history of people before conducting business with them. For example, SBTs can be used to confirm the experience of a candidate before employing them. A person can observe the SBTs of people before leasing a property to them. The upside opportunities SBTs can potentially bring to the world are quite enormous and they can even enable the realisation of a completely decentralised society.In addition, the SBT, its issuer, and Soul’s address can be publicly verified on the blockchain. Hence, anyone can easily track or prove the trust circles of a particular Soul.Imagine having a blockchain account (Soul) that stores immutable records like educational credentials, employment history, or works detailing a person’s experiences.Even Verification via the trust layer proves that any data is tamper-free. It can be verified and traced in detail by independent third parties at any time.Hope it doesn’t constitutes a breach of data protection laws.', 'Sally Sills’ critique of the SBT proposal argues that SBTs reduce highly subjective contextual dependant interpretations down to objective facts. I think a better way to frame SBTs would be as a profile or account with attestations about the owner. The owner should have full control over what is shared, and with who. SBTs reduce the surface area of vulnerability, but also concentrates vulnerability in another area - i.e., who is signing attestations. A key area of research will be who gets to make what attestations, and what weight should be given to them.Centralised institutions that provide attestations will be an attack vector in this context. Tradfi’s white paper on the gitcoin protocol aims to ensure that each member of the community is a unique human being. Identity is a core issue in this space, as the paper effectively spells out.', 'EIP-4973’s are Soulbound tokens. We just renamed them to Account-bound because it is the more precise term and because we didn’t want our work to collide with the more philosophical texts of Weyl et al.']"
                Research Summary: Attacks on Smart Contracts              ,https://www.smartcontractresearch.org/t/research-summary-attacks-on-smart-contracts/1826,Auditing and Security,27,['https://helda.helsinki.fi/bitstream/handle/10138/345427/20220607_Porkka_Pro_Gradu_FINAL.pdf?sequence=2'],"['summary', 'network-security', 'scalability', 'testing', 'defi', 'flash-loans']","['TLDR:We present an up-to-date survey of smart contract security issues and their mitigations. We achieve this by combining the top research on smart contract security with cutting edge open source knowledge and information.We find that almost all of the most common attacks could be avoided or at least severely mitigated by following better coding practices. This includes general principles as well as specific design patterns against given attack types.The most common smart contract attack types are explained on a technical level and suggested mitigations against each one are given.CitationPorkka, Otto. “Attacks on Smart Contracts.” (2022). https://helda.helsinki.fi/bitstream/handle/10138/345427/20220607_Porkka_Pro_Gradu_FINAL.pdf?sequence=2 12BackgroundSmart contracts expand the utility of blockchains to distributed state machines, where anyone can store and run code and then mutually agree on the next state. This added utility also brings up many security challenges which are of the utmost importance given the money involved with smart contracts.As a brand new and constantly changing field, the need for better coverage and understanding of smart contract security issues is increasing. A major portion of smart contract research and innovation also happens outside of the scientific field in online communities, so getting an up-to-date view on smart contract security, combining cutting edge open source knowledge and previous scientific work is needed.As a master’s level thesis, smart contracts and blockchain technologies are explained on a technical level in the background section so very little prior knowledge is needed to understand the topic.Important terminology:Smart Contracts: Autonomous accounts living in the blockchain with the code that dictates their behaviour. When triggered by a transaction, the smart contract’s code is executed on the Ethereum Virtual Machine (or, if not Ethereum, the blockchain’s native environment).Ethereum Virtual Machine (EVM): Ethereum’s execution environment. Also, a general term for the distributed state machine that the nodes of a blockchain network forms. It maintains the blockchain’s state and allows the execution of smart contracts. If a smart contract code is executed, the nodes execute and validate the code and mutually agree on the next state.Transaction: A request to update the blockchain state. Includes value transactions and smart contract invocations. Always requested by externally-owned accounts, i.e. accounts controlled by a human/private key, although smart contracts can invoke other smart contracts via transactions once executed. Transactions are mined into a block once executed.Gas: Fee to be paid to nodes for executing a transaction and including it in a block.Mining/Miner: Mining is the action of forming the next block in the blockchain. Miners are the nodes that form the blockchain network and maintain and update the state of the blockchain and EVM via a consensus algorithm. (In this thesis we refer nodes of the network generally as miners even though they are not always technically “mining” if, for example, Proof of Stake is used)Consensus Algorithm: A way for miners to mutually agree on the next state of the blockchain. Proof of Work is the most used one, Proof of Stake is making its way.DeFi: Decentralized Finance. Financial systems built with smart contracts that operate on blockchains in a decentralized manner.Attack Type: Used as an umbrella term for all the attacks and vulnerabilities that are covered. This is to make distinction between anattack, which refers to a specific action,vulnerability, which refers to a flaw in the code or technology,attack vector, which refers to a general pathway or attack surface that exploits use.Research Question or Use CaseWhat are the most common attacks on smart contracts and how can they be mitigated? In other words, what is the current state of smart contract security?SummaryIn this thesis we give an up-to-date survey of smart contract security issues. First we give a brief introduction to blockchains and smart contracts and explain the most common varieties of attacks and ways to mitigate them. Then we sum up and analyse our findings.Covered attack types include:Re-entrancyTimestamp DependenceFront-Running (First in general, then split into sub-types)Transaction Ordering DependenceBlock StuffingBlock Reorganization AttacksInteger Overflow and UnderflowDoS With RevertInsufficient Gas GriefingForcibly sending Ether To A ContractOracle ManipulationAfter each attack type, we present suggested mitigations. These are split into three classes: better coding practices, automated tools and technology changes. Better coding practices include coding practices and principles that can prevent specific problems or help design secure smart contracts in general. Automated tools includes a range of tools from IDE expansions and code analyzers, to full-on fuzzing and testing software. Lastly, technology changes refers to changes to the underlying blockchain technology, i.e., making changes to a protocol.To conduct the analysis, all covered attack types are categorized based on the location of the vulnerability (smart contract code / technology level) and the type of suggested mitigations (better coding practices / automated tools / technology changes). This categorization can be seen in the table 4.1 below.table_4.1961×681 89.2 KBAfter this, all the findings and results of the categorization are summed up and analysed a bit more to give a big picture of the current state of smart contract security.Key findings:Almost all common attacks could be avoided or at least severely mitigated by following better coding practices.This indicates how crucial good coding practices and expertise of the coders are when designing and implementing secure smart contracts.Immutability and the open nature of the blockchain make developing secure smart contracts difficult.Many automated tools for security are being developed and put into practice.This indicates movement towards more conventional coding where automated tools like scanners and analysers are used to cover a large set of security issues.Making changes to the underlying blockchain technology is usually not the best way to counter issues, even if it can work in some cases. It is hard to do, can restrict the usability of contracts and requires community coordination.Many attacks combine technological property with a vulnerability in the targeted smart contract’s code.Many challenges and dangers have to be considered when writing secure code. Developers must have a good understanding of blockchain-related threats and technology itself.They must also be aware of conventional threats, given that smart contracts are Turing-complete. (For example, integer overflow and underflow are really common outside the smart contract world).For some issues and exploits that target smart contracts, there is nothing that can be done on smart contract level.For example, block reorganization attacks are done purely on the blockchain-level even though the majority of profits are extracted from smart contracts. If someone alters blockchain history, the smart contract can’t do anything since its state is also reversed.Another good example is web interfaces. Even though they interact with smart contracts, if there is a problem with the web interface itself, the contract can’t do anything. (Interfaces and programs outside the blockchain interacting with smart contracts are outside of scope of this thesis.)The biggest challenge when making this survey was using open source information as reference material. Many sources needed double-checking and required finding scientific sources as backup.When it comes to the future of smart contract security, the Proof of Stake -consensus algorithm and changes it brings are one of the most urgent topics. Another interesting topic involving security is MEV extraction, which is closely tied to front-running covered in this thesis.ReferencesThere were many interesting articles that came along when writing this thesis. The Bitcoin whitepaper and Ethereum documentation are recommended for anyone who hasn’t read them already. To truly understand security issues, background knowledge about blockchain technology is an absolute necessity. For Ethereum, their development documentation is an especially great source, although their whitepaper is also interesting read:Here are links for those:https://bitcoin.org/bitcoin.pdfEthereum development documentation | ethereum.orgEthereum Whitepaper | ethereum.orgFrom the developer’s point of view, ConsenSys hosts a great github page with relevant information on smart contract security for anyone writing Solidity. It doubles as a great summary of smart contract security:Ethereum Smart Contract Best Practices 2For a more technical database of smart contract vulnerabilities, SWC Registry is a great source:https://swcregistry.io/ 2(Note: These latter two may not be actively maintained)', '@ode Thank you for joining us and welcome to the forum!I wonder if it is under the scope of your research to study the best practices for building secure administrated contracts proposed by another recent summary on the forum 2?They’ve suggested three best practices…deferred maintenanceboard of trusteessafe pausing…so in the case when the access of administrated smart contracts are compromised there is a fallback plan.In what brackets (better coding practices, automated tools, and technology changes) would you consider these practices to fall into?', '@Twan Thanks, happy to be here!This would indeed fall under the scope of my research, although I didn’t talk much about mitigating damages after an attack or in case of malicious smart contract administrators.I would rank all these under better coding practices since they are all enforced on the contract level in the code. They are all also essentially design patterns, which help to mitigate a specific problem. In this case, the problem being the owner of a smart contract having too much power.I would also say these practices follow the principle of preparing for failure, which indicates that “contract code must be able to respond to bugs gracefully due to the lack of patching schemes” and that “the contract must be able to pause to avoid further financial losses”. These are from a great article, “Smart Contract Security: A Software Lifecycle Perspective”, from 2019. Smart Contract Security: A Software Lifecycle Perspective | IEEE Journals & Magazine | IEEE Xplore.Other similar examples of design patterns I also mentioned in my thesis and that would fall under better coding practices are pull over push -pattern and commit-reveal -scheme. First one means it is better to let users withdraw instead of contracts sending ETH and the latter one is useful in mitigating front-running in contracts that take submissions against rewards.', '@ode Thanks for a nice research summary. I have been hoping to see a collated attack/security issues summary on smart contracts just like the one on NFTs by @LTTOguns.From this research, three methods were listed for solving issues of security on smart contracts. It is important that we also put into perspective the ease, cost, and impacts of these various methods before deciding on the one to deploy for solving particular issues.Again, most, if not all, smart contracts are audited by auditing companies, like Certik, before being deployed on blockchains. In essence, these auditing companies are supposed to ensure that codes follow due process and that potential harmful bugs are detected.Sincerely, while reading this summary, I was kind of bewildered. The summary stressed the fact that better coding practices could help smart contracts avoid the various attacks listed in the research summary.So, I am finding it a bit difficult to reconcile these two ideas : auditing by security companies and better coding practices. Does it mean that auditing companies whose job is to find these bugs and code errors are not efficient? I am trying to wrap my head around why the problem is a persistent one since the infrastructure to handle this issue is in place. I ask this knowing at the back of my mind that these codes are audited before the smart contracts are deployed.', '@ode Thanks for providing a very interesting summary of your thesis!In particular, your analysis of front-running caught my attention. As noted, front-running is a revenue-extraction phenomena in DeFi that originates from traditional finance. And, as you state, it is closely related to MEV. Actually, I would conjecture that most, if not all, MEV trading methods actually originate in traditional finance, e.g. front-running, liquidations, arbitrage etc. which all extract revenue from the regular user.Now, in traditional finance the broker takes advantage of an informational advantage to perform front-running trades at the market, thus exploiting the soon-to-be price change from the received trade. However, this type of frontrunning was made illegal through regulation. Hence, I am curious as to whether you could imagine “regulation” as a fourth pillar of mitigation strategies besides BCP, T, and TC?Further, in traditional finance, new methods of frontrunning arose with the rise of electronic trading in the 90s and 2000s: High-Frequency Trading (HFT). These traders attempt to gain informational advantages regarding trades mere microseconds before the broader market and then trade on this knowledge. This advantage can, for instance, be achieved through placing servers close to stock exchanges or purchasing access to brokers’ dark pools. This small time-advantage allows these HFT traders to perform front-running trades and arbitrage. This type of frontrunning is very similar to the type of front-running observed in DeFi, where trading bots gain an informational advantage through observing the mempool and then trade on the speed (or order optimization) advantage they achieve through attached gas fees. In general, it seems that DeFi and several types of attack vectors are mirroring modern traditional finance. Do you think some future types of attacks can be predicted/prevented by means of lessons learned from traditional finance?', 'I just stumbled on a detailed post 1 made by @lnrdpss on auditing and it provided an answer to my original question here. Sharing it here for anyone who, perhaps, desire some answer too.', '@Ulysses Hey and thanks for the comment!I see you found an answer already, but here are some of my thoughts anyway that I was just about to post These mitigation types listed here are used only for categorization and analysing. “Tools” and “technology changes” are straightforward, but for everything else I had to come up with a common term so “better coding practices” is used for that. “Better coding practices” then includes everything that can be done codewise to mitigate problems, i.e., using design patterns that are proven to be good, following principles to make contracts secure or simply having enough expertise to spot possible bugs and vulnerabilities.As I see it, security audits are there to enforce exactly that. If some bug or vulnerability went unnoticed when coding, the security audit should spot it. And if it doesn’t, it would indeed mean that the audit was not efficient.However, I think much also depends on the attack itself. Attacks like re-entrancy or timestamp dependence are easy enough to spot and (probably) not that common anymore, but for attacks like front-running or oracle manipulation it is not that simple. This is because the vulnerability is not in a specific piece of code, but in the design of the system as a whole. Take oracle manipulation, for example. The vulnerability might be in the fact that anyone can update the oracle at any time, but from the audit point of view the contract can still be flawless and working as intended. A situation that could still be avoided by using proper data validation, for example.In the end, I would say audits heavily overlap with “better coding practices”, but not necessarily all the way. This leads to a situation where good coding practices are needed especially in system design and audits are there to make sure more technical bugs and vulnerabilities do not go unspotted. (Although some audits might spot design flaws also.)', '@windr Hey, thanks for kind words and great question! I’d say that regulation would fit perfectly as a fourth type of mitigation. Laws are already used everywhere else to prevent malicious acts so why not in crypto. But then we will of course face the ideological debate about crypto regulation and if it should be done at all. Personally I’m currently more on the “code is the law” -side but law things are not really my area of expertise so I can’t pick sides. This thesis was also done from a purely technical point of view so legal things fall out of scope.What I can say is that in front-running things often fall into the grey area. For example, arbitrage ensures better and more accurate prices for everyone using DeXes, even though it is technically a frontrunning attack. In theory frontrunning is also fair since anyone can do transaction bidding and transaction pool is readable by anyone. However, in practice it is just like in HFT and traditional finance that a small group with best AI bots and/or access to private pools dominate the scene and reap the profits, so I’m not sure how I feel about that. And some frontrunning like stealing submissions or sandwich trading are only bad for common users.What comes to the future types of attacks, I feel that it is more than likely that we see the same kind of attacks and malicious acts as in traditional finance as we implement things from TradFi on Defi. Hopefully people designing these systems are aware of these issues too and can adjust accordingly. I’m not that familiar with the finance world so can’t really say more than that.For podcast fans and those interested in frontrunning and MEV extraction, Uncommon Core had a great episode last year where they interviewed an anonymous MEV searcher/extractor. (Other episodes are of course recommended too, really good crypto stuff in general): Interview with a Searcher - with MEV Senpai and Hasu - YouTube', 'I completely agree on the ambiguity of frontrunning, since, as you mention, arbitrage is a necessity for a well-functioning financial system. Just as in TradFi where arbitrageurs ensure efficient markets at (almost) all times. I suppose my biggest worries are related to the pool of searchers/extracters/MEV-bots who reap the profits at the expense of “regular” network users through e.g. sandwich attacks. Actually, I recently read a funny blog post 1 about these issues (though it is arguable painting a bleaker-than-necessary image of the future). Nevertheless, it will definitely be of importance to solve these issues, and I guess projects such as flashbots might pave a way forward.', 'Beautiful explanation @ode, I couldn’t have asked for more. The differences are explicit now. Thanks for your time.', 'Thanks for the summary. It was really a great insight into smart security issues and it’s mitigations.Blockchain technology’s cutting-edge smart contracts have applications in practically every sector.It is therefore crucial that the implementation of smart contracts is safe against attacks aimed at stealing or interfering with the assets since they deal with and transfer high-value assets.A smart contract is sophisticated but fragile as the name suggests.It can be vulnerable to hostile attacks and reentrancy attacks, as well as to coding errors and protocol problems, among other manifestations of its susceptibility.Similar to how there is no one trustworthy way to guarantee the security of any technological innovation, I believe that there is no one dependable way toto guarantee the security of smart contracts and mitigate the attacks on smart contracts just as you mentioned in the summary.It is believed that when a trustworthy blockchain development company creates a smart contract, it is believed to be safe and secured.We risk getting into a lot of trouble if we deploy smart contracts too rapidly and without the proper security checks and audits carried out.']"
                Research Summary: Analyzing and Preventing Sandwich Attacks in Ethereum              ,https://www.smartcontractresearch.org/t/research-summary-analyzing-and-preventing-sandwich-attacks-in-ethereum/1033,Mechanism Design and Game Theory,54,['#link-4'],"['front-running', 'summary', 'mev', 'governance', 'notable-works', 'oracles', 'flash-loans', 'game-theory', 'discussion']","['TLDRToken trades on decentralized exchanges can change an asset’s market price. Bots generate a profit by frontrunning these trades, buying the asset for a low price, and then selling it for a high price.In this work, we present an analysis of such sandwich attacks for twelve months. During this time there were at least 480’276 attacks in Ethereum leading to an accumulated profit of 190 million USD.By splitting frontrunnable trades into multiple transactions, traders could have saved more than 90 million USD. We present a tool that helps them find an ideal order split.Core Research QuestionHow common are sandwich attacks in Ethereum and is there a way to prevent them?CitationZüst, P. (2021). Analyzing and Preventing Sandwich Attacks in EthereumLinkpub.tik.ee.ethz.chBA-2021-07.pdf 831327.07 KBBackgroundDecentralized Finance (DeFi): A set of finance-focused blockchain protocols that enable users, among other things, to take out loans or exchange tokens.Decentralized Exchange (DEX): Allows users to trade crypto tokens without the involvement of a central intermediary.Constant Product Automated Market Maker (AMM): Currently the most widely used implementation of a decentralized exchange. The basic idea is to keep token funds in liquidity pools. Traders are then able to swap tokens in a pool, as long as the product of the token amounts is constant before and after the swap.Uniswap: One of the most popular decentralized exchanges. More than 3 billion USD in token value locked and the basis for other exchanges like SushiSwap or PancakeSwap.Network Fees: Incentive for miners to include a given transaction in a block. Needs to be paid for every Ethereum transaction. Amount depends on the computational complexity of the transaction, as well as the chosen gas price.Exchange Fees: Cost of using an exchange. On Uniswap it’s 0.3% of the input amount of a swap.Slippage Tolerance: Token prices can be volatile. The slippage tolerance indicates the maximum price increase a trader is willing to accept in a given token swap.Mempool: Nodes store pending transactions locally in the mempool before they are included in the blockchain. The time a transaction is pending depends on the chosen gas price and the available block space.Transaction Ordering: When assembling a new block, miners are free to select any transactions from the mempool and arrange them in any order. The traditional strategy is to include the transactions with the highest gas prices in descending order, but there is no requirement to do so.Frontrunning: When an attacker observes a pending transaction, they can broadcast a new transaction with a slightly higher gas price. If a miner orders transactions strictly according to the gas price, the attacker’s transaction will be executed right before the original transaction. This mechanism has been studied in traditional markets for decades and is usually prevented by regulation.Backrunning: Same principle as frontrunning, but the attacker chooses a slightly lower gas price. This is how they try to ensure that their transaction is executed right after the victim transaction.Miner Extractable Value (MEV): Since miners have full control over how they arrange transactions, profit through frontrunning is considered miner extractable value (MEV). This term refers to the value that miners can generate by adding new transactions to a block or favorably ordering pending transactions.Sandwich Attacks: A swap on a decentralized exchange can lead to a significant boost of an asset’s market price. Attackers continuously monitor the mempool to find a transaction that entails large price differences. They then release a frontrunning transaction to buy the given asset, and a backrunning transaction to sell it for an increased price.Proxy Contracts: Transactions in a sandwich attack are typically sent to a proxy contract instead of a router offered by the exchange. This allows attackers to check whether the respective liquidity pools still hold the expected amount of tokens before executing the swap. If this is not the case and an attack would not be profitable, the swap can easily be canceled.Flashbots: A software project which allows users to submit suggestions for block compositions directly to miners. This turns MEV extraction from a central process into an open market where bots continuously search for the most lucrative way to assemble a block with pending mempool transactions.SummaryBots continuously scan pending Ethereum transactions and employ different tactics to profitably frontrun them. A common type of frontrunning is the so-called sandwich attack.We created a large-scale analysis of sandwich attacks for a period of twelve months. During this time there were at least 480’276 attacks on AMM DEXes in Ethereum leading to an accumulated profit of 190 million USD for attackers.Our paper also shows that miners have recently begun to play a more active role in these value extractions which drastically changes the patterns we observe for sandwich attacks.Splitting up frontrunnable trades can be a valid mitigation strategy. We explain how traders could have saved 90 million USD by releasing multiple smaller swaps instead of one large trade.A public tool to check whether a transaction is susceptible to sandwich attacks and to find a suitable order split was released on www.DeFi-Sandwi.ch.Finally, we initiated a survey to understand how users perceive sandwich attacks and what mitigation strategies they know. While all participants had a clear understanding of the attack, most of them didn’t know how common they were nor how they could be prevented.MethodWe first provide some mathematical background to understand how a token swap affects an asset’s market price, and how the profitability of a sandwich attack is determined.The paper then describes the heuristics used to classify sandwich attacks. The approach differs from related work, as it mainly focuses on the input and output amounts instead of the sender addresses.Next, we outline the structure of our data analysis. We used a modified Geth client to export all receipts of Ethereum transactions from May ‘20 to May ‘21. This dataset was filtered to only include receipts where a swap event was emitted. We then look for two transactions fulfilling the given heuristics for a sandwich attack.The discovered attacks were further analyzed, allowing us to learn more about the strategy attackers used, the profit they generated, and the sandwich opportunities they missed. A special emphasis is put on sandwich attacks which are executed in collaboration with miners.After describing the current state of sandwich attacks in Ethereum, the paper suggests a method for traders to prevent being attacked: By splitting a token swap into multiple transactions, a sandwich attack becomes unprofitable. We introduce the mathematical concepts to find an ideal order split.A web interface was developed which enables traders to check whether a given trade is susceptible to sandwich attacks. If this is the case, the tool suggests an ideal order split. We use backtesting to quantify how much money traders would have saved by using this trading strategy.Finally, we present our findings from a user study around the perception of sandwich attacks.ResultsNumber of Attacksattacks_test1920×1440 184 KBIn the given period, we analyzed 2’367’980 blocks using specific heuristics. In total, we discovered 480’276 sandwich attacks. The graph above shows how sandwich attacks became much more common over time.Involved Token PoolsOverall, the sandwich attacks we discovered made use of 5387 different ERC-20 tokens. The most popular pair was ETH-YELD which was attacked 3498 times. Around 55% of the involved token pools were attacked ten times or less.Proxy ContractsWe found 964 different proxy contracts that received at least one attack transaction. Attackers appear to switch their proxies frequently, as a contract is only in use for two weeks on average (90’913 blocks). The most active proxy contract processed 51’475 of the attack transactions we discovered (5.36%).ProfitabilityTo make statements about the profitability of attacks, we focus on transactions where at least one of the two involved tokens is ETH (which is the case for 96.28% of attacks).Accumulated Profitprofit_test1920×1440 99 KBThe accumulated profit over time can be seen above. The profit started increasing rapidly in July 2020. This coincides with the total number of sandwich attacks we are observing. Although the number of attacks stayed high through fall 2020, the profitability of the individual attacks decreased, most probably because of increased competition. The surge of profit at the beginning of 2021 could be connected to attackers collaborating with miners. In total, attackers earned 64’217 ETH (189’311’716 USD) in the given timespan. This also includes unprofitable attacks which constitute 18.14% of all attacks. If unprofitable attacks were excluded, the profit would amass to 73’337 ETH (216’197’476 USD).Maximizing ProfitOur analysis showed that attackers almost always achieve the maximum possible profit, i.e. they choose an ideal input amount for the buy transaction and push the price to its limit. The minimum output and the actual output of the victim transaction differed by less than 1% on average.Most ProfitableThe most profitable attack with a single victim transaction occurred on Feb 17, 2021 on Uniswap V2: A known sandwich bot released the respective frontrunning and backrunning transactions, netting a profit of 39.17 ETH (100’626 USD) in a single attack.Least ProfitableThe ten most unprofitable attacks all happened on Dec 19, 2020, and seem to stem from a misconfiguration of an attacker. They lost at least 219 ETH (645’612 USD) in less than 90 minutes.Allowed Unexpected Price Slippageslippage_rate1920×1440 105 KBThe figure shows how the selected slippage rates are distributed. Especially the share of transactions with a slippage tolerance of more than 10% is notable. The official Uniswap V2 interface suggests values between 0.1% and 1%.Active Reordering by Minerstable1108×288 125 KBWe consider a sandwich attack to be in collaboration with a miner if the gas prices of the frontrunning and backrunning transactions are both at most 1 Gwei. There was a surge of such attacks since the beginning of 2021. This timing coincides with the release of the Flashbots project. The table shows how increased control over the block order leads to a surge of profitability for sandwich attacks.Unused Sandwich Opportunitiesunused_opportunities1920×1440 98.8 KBIn total, we found 3’612’343 swaps with ETH as input token that could have been profitably attacked, but were not. The diagram shows how the share of unused sandwich opportunities declined over time. The largest missed opportunity for a sandwich attack could have earned an attacker 724 ETH (2’134’352 USD). It is likely that some of these transactions were never broadcast to a public mempool.Backtesting Order SplitsTo analyze the effectiveness of order splits as a mitigation strategy, we made a selection of 226’905 profitable sandwich attacks. The analysis showed that 160’347 (70.67%) of these attacks could have been prevented if a suitable order split had been used. In these transactions alone, traders lost 42’504 ETH (125’301’792 USD). Applying the order split strategy would have saved them 30’525 ETH (89’987’700 USD).User Perception of Sandwich Attackssurvey1920×1440 127 KBTo conclude our study, we conducted a survey with experts of the Ethereum community. While almost all participants agree that DeFi users can be negatively impacted by sandwich attacks, a clear minority considers the attacks immoral. Two people even brought up the point that sandwich bots should not be considered attackers.Discussion and Key TakeawaysData Analysis: We used block analysis to investigate sandwich attacks on decentralized exchanges in Ethereum. Compared to related work, we apply heuristics better suited for the current DeFi environment and present more recent and extensive data.Order Splits: We established order splitting as a mitigation strategy in a theoretical framework, verified its effectiveness using backtesting, and created a public tool that suggests ideal order splits for given trades.User Perception: Our user study showed the large information gaps around sandwich attacks and confirmed the need for continued research in that area.Implications and Follow-upsThe paper suggests that in the future every profitable frontrunning opportunity will be used, potentially harming traders and challenging the current architecture of decentralized exchanges.Our work also shows that the number of sandwich attacks executed in collaboration with miners is rapidly increasing. It can be assumed that, in the long run, all profit generated through sandwich attacks will be made by - or shared with - miners.Projects like Flashbots enable frontrunning bots to directly influence the transaction order in a block. This allows for attack schemes far more sophisticated than the sandwich trades highlighted in our paper, potentially leading to a steep increase in MEV extraction.There are large information asymmetries around frontrunning. Whether or not sandwich attacks are immoral is a controversial topic. As frontrunning becomes more prevalent, the Ethereum community will have to think about what this new paradigm means for the platform.We are currently adapting the given report for publication in a scientific journal. It will be extended to include an additional user study focussing on the perception of sandwich attacks.The Ethereum platform and the DeFi ecosystem are undergoing rapid changes. Extending the block analysis for a longer period will reveal how sandwich attacks are influenced by different factors, like Ethereum’s switch to proof of stake, the release of Uniswap V3, or the continuous proliferation of the Flashbots project.The tool we built is a research prototype and there are several possibilities to make it more user-friendly. Apart from improving the design, it could be extended by including multiple exchanges, or functionality that lets users directly sign and broadcast the suggested order split transactions.ApplicabilityWe released a web interface on www.DeFi-Sandwi.ch 27 which is based on the mathematical principles presented in the paper. Users can check whether a given trade on Uniswap V2 can be sandwich attacked. They see how the input amount, slippage tolerance, and transaction fee impact the profitability of the attack. The tool also suggests an ideal order split, such that sandwich attacks won’t be profitable anymore.Various services were developed which allow users to submit transactions directly to a miner. Using these private mempools, traders can circumvent the risk of being sandwich attacked. Examples include the Taichi network and 1inch’s Private Transactions.The growing number of sandwich attacks also led to the development of new decentralized exchanges. They are tailored to an Ethereum network where frontrunning is not an exception, but the norm. Archer Swap, Mistx, Cowswap, and Swapswap were all built with MEV extraction in mind. They use different approaches to share profits generated through frontrunning with traders.As users become more aware of sandwich trades, they want to know whether they were ever attacked. Sandwiched.wtf is a tool that lets users check whether they ever fell victim to such an attack.', 'It’s a wonderful summary and a very nice tool 8! @PatrickZuestI have a question about the backtesting transaction. Was sent directly to the router of an exchange data source? Because if the first router of the price is different from the second access to the router, then it would need to recalculate result.', '@PatrickZuest you mentioned in 4.3.3 that although you’ve seen hundreds of unique users on good days, you have not found any traces of these trades or the suggested order splits in the actual blockchain.I’m curious about the details of how you are matching unique users and their traces of order splits?My wild guess is that you would collect their input at your website, and see if any transaction taking place shortly afterward did a split that matches the advice you gave online.If it’s that case, how do you deal with the following scenarios which could lead to these errors:Alice, after learning about the risks from your website, ended up not trading with any pool at all  (Type II error).Bob, after taking your advice, decided to do small transactions. However, the actual transaction amount was different from what he fed to the website (Type II error).And once you do observe someone taking the advice:Someone didn’t visit your site but made a split that matches a data entry collected from Alice, Bob, or someone else (Type I error).Just as importantly, thank you for the research, website, and wonderful summary.', 'Hi @Sean1992076 - thanks for the nice words and your excellent question!This is a good point and we solved it like this: At first, we are checking the original token amounts in the liquidity pool. When we simulate a transaction, we also simulate the change of tokens in the respective pool (because every swap can have an impact on the relative token prize). We hence only need to check the pool content once, but each transaction’s impact on the liquidity pool is taken into account. Hope this makes sense!', 'These are great questions and I appreciate your feedback, @TwanWe logged the input data as well as the suggested order split amounts on the website. And you guessed right - we tried to match our suggested order splits with input amounts observed on chain (+/- some margin).Let’s distinguish the specific scenarios you asked about:Alice, after learning about the risks from your website, ended up not trading with any pool at allUsing our analysis, we can’t tell whether this happened or not. However, Alice didn’t actually use the order split our tool suggested, so I wouldn’t consider it an error.Bob, after taking your advice, decided to do small transactions. However, the actual transaction amount was different from what he fed to the websiteEven if the total input amount is different, the first split transaction would be the same. You can think of that number as the largest input amount that you can trade without being sandwich-attacked. In our analysis, a match on the input amount of the first split transaction would have been identified already. However, we couldn’t find any ‘ideal’ input amounts on-chain.Someone didn’t visit your site but made a split that matches a data entry collected from Alice, Bob, or someone else.We (unfortunately) didn’t have to think about that scenario, since we didn’t find any matches. If we had found matching entries, we probably would have compared the timestamps of our logs on the website with the actual execution of a transaction in the blockchain. There is of course no way to make conclusions with absolute certainty, but if an order split was suggested shortly before the transaction was executed (and if even the total input amount matches), it would at least be a strong indication that our website was used.I hope this clarifies things a bit. It would be interesting to redo the analysis now, as the awareness of sandwich attacks (and of our tool) increased quite a bit since the publication of the thesis.', '@PatrickZuest Thank you for your wonderful summary and tool. I’m looking forward to the result of the redo as well. I wonder how do Uniswap V3 and Ethereum 3.0 affect the applicability of the tool? Did you do or expect to do any adjustments for it to gear to the environmental change? Thanks!', 'Happy to hear that you liked the summary, @Astrid_CHAnd thanks for bringing up these questions: Our tool currently only works for Uniswap V2. In V3, calculating the output amount of a swap is far more complicated than on V2: The liquidity can be concentrated on certain price ranges which means that we cannot directly calculate the results of the swap, but would have to simulate it. So far, we haven’t made the necessary adjustments for the tool to work on Uniswap V3. If there is enough interest in the community, that might be something to consider.The upgrade of Ethereum itself won’t affect the applicability of the tool, as the underlying smart contracts stay the same. However, it might have an impact on the way MEV is extracted. Flashbots wrote a blog post 4 about MEV on ETH2 which you could also find interesting.', 'That’s really interesting and worth studying. @PatrickZuest, thank you for answering the questions and sharing the blog post.', 'Excellent summary @PatrickZuest!Great to see so many excellent questions popping up.My question relates to the mechanism you devised to mitigate sandwich attacks, whereby a swap is split into many orders. In the case of sandwich attacks, you were able to show how an optimal order split drastically decreases the profitability of an attack.What is interesting to me is that there appear to be additional benefits in splitting CFMM orders beyond sandwich attack mitigation. A couple of months ago, we had a great discussion with @tarun on the privacy properties of popular CFMMs, such as Uniswap.His publication introduced the idea of using order splits to improve the privacy of CFMM users. They propose a model which samples order splits for every swap using a Dirichlet distribution. This makes it incredibly difficult to associate a set of swaps to a single blockchain pseudonym, thereby increasing privacy.Although this is a very different model than what you have devised, I wonder if there is a way to potentially combine the two and build a quasi Order Management System for CFMMs that can offer both privacy and sandwich attack mitigation.Is this something you have considered before?A Note on Privacy in Constant Function Market Makers Privacy    I was actually just about to dive into the new publication – it was featured in last week’s Research Pulse (a curation of the top papers posted that week). Appreciate the mention, thank you. URE also seems to be highly complementary to strategies that decrease the profitability of Sandwich Attacks, whereby a single swap is split into many using a relatively straigtforward algorithm. Is this something you have considered in URE’s design? Yes, in fact, for the worst case input trades, such …  ', '@PatrickZuest – Thanks for this very well-written summary of your research, which shows that during a twelve-month investigation there were “at least 480,276 attacks leading to an accumulated profit of 64,217 ETH (189,311,716 USD).”Given the huge toll taken on the community at large, I was particularly struck by the “User Perception” section of your paper, in which you state that:• Despite considerable effort, it was difficult to find traders who were aware of sandwich attacks and willing to fill out a survey.• Despite clear agreement that DeFi users and the Ethereum platform were negatively impacted by sandwich attacks, a clear majority also felt that these attacks were not immoral.• That even experts are unsure about the actual impact and frequency of sandwich attacks.How do you account for this seemingly irrational reaction to the clear facts of overt dishonesty and corruption plaguing the DeFi community?', 'Thanks for the feedback, @cipherix. I agree – it’s great to see so many smart discussions and questions in this thread.That’s a very interesting idea which I haven’t considered before. As @tarun mentioned, our tool currently suggests a deterministic split which does not help users in terms of privacy. (In fact, it might make it even easier to associate different swaps to a pseudonym.) But it seems like there is definitely potential to combine the two ideas: If trades are split up anyway, why not do it in a privacy-protecting way? Would be interesting to include a functionality in a future version of our tool which takes these findings into account. I need to have a closer look at the math they use, but will keep you posted.', 'Thanks for bringing up the user survey, @rlombreglia. So far the focus in this thread was mainly on our quantitative analysis, but I agree that it’s important to also consider the perception of users.In order to understand their reaction, it is important to consider the time our survey was conducted. As prices were highly volatile, most users were not too concerned about receiving slightly fewer tokens from a trade than expected. This would have probably been different if prices were more stable. We also noticed that many DeFi traders had a lack of understanding (and also a lack of interest) regarding the detailed functionalities of decentralized exchanges. We were curious to learn more and extended our work in that area. Among other things, we conducted several qualitative interviews and a workshop on Clubhouse. A respective paper is currently under review for publication, and I will post it here, as soon as it is out.Whether or not these attacks are dishonest is a difficult question. When users set a slippage tolerance for their trade, they agree to swap tokens for a given price. Most users, however, are not aware that other factors than natural price fluctuations can have an impact on their swap.My personal opinion is also split: These attacks definitely won’t help attracting new users to the DeFi world. It should be easy and safe to swap tokens on a DeX, even for non-technical users. However, I also believe that there are examples of dishonesty and corruption in the DeFi world far worse than what we describe. Regular arbitrage trades are not too different from sandwich attacks.', ' PatrickZuest:My personal opinion is also split: These attacks definitely won’t help attracting new users to the DeFi world. It should be easy and safe to swap tokens on a DeX, even for non-technical users. However, I also believe that there are examples of dishonesty and corruption in the DeFi world far worse than what we describe. Regular arbitrage trades are not too different from sandwich attacks.Thanks for your answer, @PatrickZuest. You’ve helped expand my understanding of the issues.', 'It’s an excellent summary', '@Mehrdad_Khosravi was there anything in particular in Patrick’s summary that was particularly useful or interesting to you? We like to use the comments section for discussing scientific research and industrial applications of scientific research, so we’d love to get your input.', 'This is a great analysis of your research and a great explanation of the logic behind the methodology.  I am curious to know if there is a failure rate associated with the separated ordered transactions either due to gas fees not staying the same during the attempted execution or sandwich attacks occurring on transactions that are just large enough to flag bots?This type of tool could be extremely useful for solving many aspects of network congestion from the user side, so that is what makes me want to know the success rate of the transactions that get broken up and if there is an upper limit on how much can be transacted before bots get flagged.', 'This is very interesting @PatrickZuest! I might have missed it, but did you release your code anywhere on Github or anything like that?', 'I see that the comment section is filled with comments from people that have much more background knowledge than I have on this. This is a good opportunity for me to delve into a field I’m totally unfamiliar with, so I have some noob questions/observations. So basically a sandwich attack starts with someone selecting a target transaction that has a large potential for changing value, purchasing it, and selling it back for a profit. From an ethics perspective, it seems really difficult to clearly define the ethicality of this type of attack considering that most (Ethereum community) experts don’t even classify it as an attack. It’s not like exploitation of a bug like in the infamous case of “The DAO” from 2016, where money was actually stolen from its investors and where the hackers clearly had the option of informing in advance (and thereby preventing) themselves or others from doing so.Recently I’ve been reading up a little bit on the 0x DAO - this is the second tidbit of research I’ve done on DAO’s so far. 0x was founded by Will Warren and Amir Bandeali. Headed by 0x Labs, this DAO is focused on trading ERC-20 tokens like MKR, BAT, REP, etc. I wonder what sandwich attacks look like in the context of 0x. Besides for sandwich attacks, what other types of frontrunning attacks are there? Thanks for providing this awesome summary of your work!– Incidentally found out we have the exact same primary programming languages - Java, C/C++, and Python - and research areas of CV and (of course) blockchain.', 'A great write up, but I’m just going straight up to my question. Well I firmly believe that there are many worse instances of dishonesty and corruption in the DeFi community. But how can this be stopped  ??']"
                Discussion Post: The Metaphysics of NFTs              ,https://www.smartcontractresearch.org/t/discussion-post-the-metaphysics-of-nfts/1739,Mechanism Design and Game Theory,70,[],['discussion'],"['Discussion Post: The Metaphysics of NFTsLink to sourceThe Metaphysics of the Metaverse 15Content type tag (summary, discussion)DiscussionCategoryMechanism Design and Game TheoryProposed tags#NFT #dNFT #smartcontracts #mutability #discussionKey ProblemHow to represent paradoxes of identity that can be depicted in the real world as actions on NFTs on a Blockchain.  The following copy of my article published on LinkedIn describes the problems and an outline of proposed solutions using Smart Contracts and Oracles.Specific QuestionsHow to express and implement the arbitrary mutability of objects represented as NFTs.  Representing all objects as composites of NFTs corresponding to molecules, atoms or quarks is not practical nor scalable, neither is representing all space as Planck length voxels.  However, actions and potential actions on objects representing the real world require more fidelity than pre-defined components.Approach / PaperThe Metaphysics of NFTsby David Edelsohn, IBM ResearchHow does one represent the Ship of Theseus paradox (also known as the Grandfather’s Axe paradox) with NFTs in the Metaverse? If one replaces the axe blade and the axe handle, is it still the same NFT?The Ship of Theseus paradox is a foundational principle of the metaphysics of identity that dates back to Heraclitus and Plato circa 500 BCE.  If one replaces the boards in a ship (or the axe blade and axe handle of an axe) is it still the same ship (or axe)?  If the objects depicted as NFTs on a Blockchain cannot represent the paradox, it cannot represent and emulate reality.The Metaverse digitally represents both real and virtual objects on a Blockchain. Paradoxes that exist in the real world must be equally representable through NFTs and operations on NFTs.  An object represented in the Metaverse must be arbitrarily and dynamically divisible or fusible with other objects on demand, as determined by the “laws of nature” in which the object exists in the respective Metaverse. While NFTs are immutable, the objects that they represent are not immutable and the NFTs must reflect the transformations that occur to the real object or are permitted by objects in the Metaverse.Consider a shirt in the Metaverse for which the owner chooses to move a button to another shirt in the Metaverse, or a thread, or a speck of dust, or the impurity in one silk thread. Or consider a container of Himalayan salt for which the owner wants to separate the sodium chloride from the 84 other trace minerals.  It’s not practical nor scalable to divide space in the Metaverse to the Planck length volumes or to track the all of the components of objects at the subatomic level.  And it’s also unnecessary.  Instead of tracking the composable objects at arbitrarily fine and/or limited granularity, an operation associated with the NFT must provide the ability to respond to a request for an arbitrary transformation.If the rules for permissible actions were associated with, encoded in or calculated by NFT Smart Contracts, NFT Smart Contracts would permit fusion or fission of an object on demand, when queried, and at the granularity requested. The Smart Contract would allow the Blockchain network and validators to divide, combine or copy the NFT, as allowed by the contract, to represent the transformation of the corresponding object, and to generate a new object or objects representing the final disposition of the transformation, ideally at the coarsest granularity that can accurately represent the final state.  For example, an NFT representing a quantity of Himalayan salt would be transformed to an NFT representing a chemically appropriate quantity of sodium chloride and an NFT representing an equally appropriate quantity of trace minerals, not N mole NFTs of NaCl and M mole NFTs of KCl, Q mole NFTs of Mg, etc.[1]The Smart Contract program (possibly in conjunction with an external “oracle”) would determine if an object transition is allowed using a combination of predefined decision tree, predefined allowed transitions, predefined rejected transitions, predefined transitions based on the identities of the entities in the transaction (principals and agents), transitions based on cached information from recent, previous transitions, transitions based on cached transitions from other, similar objects handled by the Blockchain miner or validator, transitions based on cached transitions from other, similar objects handled by the Blockchain network, transitions based on artificial intelligence (AI) inference, and transitions based on an external, central authority. The Blockchain network would agree, through consensus of multiple miners or validators achieving the same result, that the transformation is valid.On a private Blockchain network, the decision making authority is clearer and the decision making process for permitted transformations is simpler (e.g., consensus unnecessary), but the scalability issues driving a more efficient solution remain. It is not practical for the Metaverse to represent and track the finest granularity of every object.  Video games follow the same principle to only generate what can be seen when it is observed.  The Metaverse must follow the same principle of object representation: NFTs should be created and correspond to the coarsest granularity that represents the object and its ownership necessary for the required, observable action.The current NFT ecosystem provides the concept of fractional ownership or fractional value, but that corresponds to portions the NFT representing the object or a quantity of an unchanging object, not characteristics or components of the object itself mutating.  The ecosystem also has been extended to composability of NFTs, but applied in a predefined, large granularity sense. Operations on NFTs (Smart Contracts) must intermediate and arbitrate these operations on demand.[2,3,4,5]An object in the Metaverse has properties that can be bought, sold, rented, lent, transferred, loaned, moved, or copied to one or more other objects based on the rules defined for the object.  If the property is not shareable, then the object loses that component.  If the property is shareable, then the entity receives a copy with a subset of the permitted, shareable properties.  The allowed sub-properties of the object can be determined by the creator of the class of objects, by the owner of the object, or by consensus among the NFT and/or Blockchain market (Metaverse) in which the object exists.Components that are shareable allow NFTs to represent non-exclusive behaviors.  A work of art can be loaned to a museum, giving the museum the limited rights to exhibit the work of art, and everyone who enters the exhibition gallery of the piece of art is granted the limited rights to interact with the artwork without the other rights of ownership.  A hotel guest is granted the limited rights to use the contents of a hotel room represented by an NFT, which is a fraction of the hotel, without owning a fraction of the hotel and all of the devolved benefits, liabilities, and responsibilities.The NFTs on the Blockchain represent the state of ownership of objects or components of objects. The Blockchain is updated when a transition of ownership occurs, but the arbitrarily complex object represented by the NFT can be acted upon in ways that don’t change ownership and without an action that triggers an event on the Blockchain.  An agent or entity can inquire of the Metaverse to materialize an arbitrary view of the object, and multiple agents can observe different aspects, characteristics, and granulatiries of the object simultaneously.  For example, one person in an art gallery can view an entire painting while another person examines a specific brush stroke of the same painting.This behavior of multiple views that hypothetically could result in a transition can be expanded as a method of optimization where the Metaverse can proactively allow multiple agents to propose an action on an object that would cause a change in ownership of the object.  If the actions are not mutually-exclusive, any or all eventually can complete.  If any of the actions are mutually-exclusive, then the object must conform to the “laws of nature” for that Metaverse and only one action succeeds.  In a large, distributed Metaverse environment, this permits speculative actions concurrently proposed by many agents with the hope of non-conflicting success instead of serializing the activities of the agents, which makes the entire Metaverse less scalable and less responsive.  Examples of this include an e-commerce purchase where the online vendor speculates that an item with limited supply remains in stock and available until the moment of purchase, when database ACID properties must be applied so that the transactions are self-consistent in the ledger.The human perception of the environment requires an observer.  Computer vision needs an embodied observer to interpret its input.  Some optical illusions rely on human assumptions about the geometry and composition of the real world mapped to our visual perception.  Each person applies his or her own attention and focus to perceive the world differently, from his or her own perspective.  The Metaverse similarly needs a mechanism to integrate and record the multitude of individual perceptions of objects.  The objective reality is one representation of the object, but each individual may need to perceive and interact with a unique, private aspect of the same object, with more or less granularity, depending on his or her attention, biases, and actions.  The Blockchain for the Metaverse is providing the underlying information (objective truth) for a multi-agent simulation where each agent may require a different view or aspect, i.e., a different granularity of the data.  Some agent inquiries and actions refer to the composited object and some agent inquiries and actions require the individual components of the object or a composite of multiple objects.Even when the object cannot or should not be reconstituted, the Metaverse and Blockchain network can maintain a cache of the lower-granularity view constructed while proactively scanning for fragments of objects that could be merged.  The presentation to the user may correspond to a different configuration and composition than the object or objects represented by the NFTs. The implementation of a Metaverse as NFTs on a Blockchain must continually balance the amount of granularity of an object represented on the Blockchain versus the amount of granularity encapsulated within the object.  A single object in the Multiverse could be represented as a composite of multiple NFTs or an internally granular and complicated object could be represented as a single NFT.  Depending on the actions or transformations on the object, the ownership, the history of the object, and any proactive scavenging, each object can have an arbitrarily complicated relationship with the NFTs associated with it.  The relationship between the representation or materialization of objects and the NFTs on the Blockchain must dynamically adapt to the ownership and the actions being performed; it’s not always a one-to-one mapping.With the ability to exchange and/or replace tokens based on the mutability of the underlying object (fusion and fission), it is computationally more efficient for the Metaverse to proactively scavenge and/or reap objects whose underlying representation is fragmented (individual components unlikely to be used individually in the near future or separate ownership no longer relevant).  Each NFT should correspond to the maximal complete subgraph of the components of the object that is necessary and sufficient for the operation to be performed on the object and the ownership of the components.This capability of the Metaverse provides an important motivation to incorporate AI into the Blockchain, NFTs, Smart Contracts and the Metaverse.  It is impractical to predefine all transitions and equally impractical to request the intervention of a human expert for all transition. The mechanism for determining the outcome of the NFT Smart Contracts needs to scale with the number of objects while providing “answers” in a timely manner to maintain the transaction velocity of the Metaverse.  This can be an important use case for Neuro-symbolic AI that is able to reason about the objects in the Metaverse, their behaviors and the requested transitions.  It needs both the knowledge base of “common sense” about the objects in the Metaverse and also needs to understand the transitions that are attempted so that it may successfully navigate the requests without damaging the value and consistency of the particular Metaverse in which the objects are represented.The Blockchain will track the history and origins of the components, but the linear ledger nature of the Blockchain introduces some challenges.  Incrementally appending and merging each component scales with the number of components, which is expensive at fine granularity. Building up a complicated object by hierarchically merging pairs of objects remains expensive and is expensive to search if the history must be analyzed. An efficient representation requires an NFT token that can represent a hierarchical merge that is equivalent to the hierarchical graph of the components of the object itself.As NFTs and the Metaverse confront the complexities of representing real world objects and the inherent philosophical paradoxes of reality, NFTs must incorporate transformations that mediate between NFTs and the metamorphosing objects that they represent.  Smart Contracts or other algorithms and characteristics associated with the NFT that allow an NFT to be replaced by another NFT representing the fission or fusion result of such a transformation on demand, when the object is required to mutate in a manner, is an efficient and scalable solution. This approach will allow NFTs to efficiently represent the hierarchical graph of components of an object and the complex set of operations and transformations that can be performed on the object at any level of granularity permitted by the Metaverse in which it is embedded.The complicated relationship between composited objects in the Metaverse and the NFTs representing the objects on the Blockchain opens significant and exciting opportunities for additional innovation in this ecosystem. Let’s build some ships![1] 1 mole = 6.022 x 10**23 elementary entities.[2] ETH EIP 864 Divisible non-fungible tokens.[3] ETH EIP 998 Composable non-fungible tokens.[4] ETH EIP 1634 Re-fungible tokens.[5] Chainlink 2.0: Next Steps in the Evolution of Decentralized Oracle Networks. Chainlink Labs. https://chainlinklabs.com/', '@edelsohn I wanted to thank you for posting one of the most interesting discussion posts I’ve seen on this site! During our Monday morning meeting you mentioned some of shortcomings of NFTs as they’re currently defined in the Ethereum ecosystem, problems with lending, problems with provenance. Could some of the metaphysics of NFTs be solved by simply changing the NFT framework or creating a new version of them that makes these issues easier to solve?', 'Thanks for your kind words. Glad to launch the conversation. I hope to find other researchers who would like to collaborate and explore the solutions to these issues.I proposed a solution that leverages the existing design and concept of NFTs, through expansion of the role of smart contracts and oracles.  Basically, one continues to create and destroy NFTs, but more rapidly and dynamically as their components change in ways that cannot be represented by dNFTs, e.g., changes to the data of the NFT without change in ownership or tracking the disposition of the sub-components that changed.NFTs are immutable. That’s a fundamental principle. There is no new version of NFTs that can change that concept. Objects in the real world are continuously changing. It’s Heraclitus again: “No man ever steps in the same river twice , for it’s not the same river and he’s not the same man.” At what point does the NFT have to represent that a change has occurred?  The NFT of the river or the NFT of the man has a temporal component, but it’s impractical to create and destroy NFTs for each object on an infinitesimal timescale.  So when do you do it?  When it matters, when it’s observable.', 'At @dwither @Fizzymidas – I would love to hear what you think about some of this discussion of the metaphysics of NFTs from the perspective of presistent soulbound tokens – any possible connections between them? @cipherix what do you think about this, you certainly have the widest range of experience with crypto data objects of anyone I know on this forum?', 'And of course @Larry_Bates – I’m sure you have some ideas about this as well! (I just saw your like)', 'I have been thinking about this post to wrap my head around it before I post a response!  There is a lot to unpack here!It has made me revisit the concept of “what type of digital uniqueness is necessary vs. simply a novelty?”', ' edelsohn:It is not practical for the Metaverse to represent and track the finest granularity of every object. Video games follow the same principle to only generate what can be seen when it is observed. The Metaverse must follow the same principle of object representation: NFTs should be created and correspond to the coarsest granularity that represents the object and its ownership necessary for the required, observable action. edelsohn:The human perception of the environment requires an observer. Computer vision needs an embodied observer to interpret its input.I’m adding my praise to @jmcgirk’s appreciation of this essay, and focusing on the implied analogy to 3D (i.e., video games) because that helped me understand the point you’re making.If you’re portraying a rocket ship approaching a planet, a very complex model is wasteful when the planet is very far away (from the camera) and the viewer can’t perceive the complexity. As the viewer (the camera) gets closer to the planet, the original model (perhaps only hundreds of polygons) needs to increase to thousands and then tens of thousands of polygons to faithfully reproduce the “reality” of the planet at high granularity.In 3D, the illusion of “closer” and “farther away” are the result of where the camera is located and how much light is falling on the object. (Standing on a planet in complete darkness requires zero polygons.)I wonder how useful “proximity” and “illumination” might be to your explicit analogy of replacing all the boards in a ship (or all the cells in a living thing). When we meet a year after our first meeting, I’m clearly the same person I was a year ago, yet I look older, and my doctor (using the proper camera and illumination) may find something undesirable growing inside me that wasn’t there before.As a side note, this also reminds me somewhat of the work of Italian roboticist and philosopher Riccardo Manzotti (The Spread Mind and Dialogues on Consciousness) who asserts that consciousness is not in our minds at all but in the world, and thus when we look at a star many light years away, our “spread mind” is literally encompassing that distance.', 'Thanks for the thoughtful reply.The object itself changes and the perception of the object varies among observers.Most of us perceive stars in the same manner. But now with the Hubble Space Telescope or the new Webb Telescope, a few of us have a more detailed perception. That difference probably doesn’t matter for a star, but it may matter for my view of the front of a car versus your view of the back of the same car. It’s the same car – the same object, the same NFT – but the perceptions – observation embedded in a viewer – can have implications to the operations on the NFT.You also raised the issue that you are the same person a year later but that the cells in your body have changed.  Some hair fell out and our hair is a little more grey than last year.  Those changes might not be important and could be represented as data changes to a dNFT, but if we are celebrities, a fan might want a piece of our hair.  In the latter circumstance, the hair and its origin matter.  The hair is not new, it came from you, so it is not appropriate to materialize a new NFT from scratch. Instead, one should bifurcate your NFT into two NFTs: one of you without the hair and another of the hair. Then someone can obtain the NFT corresponding to the hair.And this again highlights perception. Maybe some people are interested in the hair and others are not.  And maybe some people pick up the hair and examine it or some fans fight over it.  The state and disposition of the NFT varies among observers, and there are periods of time when the Blockchain network / Metaverse may speculatively allow some observers to perceive and experience changes to the NFT that have not been committed to the network.All of the metaphysical questions about identity, observability and consciousness come into play. The Metaverse must confront these questions, even if the developers don’t yet realize it.', 'Firstly, I’d like to echo the others and say that this is a fascinating post @edelsohn.As I was reading through, the first thought that came to my mind was dynamic NFTs, which Chainlink has been enabling: What Is a Dynamic NFT? | Chainlink Blog. This is just a component of what you are articulating here, but it is an example of how an NFT might change and adapt, while still remaining the same NFT.Broadly speaking, I recognise the problem you describe in this post and agree with the direction you propose when it comes to a solution. I have been thinking along similar lines but in different contexts – my own area of expertise focuses on people, rather than objects, but I think many of the same principles apply. I especially appreciate the emphasis you place on the importance of observation as well as perspective. Overall, I think that at a theoretical level all of this makes sense, however, the real challenge will be in how this translates from theory into reality.When thinking about how I might add value to this post, I thought it might be useful to analyse the similarities (as @jmcgirk noted), between this discussion and one we are also having about soulbound NFTs as a way to capture and use identity on the blockchain (Discussion Post: Are Soulbound Tokens the best way to create a trusted layer on the blockchain?). I think this serves as a good practical example of the challenges you are talking about, except that it focuses on how to represent people on the blockchain, rather than objects, meaning that we talk about an NFT that changes with the object (person) it represents.A ‘soulbound NFT’ can generally be described as a framework for attestations that encapsulates the digital representation of a person on the blockchain. A key problem is that we must establish both the identity of the person, as well as attributes about that person that are both objective and subjective and relate to social, human, and financial capital. Most importantly, these attributes will change over time. To do this, we need an oracle and smart contracts to create, maintain, and update attestations. An oracle can attest to a person’s real identity using existing government documents such as a passport or driver’s license, and other objective factors such as qualifications and income etc. However, when it comes to subjective attestations (see Kate Sills critique of soulbound NFTs), secure smart contracts can provide a credibly neutral mechanism by which people can make attestations about each other.Do you think that soulbound NFTs, when described like this, provide a practical example of the problems and solutions you describe? There is more depth to your analysis, but I think it maps into this context pretty well.I also have a few questions/comments about the key points you made.An object represented in the Metaverse must be arbitrarily and dynamically divisible or fusible with other objects on demand, as determined by the “laws of nature” in which the object exists in the respective Metaverse.I am interested in why you choose to use the words ‘divisible’ or ‘fusible’ here. This may simply be a product of different backgrounds, but the words I would lean to are adaptable and transformable. Perhaps this is just semantics, or am I not fully understanding what you mean? I think the core concept you are getting at here, is that the NFT which represents an object must be able to change (adapt or transform) in accordance with both the object it represents in the real world, and according the “laws of nature” on the blockchain.I think we could also have a discussion about how we define the laws of nature in this context, as they are presumably determined by people, and therefore driven by social consensus, rather than us discovering them through trial and error such as in the real world. In this sense, there are limitations to analogies between the metaverse (something we create) and the real world (something we exist in, and discover things about).NFTs should be created and correspond to the coarsest granularity that represents the object and its ownership necessary for the required, observable action.I particularly like how this principle maps onto the people side of things, as I think it could be used for privacy related purposes which are a huge challenge. “An attestation about a person should be created and correspond to the coarsest granularity that represents what is being attested to, and its ownership necessary for the required, observable action.”The relationship between the representation or materialization of objects and the NFTs on the Blockchain must dynamically adapt to the ownership and the actions being performed; it’s not always a one-to-one mapping.Each NFT should correspond to the maximal complete subgraph of the components of the object that is necessary and sufficient for the operation to be performed on the object and the ownership of the components.I am intrigued by the tension between representing the ship of Theseus as a single NFT on the blockchain, or representing each plank of wood as an NFT, and the ship itself becomes a composite of those NFTs. Do we then also represent the trees that are cut down to create the wood as NFTs? Would this NFT be destroyed or transformed during this process? This could be an interesting way to systemically account for previously ignored externalities.', 'Thanks for your great comments and insights.  You raise important points about identity and attributes.Soulbound NFTs definitely are an aspect of the issue. I don’t know if they are the correct solution.I believe that identities need to provide a capabilities-based system.  A person has various characteristics and capabilities.  Some of these are inherent, like a person’s DNA, but some are capabilities that they have gained, such as access to a bank account or a car or a house or a SCFR forum account.  Instead of “this person has permission to do X”, a better model is “this person has the capability to do X”.  Then the capability can be transferred or copied or rented.  A person could delegate (copy) capabilities for certain financial activities to a business manager or accountant.I imagine the example of someone invited to a business luncheon and allowed to bring a guest.  Ignoring issues like security pre-screening, the person could show up with the guest.  The guest does not need separate permission to attend the luncheon.  The guest does not need a separate invitation or a separate listing on the guest list.  The person who was invited has the capability to bring himself and a second person.  That is an attribute (capability) added to the person.  And at the business luncheon, the identity of the person who brought the guest is important.  It’s like degrees of connection on LinkedIn, fundamental to psychology and persuasion, and the reason that warm introductions are important: the title and history of the individual establishes a bias for the guest (or maybe vice versa). This aspect of identity and transferrable “trust” (for lack of a better term) needs to be represented and actionable through identities depicted as NFTs.In regard to “divisible” and “fusible” versus “adaptable” and “transformable”, I am trying to distinguish between data that changes within an NFT (dNFT) to represent the change to an object and components of NFTs that change ownership.  As I described in the post, if I have an NFT for a collectible shirt and give someone a button from the shirt to replace a broken button in their shirt, it’s not a fungible button. It’s not simply a button moving from one shirt to another.  The NFT for the original shirt no longer correctly represents the shirt.  The initial shirt now is a shirt with one less button.  And the second shirt now is a shirt with a foreign button from the first shirt.  The two original NFTs have changed in ways that should be tracked.I propose that they should be tracked as operations on the Blockchain, not solely internal to the data of the NFT (dNFT).  The original NFT should be destroyed, under control of a smart contract, and replaced with two NFTs for the initial shirt without the button and a separate button.  That is what I mean by divisibility of the NFT: one NFT transformed into two NFTs based on the “laws of nature” (not a shirt and a bar of gold).  And the second shirt incorporates the button through fusion, under control of a smart contract.My reference to granularity of the NFT is trying to express that one could maintain a composite NFT of a shirt without its original button and a button obtained from another shirt, or one could proactively scavenge the extra complexity of two NFTs and replace it with yet a newer, single NFT that represents the composite object of the second shirt with the foreign button.And your final comment about trees cut down to create the planks is exactly correct. I propose a mechanism that avoids tracking every molecule or atom or quark as a separate NFT and compositing them together.  To start at an arbitrary point, one cuts down a tree (NFT), now there is a stump (NFT) and a trunk (NFT) and branches (NFTs).  The trunk is cut into planks (NFTs).  The planks are treated and processed and eventually construct a ship (NFT) or a house (NFT).  At various points in time it is a tree, then a trunk, then planks, then a ship.Or consider vegetables farmed and eventually used in a salad. Is it vegan? Was it harvested on a fair-trade farm? Was the vegetable later discovered to be contaminated with E. Coli or Listeria?Or steel girders in a bridge.  Did the steel contain defects?Depending on the context and the individual, one may care about the tree, the planks, the ship, the house.  When is the house NFT a single NFT and when does it need to be the components?  I refer to arbitrarily and dynamically divisible and fusible because if I purchase a ship or a house or a salad or a bridge, I interact with the object at the coarsest granularity until something forces a finer granularity.This is related to attention in human consciousness and perception. The human brain would be overwhelmed if it tracked all of the detail of all senses.  Most information is filtered out.  But in a particular circumstance and task, the brain focuses on the additional information from particular sense.  I propose that NFTs and the Metaverse must operate in a similar manner to avoid overwhelming the Blockchain networks in ways that are not scalable.', '@dwither How would you envision the transactions operating solely through dNFTs?For example, tree to trunk to planks to ship. On which, single NFT would one attach the transformed data? One might not know that the tree will be used to construct a ship when the tree is felled, and one may not know about the future disposition of the planks from the ship.  Maybe the ship participates in a famous event and the wood is reused.For example, the UK Diamond Jubilee State Coach includes fragments of Henry VIII’s warship, the Mary Rose, and Lord Nelson’s ship, the Victory, as well as pieces of wood from Westminster Abbey, St Paul’s Cathedral, Kensington Palace, and even a supposed piece of Sir Isaac Newton’s apple tree. But it also contains modern history in the form of wood from the royal yacht Britannia.  How would one preemptively know that fragments would be reused?Yes, dNFT allows one to modify the object, but if one transfers components of the original object, one needs to be able to dynamically specify the components and declare some NFT to which one attaches the results.  One can create a new dNFT and modify the object that it represents, but it’s still a new NFT.I’m curious about the semantics of adapt and transform in this context.', ' edelsohn:The hair is not new, it came from you, so it is not appropriate to materialize a new NFT from scratch. Instead, one should bifurcate your NFT into two NFTs: one of you without the hair and another of the hair. Then someone can obtain the NFT corresponding to the hair.Thanks for the thoughtful answer, and especially for linking it to specific NFT examples.', 'I’ve been traveling recently, so just catching up - we’re largely on the same page here. edelsohn:How would you envision the transactions operating solely through dNFTs?I see transactions as occurring both intrinsically through and extrinsically around dNFTs. The distinction you make between divisible and fusible is much clearer when considering objects, especially the examples you provide. The problem is less clear when considering identity (which is my main focus), but still relevant now that I think through the problem. I would describe adaptation as change within an NFT, and transformation as the changing of that NFT. It’s definitely semantics, but in this sense it could be seen that the original NFT isn’t destroyed when it is turned into others (tree into stump + planks etc), rather it is transformed into them, but the linkages/history remain. This is true regardless of labels, I’m just more familiar with adaptation and transformation as concepts and so gravitated towards them. Do you have any thoughts about what the first use case for these types of NFTs might be? I can see the purpose in the big picture, but where might the initial product market fit be?There is another layer to this. Depicting objects on the blockchain is important, but often the relationship between two objects is more important than either object individually. For example, in a post disaster context, one of the best predictors for good outcomes during response activities is a strong connection/relationship between the emergent community response and the institutional response. The capabilities of those groups in isolation is less important than they are combined. Consider a rural community affected by a large magnitude earthquake, the institutional response has the necessary resources (engineering equipment, food, water etc) but does not understand the local context and how to best utilise it. The emergent community response has the correct granularity when it comes to understanding which infrastructure needs priority repairs, who needs help, where food shortages are etc. This type of connection can be described as linking social capital (eg https://journals.sagepub.com/doi/abs/10.1177/0002764214550299), and where it is present there are generally better outcomes, and where it is not, generally worse outcomes.Representing these types of relationships on a blockchain is difficult. My initial thoughts are that for the above context you would want to create a role (NFT?) for each party which have a formal relationship, the institutional response would be clearly labelled and predetermined and likely be adaptable/fusible. However, it’s almost impossible to pre-determine who or how many will lead a community response, as it’s emergent. In this context the role (NFT) for the community response would have to be open with a call for volunteers who could step up and interact with the responders through this type of framework. This second NFT would likely need the capability to transform or divide as the response progressed and volunteers changed. This example only really works in this context, however, and a wider framework to account for different contexts would be required. Do you have any thoughts about how to represent these types of relationships between NFTs on the blockchain?With regards to your previous post I agree that identity needs to provide a capabilities-based systems, and this is one of the problems I am thinking through. Who decides who has what capabilities in what contexts? How do we ensure that the granting of capabilities is not abused for personal gain? Inherent capabilities which can be objectively determined can be relatively easy in certain contexts, but subjective ones (such as those often based in social and human capital) can get tricky. There are significant implications for power relationships here, and the problem is not limited to people – Goodhart’s Law (when a measure becomes a target, it ceases to be a good measure) becomes relevant. This is veering off into the oracle problem though, and away from the metaphysics of NFTs. One area where NFTs can be useful in this context, however, is tracking history or reputation of particular vendors, but it requires being able to effectively measure what is being optimised for. For example, it’s quite difficult to determine if produce is fair trade or organic ex-post facto.', 'I envisioned the history on the Blockchain, not within the NFT.  NFT-1 is destroyed (burned) and NFT-2 and NFT-3 are created (minted) to replace it.I complete agree with your comment that relationships are important.  I was trying to allude to that with the statements about proactively scavenging NFTs and about observers.  In regard to your emergency response example, in one context its a rescue team, in another its the community team and institutional team, and yet another are the individual people, to pick some arbitrary “resolutions”.  In the Metaverse, different activities would interact with the entities at different resolutions.  Does one provide the rescue equipment to one of the representations of the team or to an individual?In regard to the representation, EIP 998 already describes composable NFTs, which addresses at least part of the relationship. There is a very interest, unresolved problem of the interaction between the NFT “objective truth” of the “world” and the lower-granularity “views” depending on the observer.  This could be a great research topic.How this environment is implemented through Oracles is a fundamental question.  The Oracles are intimately tied to the metaphysics.', ' edelsohn:Does one provide the recue equipment to one of the representations of the team or to an individual?I see this more as an accountability function. Where does the equipment/resources go, and for what purpose? The idea would be for the blockchain to mirror the real world as far as practical, to provide ease for retrospective analysis as to how resources were used, and whether lessons can be learned for next time. Otherwise there would not be much use in putting this onchain.', 'Consider: that was the rescue plane flown by Jimmy Stewart or Harrison Ford or Prince William.Also, consider a completely virtual world, not only digital twins. In a solely digital representation (“game”), the Blockchain is the only representation of the disposition of the objects.  If the  Metaverse game represents a disaster and rescue, all of the participants and objects are NFTs on a Blockchain.', '@dwither First use is the million dollar question.  History implies military or “entertainment”.  So far, artists and creatives are early adopters of NFT technology.  They are trying to monetize their abilities in the new gig economy and have greater psychological trait openness.As art NFTs evolve from static or animated images to audio, video, AR, VR, MR, XR, etc. one can envision editing the material represented as NFTs. Consider a TikTok creator who records lots of raw footage for potential clips, followers invest in the raw footage as NFTs to fund the creator, and the footage is edited together into clips.  Plus there is the question of how media represented as NFTs relates to music and video rights management (mechanical rights, performance rights, synchronization rights, broadcast rights).The financial opportunity will take another leap when this model is applied to assets like real estate, energy, food, and healthcare. Just as with PC-native generation, Internet-native generation, Social Media-native generation, the Blockchain-native generation will bring a much different attitude and be much more comfortable with Blockchain as the basis for all transactions.', 'Thanks for your thoughts on first use. It will be interesting to see where the blockchain-native generation takes things. edelsohn:Consider: that was the rescue plane flown by Jimmy Stewart or Harrison Ford or Prince William.Also, consider a completely virtual world, not only digital twins. In a solely digital representation (“game”), the Blockchain is the only representation of the disposition of the objects. If the Metaverse game represents a disaster and rescue, all of the participants and objects are NFTs on a Blockchain.Exactly, this is why I am so interested in identity, and how it is represented on the blockchain. Both the subject and the object are important, but we seem to focus on one or the other, when a structured approach to both is necessary to achieve desired outcomes.Additionally, borrowing from Daniel Kahneman’s ‘what you see is all there is’ cognitive bias, if we only partially represent reality, then we are cognitively biased to ignore what isn’t represented. This comes back to the importance of observation that you mentioned in the first post - what do we see, and what do we not see? The second question is harder to answer, but no less important.', 'Blockchain and NFTs are going to get into everything.  When you ask First Use, part of the confusion is First Use in what sense?copper.co already is being deployed by State Street.  Metaco already is being deployed by Citi, BNP Paribas, DBS, BBVA, Societe General Forge.  But that’s for traditional settlement and custody.Animoca and its subsidiaries is deploying it in gaming.My curiosity is the novel, future deployments and applications driven by the Blockchain solutions to these metaphysics paradoxes.', 'I can imagine Rolex creating NFTs out of every single component of a watch or an airplane manufacturer doing so for safety reasons. Actually I wonder if Toyota is up to something similar – they’re really pushing “Keep your Toyota a Toyota” with their OEM parts. I guess the blockchain would be the next step. You can scanning the car’s wallet and reading that it’s 95% intact or something like that.']"
                Proposal to add a Community Category on the forum              ,https://www.smartcontractresearch.org/t/proposal-to-add-a-community-category-on-the-forum/1692,Meta,58,[],['summary'],"['This post follows up on the June 23, 2022 Community Call 1 where I proposed that SCRF add a Community category to the forum. This post is intended to be the next step in that proposal so that people can add additional insights, requirements, or concerns. I’ll also lay out some next steps and timelines for feedback. The implementation of this category, assuming we have general consensus, would be tracked on the Create Community Section on forum 3 issue.TLDRThis is a proposal for a community category on the forum where SCRF community activity can have a place for the output of those activities.There are several content types proposed that would be part of this categoryReading GroupCommunity GuildsComment of the MonthCommunity CallSCRF RecommendsNotable Works and Key QuestionsCommunity Roundup or SCRF EmbassyWelcome/How ToThere are a few tags proposed as wellDecisionProposalGuild WorkAmbassador ReportThe following questions are proposed to help guide conversations on this proposalDoes a Community category belong on our forum at all?Assuming yes above, what feedback and thoughts exist about the proposed content types and tags in the above proposal? Part B, are there additional proposals?What work do we need to do in order to make this a successful category?JustificationAs the SCRF community grows, it needs a space for community outputs. Members of our organization are now forming reading groups, participating in guild meetings to make decisions, and contributing to community grants like Comment of the Month. Some of this work happens through issues and project boards on our GitHub. Others accomplish their work via our chat. Neither of those places ideally facilitates the general goals of our community work, however. For the work our community does, I think creating a readily visible, on-forum, long-tail space for community contribution would support the growth and value of our community. a to include this as well.Proposed StructurePart of the rationale in proposing this new category for the forum is there are already activities happening at SCRF that would fall within a Community section. These are::Reading GroupCommunity GuildsComment of the MonthIn my mind, only the Comment of the Month is potentially out of place here because it is dealing with the awarding of a grant. There is the Grant Proposals, Bounties, and Awards 1 category that already exists for this. However, because it is the community that nominates and then votes on the awarding of this particular project, I propose that the nomination and decision making happen in the Community category.Each of these already have somewhat of a description on the forum as well, but they would likely need to be updated some to help people make use of them on forum. That is a bit of a lift, but I think it’s a small one.In addition to what we are already doing, however, I am proposing the following also become part of the proposed Community category.Community CallWe have great discussions in our weekly community call and we often run out of time as a result. Also, while it is on the schedule of the calendar, it is not always easy to find what the topic of the community call will be unless you happen to catch something in our chat. Additionally, this is a place where links to forum posts and long-tailed discussion about the content of the community call can be sustained.SCRF RecommendsThere have been several discussion in meetings and in chat about where to start in crypto. As an organization, that is not really part of SCRF’s mission nor are we in a position to really do a better job than many of the organizations out there that do focus on crypto education. However, we do have an organizational interest in making sure people have access to this material and we do have a community of people who are voracious consumers of crypto information. Having a SCRF Recommends section would allow us to put forward such resources for discussion and potential inclusion in a SCRF outward facing location (the thread, website, repo perhaps). This helps us onboard and socialize people new to the space without having to be the primary educators. It also potentially creates collaboration opportunities between SCRF and others.Notable Works and Key QuestionsThe other categories on our forum have pinned posts that collect notable works and key questions. As a member of the Engagement Vertical, I see a lot of value in copying these content types for the community category. Seeing as SCRF is building a community around research and evidence, it certainly makes sense to me that we use research and evidence in co-developing best practices for community activities as well. Notable Works in Community could even be a outwardly useful guide for other communities to refer to. I suspect we would also be collecting the insights from other communities there as well.Key Questions in Community would be a location where the types of questions we have on “how to do” in a community could be collected and potentially lead to recommendations. This has both onboarding and culture development benefits.Community Roundup and/or SCRF EmbassyDuring the community call, I had suggested that the Community category would benefit from a roundup of our community discussions/activities. The cadence of this might be weekly or longer, but it provides Discovery and Outreach an opportunity to point to what SCRF is up to as a community. During the call, some great ideas regarding a place for our community to report out and others to report in were raised and discussed. These are still pretty nebulous in my mind, but I’m generally calling this the SCRF Embassy. It might be a place for our cross-pollinators to interact as well. I’m looking forward to how this develops, as I think it is a great idea!Welcome/How ToI am less committed to this being on the forum, but it does seem like an opportunity to discuss how to guides/best practices for new community members or potentially a thread that helps new members become oriented to life on the forum. I would also be interested in developing this section with others if they have a clear vision of what this could look like to provide utility to our community.Additionally, there are some tags that would be valuable for both the community section and the forum overall that I am proposing here.DecisionThis tag indicates that the thread includes either a call for a decision to be made or that it contains a decision that has been made. In addition to connecting these types of decisions to our GitHub repos, capturing decision points visibly on the forum can also help people better understand SCRF and the work SCRF does as a community.ProposalThis tag indicates that a proposal is being made. This has value across the forum as research content also includes proposals being made.Reading GroupThis tag will help people readily find information about reading group activities and outputs.Guild WorkThis tag will help people find information about the output of guilds. For example, the discussion about SourceCred, now that it is in the hands of a guild, would receive this tag. Additionally, this would make it easy to find any newsletter like material that guilds might use the forum to accomplish.Ambassador Report or Community ReportI am certainly not fully committed to either of these names. The intention here is that work that reflects the reporting out or reporting in of community, cross-pollinator, or ambassador type work happens in our community section, it should be easily discovered. Please help in naming this if you feel they are good tags to have!Costs to ForumA new category on the forum certainly comes with potential costs. As a community, I want to make sure we identify the potential costs and have some discussion on how to mitigate them if we go forward with this category proposal. In my mind, there are two areas of potential concern, but I certainly want to encourage people to bring up additional ones to help me overcome by blind spots.Attention CompetitionThe forum is primarily focused on long-tail discussion around research summaries, discussion posts, and other Web3 problem-oriented content. The addition of community activity to the forum potentially drowns out some of this activity. Deep and thoughtful interactions are difficult; that is why it is so valuable that SCRF is creating a space for that to happen. Community content, while still requiring thought and effort, has lower barriers of entry. I would want to make sure that we continue to be able to highlight the key mission of SCRF and also incentivize the high level research discussion. I am interested in thoughts others have of how big of a problem this potentially is and how we might be able to create a forum experience that mitigates it.Moderation EffortAssuming that the content types above are something we adopt, there is then increased moderation effort needed to help guide people into our content type frameworks and also to encourage and curate the type of interactions we are looking to house in this category. This will take additional staffing or dedication from the staff we currently have. I think this is worth it, but we need to identify those resources and encourage the development of those resources within our community. This also means we have some need for documentation and maintenance as well.Next StepsIf there was a Decision tag available, I would be using it for this post as I am looking to this thread to produce a decision. There are a few to make here and some discussion to have around it. There are three questions I am particularly interested in exploring here:Does a Community category belong on our forum at all?Assuming yes above, what feedback and thoughts exist about the proposed content types and tags in the above proposal? Part B, are there additional proposals?What work do we need to do in order to make this a successful category?My general timeline (open to discussion) is that this thread be open to discussion for approximately 10 days to give a reasonable opportunity for people to contribute. I am then hoping we can set up a poll that essentially “calls the question” that either takes on this whole proposal or breaks it out into a series of polls and questions. Those would also be open for about 10 days. Seeing how that goes, then we start to implement what we have worked through.I am excited to engage in this as a conversation. Obviously, this is something I am incredibly in favor of and think that there is a lot of value add for us as an organization and community by implementing a Community category. Hopefully our community is also aligned here too.', 'I applaud this idea, I think our forum is a tiny bit stuffy, and now that we have a critical mass of smart cryptonative people we could take some of the energy we have in our Discord and feed it into the forum itself. I highly encourage this and hope people can use the Community category as a place to relax, think out loud about web3, and bat ideas against one another (or even argue!)', 'I think this is an amazing idea! A community category would help us have a specific place to discuss things that you have listed. For instance, I would love to have more discussions on the Impact Networks book that we read in the reading group for June, but the group meeting is over and it seems tough to find like-minded people. I think a community category would make it easier to have those conversations. Also, conversations stemming from the community calls can be continued here, if created.On the flip side, how do we organise/moderate conversations? Would a community section on the forum duplicate what could be a channel on the chat? Would it be possible to have a parking space for less formal (research-related) conversations?To try to answer these questions zube.paul:Does a Community category belong on our forum at all?Assuming yes above, what feedback and thoughts exist about the proposed content types and tags in the above proposal? Part B, are there additional proposals?What work do we need to do in order to make this a successful category?I think yes, a community category belongs on our forum, the potential pros seem to outweigh the cons.I also think the proposed contents types and tags look good, however, I’d like to suggest a little veering off the serious track to allow the posting of memes There may be a need for the engagement team to initiate conversations pending when community members become more familiar with the category. The engagement team may also need to be able to track people’s interests and tag them to conversations that may be of interest to them.', 'Great observations about what the engagement team will need to do to get this started. I generally agree with you that will have to be an effort by the team to direct people to this category. My long-term hope is that this section of the forum becomes “owned” by the SCRF community and onboarding teams.  and that also there will be an onboarding element to this category as well.The idea of memes has come up in the past in chat also. For the forum, I’m a little less interested in having there be a lot of meme-type content because I think that is antithetical to the type of discussion that SCRF is trying to support and build. Maybe some in the chat? I am curious, however, what the value of meme content is for our community? To me, there are several places on the internet that support that well and we’re just not going to be able to or maybe even want to compete with those spaces.', 'Thanks for proposing this idea. I think adding a Community Section would be a great addition to the forum. I like the ideas that you outlined - it’d be great to find what version of bringing the community calls to the forum that makes sense/gets people excited to interact.I agree with @Tolulope that it’d be great to keep the conversation going on Impact Networks and future reading groups.One question is about the decision tag. If the goal is to signal something needs to be decided, how is will that be delineated from proposals. Will all posts that need a proposal tag also need a decision tag? Also, if there’s a desire to link to github, how would that be operationalized?I think getting some kind of community report that highlights external communities and research arising/being discussed would be great. That ties into some of the work being done on the Cross Pollinator side - @Hermes_Corp @Hazel_Devjani it’d be great to get your opinion on that.One area of thing to think through would be when we start doing more research oriented community events, where would that content go. E.g. if we’re discussing research summary x in a live chat, would that need a community post or would any relevant forum convo get captured in the summary post?Excited to see where this goes!', 'Great questions!For the first question regarding the decision and proposal tag, they may actually be redundant. All proposals will need some type of decision, so the presence of a proposal tag should also indicate that a decision is happening there. I suppose my original thinking was that something like Notable Works or Key Questions would also come with decision tags, providing that tag with utility across the forum. Threads like the SCRF Terms Glossary and Content Tags might also benefit from a decision tag so that people could filter by the decisions that the SCRF community has made over time. Perhaps to start with, just the proposal would make sense.Regarding the second question about research-oriented community events, I think the key here is to make sure we are highlighting research content above all else. I am interested in seeing the success of this community category, but it is a support category to help us produce more research content. If we have a live chat about a research summary, I think that the output on the forum would be best categorized as a content type in the appropriate research category. For example, a live chat on cryptography would go in the Cryptography category. Even a reading group that focuses on a core cryptography paper should probably have output in the Cryptography category. Perhaps the planning of these events could be facilitated by the Community category discussion, but the output itself should be in the research categories if possible.Thanks for the opportunity to do some thinking through those.', 'I’m in strong support of adding a community section to the forum. I hope it will open the door to a lot of discussions that don’t currently have a home and help make our community more tight-knit. zube.paul:Assuming yes above, what feedback and thoughts exist about the proposed content types and tags in the above proposal? Part B, are there additional proposals?The proposed content types are fantastic. I’d also love to see room for ‘speculation’ or theorizing in the community section of the forum. I think a lot of great science often starts as conjecture and having a place to make guesses together is really valuable. What do you think?I also really appreciate you sliding in those proposed tags for the overall forum. I think they would be a great addition', 'Thanks, @UmarKhanEth.I am interested in your speculation idea, as there have been some side discussions in chat about being able to highlight that something is a think piece or speculation. I agree that can be good for science overall, but I’m not sure about if it is good for SCRF at the moment. If it is in the Community section, then it is somewhat of a free-for-all on topics of conjecture. That’s not really where SCRF is providing value. If the speculation happens in the content category it is speculating on, I think that has more value, but it does potentially drown out the research content. We would need to carefully distinguish between speculation and summaries. I think at the end of the day, I would want them associated with content type, but do you have ideas of how to keep the research more clearly distinct?I am on the verge of calling the question on this proposal, would you be opposed to continuing a discussion about where think pieces, speculation, theorizing fits post the section vote and potential creation?', 'Why don’t we add this to the Discord rather than the Forum? I’m not sure a community section makes sense on the Forum when we have a better community-focused tool. Let the Forum be content. You’re going to start diminishing the quality of the Forum itself when we begin using the Forum for everything and the kitchen sink.If we want the community to interact for ‘community things’, let’s use the community tool.I do see the tags proposed as valuable enough to organize content without needing a full Community Category. Serve the same purpose and would avoid the duplication of a category and tags if they achieve the same outcome.We could add several Categories and Channels to support this:Rename the “community” channel to “general”Add Category: SCRF EmbassyChannels: CommunityNotable WorksSCRF RecommendsResearch DAOsAs the other channels on Discord (Chat) follow, this would have a #start-here.Thinking the cross-pollinators can really own and design this section.', 'Hello @zube.paul,I’m happy to keep the discussion going after the vote.To keep the research distinct, maybe this is another use-case for tags. We could have a tag for ‘thinkpiece’ separate from a tag for ‘research’ and maybe require the appropriate tag be a part of the topic title. I also share your perspective that this sort of conjecture may not be where SCRF is providing value, although I think it could be fun and lead to connections between researchers who are able to be a bit looser with their thoughts.I’m also keen to see what happens in the community section. I think it may be easier to just see what happens and correct or reverse if things go wrong than try to predict and prevent.It would be so exciting to see researchers on the forum hypothesizing in the open and coming up with questions that may turn into serious research forays or papers. But even without this sort of speculation, I’m excited for a community section that helps connect people.', 'I like your recommendations for what we could be doing in the chat. There are some places where we could definitely enhance chat to support the type of content being discussed in this thread.  I would be particularly interested in what @Hermes_Corp and @Hazel_Devjani would add for the embassy idea. Based on their most recent presentation 1, I no longer would argue for a content type being called an embassy, but would want to work with them to design a hub for output of cross-pollinator activity.I still believe that a forum category is warranted, however. To me, the chat has an ephemeral quality and is not particularly valuable for long-tail discussions or output. I agree that we do not want to diminish the content quality of the forum, but I would argue the type of output that would be in the Community Category would augment, as opposed to diminish, forum quality. As an example, there are the beginnings of some great discussions that are generated by the Community Call. Having a place for a long-tail discussion where people could also bring some research into the discussion. I suspect it might end up looking something like the various Smart Contract Summit posts from 2021.This category also helps to create a history of decisions and thought processes that is seemingly more accessible than having community output live in GitHub, which would be the other platform that we have that might be able to capture this institutional knowledge. There are some UX considerations we could add here to help people navigate that knowledge, but doing the “work” of community on forum gives us the ability to accomplish those goals. I believe that the chat is a valuable place to brainstorm and connect, but it is not particularly good at housing output. That’s why I think the forum would be a good location to have a Community section.', 'There has been some good discussion here that I greatly appreciate. I do think it is time to call the question though. There is a lot in the original proposal, but I think that the best course of action is to vote on whether to have the category or not. Following the vote, we can build out content pieces if approved or put in no effort if not approved by the community.Adding a Community category to the forum vote88%Yes, I support adding a Community category to the forum12%No, I oppose adding a Community category to the forum16votersShow voteClosed Jul 30', 'The proposal passes.Thank you everyone for your input and discussion. Follow the Create Community Section 5 issue as we implement this section and the proposal.', '@zube.paul I am interested in seeing the success of this community category, but it is a support category to help us produce more research content. If we have a live chat about a research summary, I think that the output on the forum would be best categorized as a content type in the appropriate research category. We would need to carefully distinguish between speculation and summaries. If it is in the Community section, then it is somewhat of a free-for-all on topics of conjecture.']"
                Scalable and Probabilistic Leaderless BFT Consensus through Metastability              ,https://www.smartcontractresearch.org/t/scalable-and-probabilistic-leaderless-bft-consensus-through-metastability/181,Oracles and Data,25,['https://arxiv.org/abs/1906.08936'],"['consensus', 'bft', 'summary', 'oracles', 'scalability', 'iot', 'defi']","['TLDR:This paper introduces a group of leaderless Byzantine Fault Tolerant protocols called Snowball, Slush, and Snowflake.The Snow protocols attempt to leverage a metastability layer to reduce input latency and increase throughput speed.These protocols were designed to increase energy efficiency without compromising safety or livenessThey succeed in being more energy-efficient than existing solutions and offer promise for high throughput, quiescent PoS protocols, but more research needs to be done before a provable solution is found.Core Research QuestionCan we find an energy-efficient alternative to the Nakamoto consensus in the Snow protocol family without compromising safety or liveness?CitationT. Rocket, M. Yin, K. Sekniqi, R. van Renesse, and E. G. Sirer, “Scalable and Probabilistic Leaderless BFT Consensus through Metastability,” 24-Aug-2020. Available: [1906.08936] Scalable and Probabilistic Leaderless BFT Consensus through Metastability 8BackgroundThis paper analyzes multiple types of leaderless Byzantine Fault Tolerant consensus algorithms to ascertain each respective algorithm’s scalability through various dimensions that go beyond the scope of typical consensus analysis.Bitcoin currently consumes roughly 63.49 TWh/year, nearly twice the consumption of Denmark. This excessive energy consumption is targeted as a problem to be solved by the Snow protocol family.As Nakamoto consensus requires energy consumption even in an idle state, the Snow protocol family does not. Nakamoto Consensus requires the participation of miners to maintain security even when no decisions are being made.SummaryThe mechanisms introduced in the paper are constructed around metastability achieved through network subsampling.Instead of the traditional linear bit processing approach, these models process logarithmic bits reducing the need for nodes to have as much information to maintain concurrency.The Snow protocol family 3 operates using an intentionally metastable layer that constantly samples the network to steer nodes towards the correct outcome.Nakamoto Consensus takes a proportionate amount of time to finish a round the more adversarial nodes are attempting to break the consensus.Flooding or Spamming Attacks are a problem in any system, which Avalanche addresses using transaction fees as a deterrent.Adversarial Nodes can effectively run attacks between infinitesimally small intervals, as they are not bound by the latency associated with the responses from correct nodes. On the other hand, it cannot coordinate or schedule communications between correct nodes. Thus the researchers opt not to make assumptions about adversarial nodes’ behavior as they are computationally bounded even though they are informationally unbounded and round-adaptive.Sybil Attacks occur when a single adversary can control a significant number of correct nodes within a network due to identity masking or occlusion. This paper concludes that the protocol can adopt any Proof of Stake strategy to protect against Sybil attacks instead of bake-in the Sybil resistance into the hardware layer, inherently making the process more labor-intensive and consuming energy.Slush is the metastable layer that is the foundation for the Snow family. It is not Byzantine Fault Tolerant but is Crash Fault Tolerant. Slush starts in an uncolored state, and upon receiving a transaction from a client, the uncolored node updates itself to the color associated with the query. The protocol then sends a query to the network to get a random but constant sized (k) sample, which receives the color of the node in the query, to then initiate another query repeating the sampling process to then repeat another query until the node collects k responses for a total of m rounds until the node finally decides the color after running the chosen number of samples and rounds.460×521 28.1 KBSnowflake augments Slush by adopting a counter to indicate the confidence of a node’s conviction in its known color, in addition to showing the number of consecutive nodes to maintain the same color.Snowball adds a confidence counter to record the number of queries that have surpassed the required threshold. This approach makes it unnecessary to keep track of the execution path through the DAG, as the confidence counter is the record of each point through the DAG.Liveness is achieved in this protocol using a leaderless initialization mechanism which operates independently, assuming the network is synchronized.Adding a DAG to Avalanche establishes an underlying structure that allows non-linear paths to be created between any points that have edges. This topology also makes it easier to assess conflict sets, as they are demonstrated in this image by the rounded gray regions.MethodUsing mathematical proofs to establish the basis for the protocols, the researchers used up to 2,000 virtual machine instances to prototype the proposed frameworks in a test net.The researchers built prototype versions of the protocols in Avalanche. They then used the Bitcoin transaction schema as the data input to be analyzed by the Snow protocols to test for internal concurrency. The researchers simulated a constant flow of transactions from both users and adversaries to test the prototypes’ liveness.As the researchers analyzed each protocol, the cryptographic bottleneck was analyzed to determine the difference in throughput in a system in which the cryptographic security was employed compared to the same protocols with the signature verifications disabled. Statistical analysis of the transactions per second at various node intervals was used to determine whether the number of nodes had a dramatic effect on throughput one way or the other.ResultsRunning batches of 20-40 queries, Avalanche only degrades roughly 1.34% when scaling the nodes from 125 to 2000Turning off the cryptographic signature check increased the TPS by almost 2.6x with a latency of roughly 0.4 seconds at most to confirm a transaction, revealing that cryptographic verification is the current bottleneck for this system.Discussion & Key TakeawaysIn comparing Avalanche to other projects like Algorand and Conflux, the Quorum-based Byzantine agreement found in Algorand and the DAG-enabled Nakamoto consensus in Conflux present fundamentally different approaches to achieving BFT in an attempt to improve on traditional Nakamoto consensus.Although the researchers are confident the Snow protocols offer promise for quiescent PoS protocols, there is much more research that needs to be done in the area before a provable solution is found.Implications & Follow-upsThis family of protocols offers an energy-efficient way to monitor states when no transactions are taking place. In PoW-based protocols, The need to have miners operating constantly to maintain concurrency becomes a long-term pitfall for maintaining data.The Snow protocols offer an energy-efficient leaderless Byzantine Fault-tolerant framework conducive to higher throughput and gives the option to disable cryptographic signatures to increase throughput.The tests show that the protocols scale with a limited reduction in transactions per second regardless of how many nodes are operating. The asynchronous nature of the protocols reduces the capacity for bottlenecks to occur as the network scales.ApplicabilityThis novel set of protocols will likely have a better fit in cases where data needs to be preserved on a distributed network, but does not need to be constantly updated even though the Avalanche protocol could handle many simultaneous queries.', ' Larry_Bates:Slush is the metastable layer that is the foundation for the Snow family. It is not Byzantine Fault Tolerant but is Crash Fault Tolerant. Slush starts in an uncolored state, and upon receiving a transaction from a client.I have a question about this uncolored consensus area in the metastable layer. Is the slush means that continue receiving transaction and lockup for a period of time? If two of the nodes sluch at the same time, which one is the broadcast leader at this time?', 'That is a great question!  If I read the paper correctly, the sampling is random and thus “leaderless”.  This is how it prevents collusion between nodes such that the transaction senders and receivers never know which nodes will be sampled for colors, thus it makes it near impossible to collude to trick nodes.', 'It totally makes sense! And it is fair enough to select random nodes as the leader. Thanks for your clear point.', 'No leaders. Each node comes independently to their own decision about what the rest of the network is deciding on.', ' Larry_Bates:This novel set of protocols will likely have a better fit in cases where data needs to be preserved on a distributed network, but does not need to be constantly updated even though the Avalanche protocol could handle many simultaneous queries.Why? I don’t agree. Updating data is the same as writing data, and writing data is just a transaction. It shouldn’t matter.', 'In that updating a google spreadsheet would become ridiculously costly compared to a free distributed spreadsheet if it was run on a blockchain whether the transactions were cheap or not.  It’s just not practical to expect businesses to implement blockchains as the basis for data tracking when distributed spreadsheets will suffice.It does matter when the cost of a transaction using a google doc is $0 when using any blockchain-based data tracking in which transactions incur a cost will be $0<.It’s just not practical to ignore the cost.Cost of recording a single transaction will definitely matter to companies that have high volumes of transactions that could be recorded in a spreadsheet for $0 for recording the transaction.  It wouldn’t be “immutable”, but they wouldn’t need it to be or else they would pay for immutability in a blockchain.When standards like HIPAA and Sarbanes-Oxley demand records be destroyed after a certain amount of time, costly immutable records will not be preferable.  This is why the space has to be cognizant of regulations that will make some technologies unusable in certain settings.Just as HIPAA lays out a timeline for the destruction of medical records, they specify the types of records that must be destroyed. These include patient charts and medical records as well as any other patient health information (PHI) that includes personal and confidential data.Here’s the list of information that must be properly secured and destroyed under HIPAA regulations:Patient namesDates (birth dates and other relevant dates)Geographic identifiersSocial Security numbersPhone numbersFax numbersEmail addressMedical record numbersHealth plan beneficiary numbersAccount numbersBiometric identifiers, including retinal scans or fingerprintsFull face photos and comparable imagesCertificate and license numbersDevice identifiers and serial numbersVehicle identifiers and serial numbers, including license plate numbersWeb URLsIP addressesOther unique identifying numbers, characteristics, or codesSo imagine trying to build a protocol that uses “immutability” while having to keep all of the above information “mutable”.Businesses are not going to pay for technology that makes destroying records impossible in cases where they are legally mandated to destroy records.This is where a protocol like those found in the Snow family would be good for pushing batch transactions for an individual organization, but trying to use it for individual transactions would still be too costly to be a practical solution for a “business” that could use a free and mutable alternative.  Obviously for tracking a network of users transacting currency p2p, this is going to be a superior method than a non byzantine-fault tolerant approach, but that is where the use-case is limited to specifically decentralized financial transactions and wouldn’t be preferable in orthodox business settings.', '……most of what you said is filed under “no duh” and I didn’t expect it to be considered relevant.Also, nothing is immutable. You can always edit the data. If you’re concerned about transaction history, it’s not required to be kept using Avalanche. However, it can be kept by any participating node, so if you need privacy, build a network with private membership.Again, “no duh”.I fail to see why anyone would consider using any BFT consensus protocol for the examples you provided. It’s as true for Avalanche as any other of its kind.With the statement now clarified, I don’t think the part I quoted was very pertinent or useful, and this long response is even less so.', 'You said you disagreed with my assessment of proper use-cases.I reinforced why they would not fit in many use cases.For the record, that assessment is the conclusion drawn by the researcher on the original full work.  If the work has changed since then and the use-cases have changed, that would be something to cover.', 'No no no no no…You said:This novel set of protocols will likely have a better fit in cases where data needs to be preserved on a distributed network, but does not need to be constantly updated even though the Avalanche protocol could handle many simultaneous queries.Which I disagreed with. You can constantly update the database being agreed on.“Updating data is the same as writing data, and writing data is just a transaction. It shouldn’t matter.”That’s what happened. I still don’t agree with the statement made. The intent is to write to the state of a database (no duh).You then went on a tear about some random HIPPA nonsense that was flat-out not relevant.I don’t think your assessment matches Gün, Kevin, Ted, etc’s conclusion in their paper or any intent of this (or any other) consensus protocol. But what do I know. I’ll ask Gun or Kevin or Stephen or Ted next time I talk to them. ;-)', 'Hi @collincusce.Welcome back to the forum. Pleased to see a spirited discussion but phrases like ‘no, duh’, characterizing what appears to be a good-faith response as a ‘tear’ and ‘nonsense’, then closing with an appeal to authority doesn’t further the conversation. I’d like to ask you to keep the conversation civil please.', 'Exciting summary, Larry.I am just wondering whether Bitcoin can successfully adopt the consensus mechanism. And if it can, what positive/negative effect would it have on scalability and settlement time?I like that the Snow protocols are more energy-efficient than existing solutions, but that is all theoretical, right? It would be interesting to see the protocols solving consensus problems soon.', 'Thanks for the question!  Based on the Taproot 1 deployment, I would assume adopting this type of protocol (if possible) would be something that be more an issue of consensus once there was a viable means of shifting the protocol.  As far as I know, the Snow protocols were being tested in a virtual machine environment for the sake of showing throughput. Although the timing of the original post was during the testing phases, the Snow protocols have been deployed on Avalanche’s main net.', 'Alright!That makes sense.Concerning the Snow protocols’ current implementation on Avalanche, I had no idea.Although, I guess I shouldn’t be too surprised. The names and their linkage to each other are obvious (snow, avalanche). Also, I read about what makes Avalanche’s settlement fast which is it’s metastability and leaderless BFT consensus.Pretty good work from the authors ']"
                Research Summary - What is in Your Wallet? Privacy and Security Issues in Web 3.0              ,https://www.smartcontractresearch.org/t/research-summary-what-is-in-your-wallet-privacy-and-security-issues-in-web-3-0/1225,Privacy,60,['http://arXiv.org'],"['summary', 'privacy']","[""TLDRDeFi has been perceived as a more secure, private, less centralized alternative to traditional financial systems, but DeFi applications may not be as secure as previously thought.Third-party applications embedded in DeFi applications and services may result in data leaks; script blockers can help to prevent trackers from operating but are not a permanent cure.The authors proposed a remedy to identified vulnerabilities by designing an in-browser patch for MetaMask to prevent third-party address leaks.Core Research QuestionWhat requirements do DeFi sites need to ensure user privacy and security?CitationWinter, P., Lorimer, A. H., Snyder, P., & Livshits, B. (2021, September 14). What’s In your wallet? privacy and security issues in web 3.0. arXiv.org. [2109.06836] What's in Your Wallet? Privacy and Security Issues in Web 3.0 9.BackgroundCentralized Finance (CeFi): Transactions in CeFi are handled through a central exchange. Centralized exchanges are vulnerable to security threats.DeFi: Collective term for public financial products and services that are publicly accessible, built on top of smart contract-based blockchain systems, and are not under the control of a single entity.Ethereum: A decentralized blockchain that implements automated smart contracts in a payment and transaction-oriented architecture.MetaMask: Available as a browser extension and mobile application; it is a commercially available software cryptocurrency wallet used to interact with Ethereum blockchain.Third-party scripts: Scripts embedded in a site from a third-party vendor. Can increase privacy, security, and performance concerns.Web tracking: A common way of tracking users’ activities across websites is through third-party cookies and alternative tracking vectors.Personal Identifiable Information (PII): Information that can identify a person or link a user’s real-world identity, e.g., name and demographic information.eTLD+1: Effective Top-Level Domain (TLD) +1 is the effective TLD and the part of the domain before it. For example, given a URL of https://my-project.github.io, the eTLD is .github.io, and the eTLD+1 is my-project.github.io, which is considered a “site”.Remote Procedure Call (RPC): Software communication protocol used for client-server-based applications to call processes on remote systems like a local system.Application Programming Interface (API): Software interface that allows two applications to communicate with each other.SummaryDeFi is claimed to be a more secure, private, and less centralized alternative to Centralized Finance (CeFi), but this claim is yet to be substantiated by adequate studies.DeFi applications suffer from similar privacy and security issues common to other parts of the web.The presence of third-party scripts on DeFi sites poses a substantial threat to sensitive information.On over 56% of the analyzed websites, a common tracker provided by Google was found to record Ethereum addresses.Third-party scripts can leak users’ Ethereum addresses, connect them to their PII, and track sensitive user browsing activities.Third-party scripts threaten the security of user funds as their ability to interact with connected wallet applications could open the door to phishing attempts or allow unauthorized transactions from the user’s wallet.Numerous DeFi sites use services like Google Tag Manager and Google Analytics to leak users’ financial activities and Ethereum addresses.The sensitive nature of DeFi sites magnifies these threats as they have custody of substantial amounts of user funds and a unique identifier in the form of an Ethereum address.Essentially, a DeFi site’s operation consists of a two-way interaction, one with the Ethereum blockchain, the other with the user’s wallet.MetaMask aids some of these interactions and allows DeFi sites to access users’ Ethereum addresses and balances, and to create transactions with a users’ permission.Although script blockers can help protect users from trackers, they are not a permanent solution to the threat posed by third-party scripts.Therefore, the researchers propose the implementation of a privacy-enhancing patch for the MetaMask in-browser wallet to remedy privacy and security vulnerabilities.MethodFirst, the researchers conducted qualitative studies to collect data from 78 sites on DeFi Pulse.Then, the sites were manually converted by the researchers to clickable URLs for easy access.Finally, using a puppeteer-based crawler, the researchers visited all listed sites and recorded every request the sites made (consisting of the requested context, the requested URL and the type of request) on a JSON file.Design of Countermeasure:A patch was developed to prevent third-party leaks of Ethereum addresses to DeFi sites. The patch hands out key-generated site-specific Ethereum addresses.The key generation procedure ensures thatDeFi sites see the same Ethereum address during repeated visits;accidental transactions on a user’s fake address can be undone;different sites see different addresses and third-parties cannot easily link the user’s addresses.The designed MetaMask wallet patch can protect users from third-party data leaks.ResultsEthereum Address LeaksTo see if users’ data would be leaked to third parties, the researchers recorded requests from popular Ethereum sites. They then looked for requests whose destination had a different eTLD+1 than the origin and whose URL contains their Ethereum address.Fig. 3 gives an overview of DeFi sites that leaked the Ethereum addresses. The results indicate that 13 out of 78, or approximately 17% of the sites leaked Ethereum addresses to third parties.Cross-origin DependenciesThis figure above gives an overview of the top ten third-party sites with the most embedded scripts on DeFi sites.When the researchers extracted script requests from their DeFi sites list whose destination eTLD+1 differs from the site’s origin, the study data showed that 48 DeFi sites, or 66%, embed at least one script from a total of 34 third parties, increasing the possibility of phishing attempts. Additionally, 56% of all analyzed DeFi sites embed at least one script provided by Google.Conversion analysis760×258 29.3 KBAs shown in Fig. 5, after assessing the position of third parties in a DeFi funnel and passing all relevant entities through a 1% selection criterion, five companies were selected. Google was the largest, with substantial reach across all three parts of the funnel.Discussion and Key TakeawaysLimitation of Access: False Ethereum addresses and private keys should be generated by DeFi sites to ensure third parties do not have access to the user’s real wallet address.Interception of Wallets: RPC calls of Ethereum wallets can be intercepted to replace fake wallet addresses with real ones to validate transactions and generate users’ wallet balances.Limitation of Countermeasure: The countermeasure is limited because DeFi sites are not prevented from searching out users’ real wallet addresses and are unable to intercept requests that do not use MetaMask’s Ethereum provider API.Implications and Follow-upsThe research examines the interconnection of privacy and security in DeFi applications and recommends solutions to the vulnerabilities identified, with two major limitations to consider.First, deliberately disguised leaks are difficult to reveal.Second, results are difficult to generalize due to the presence of selection bias.DeFi sites users should block analytic scripts and shouldn’t connect wallets except when necessary to improve their privacy and security.DeFi developers should use self-hosted analytics and consider addressing privacy as a first-class tool as well as reconsidering threat models.ApplicabilityThe researchers emphasize that the lightweight patch designed in the study is useful to protect users’ real Ethereum addresses from data leaks on the MetaMask in-browser wallet.Third-party scripts that expose users to serious threats can be circumvented if DeFi site developers stop using third-party scripts.Users who are concerned about their privacy and security can install the lightweight MetaMask in-browser wallet patch to access adequate protection."", 'A question for you (from @rlombreglia) The paper states that 56% of DeFi sites analyzed contain tracking scripts from Google that leak Ethereum addresses or other data that can link people to other sites including gambling or adult sites. How important/controversial is it to publicize this fact?', ""@Tolulope What a well-written summary! I will be doing some re-editing on mine after getting such a good writeup from you! Tolulope:On over 56% of the analyzed websites, a common tracker provided by Google was found to record Ethereum addressesTo address @jmcgirk 's question here as well, I find it vitally important to publicize this. It breaks the ethos of blockchain & crypto (in my opinion) that Google is still mining and analyzing our data in DeFi dApps. We can’t hold to principles if we don’t know when they’re being undermined. It’s also concerning as a user. Insightful as a developer (though not a coder myself).I appreciate how actionable the #applicability section is: Tolulope:DeFi site developers stop using third-party scripts Tolulope:Users who are concerned about their privacy and security can install the lightweight MetaMask in-browser wallet patch to access adequate protectionFor clarity too, I don’t believe there’s any other patch offered right now besides the upgrade that was initiated on February 15th. I’m seeing that the most up to date version of MetaMask is 10.9.3. Their Github hosts 10.10.0 and was released hours ago. It may include the patch described in the paper but I don’t see it (if someone else finds it to confirm). Your MetaMask wallet will update simply when you log out and log back in. Hope that helps the SCRF community!"", 'Recently the EU  found the use of Google analytics violates the 2020 Schrems II decision. Some countries in the EU are also warning that using Google Analytics may soon be illegal, while others are advising that companies start looking for alternatives to Google tools{1}. I think it is important that we create awareness on the fact that 56% of DeFi sites analysed in the paper used at least one script provided by Google alongside the potential risks involved with their use. Creating such awareness will help people, especially Web 3 users and developers, conceptualise the extent of risk they are in and learn how to mitigate such risks.{1} US-EU data transfers on life support after French Google decision. (2022, February 10). POLITICO. US-EU data transfers on life support after French Google decision – POLITICO', 'The problem is real, and the solutions offered are simple and easy to apply. Interesting how one of the third-party sites is a .net facebook domain rather than .com. The generation of false addresses seems to make a lot of sense for avoiding phishing attempts. Kind of similar to how we can poison data in AI which may be used for both good and nefarious purposes.I’d say poisoning the data is good if the model is being used for nefarious purposes such as in the case of throwing off the models used for unwanted surveillance. Kind of on a tangent - I remember in the video game “Watchdogs 2” they were trying to use mass surveillance AI to find Marcus, and the hackers threw off the surveillance by making 40 different Marcus’s walking just around the SF bay area. Naturally, data poisoning can also be bad if used to provide false authentication, or for things like deepfakes. To me, it’s slightly surprising that there are no universally used methods for proving the legitimacy of data. This is something that blockchain has largely already made possible. We can use blockchain for verifying whether a video was actually recorded in a specific place and, with the timestamp, at a specific time, thereby preventing deepfakes.@zube.paul @Larry_Bates tagging for if you’d like to carry our discussion last month to the forum', 'In this context it’s interesting that Google is now creating a blockchain division under the Google Labs group: Report: Google Launches Blockchain Division – Blockchain Bitcoin News 3In the past, Google has shied away from crypto in general, but this is less and less the case now. Without namely ‘endorsing’ crypto or supporting it as a means of commerce on its own platforms, this branch seems to be primarily focused on the distributed computing elements of blockchain.', 'This research is handy especially now that data protection is a thing all over the world and could see DeFi operators fined for such privacy breach. Welldone @TolulopeWill the fake wallet address generated still lead to identification of the user whether directly or indirctly?If it does, there maybe need to introduce anonymisation technique so as to take it away from the ambit of data protection.', 'Thank you for your comment @Samuel94The purpose of the fake address is to disassociate the wallet owner’s identification and footprint from their original address; therefore, the fake address shouldn’t lead to the identification of the wallet owner under normal circumstances.', 'That was really a nice summary on web3.0 security and privacy @Tolulope. For me,I believe that Web 3.0 is a given, and generally speaking, there are more reasons to be optimistic than negative not minding the security issues and privacy as regards to users data’s. However, firms,users and developers who wish to take part in Web 3.0 must be aware of the security concerns involved. I still believe that no system is perfect, and anonymity can make it difficult to identify and prosecute hostile actors or even to recover monies that have been taken.For identity issues, Web 3.0 improves upon many of Web 2.0’s privacy concerns, but anonymity and decentralization also have a downside. For one, anonymity makes it difficult to hold bad actors accountable for their actions and offers little to no protection for consumers. Furthermore, anonymity makes regulation more difficult and simplifies money laundering and terrorist funding In addition, decentralized identification complicates current regulations like GDPR and makes it difficult to discern user identity for data controllers. Finally, most self-sovereign identity (SSI) and crypto wallets require a lengthy security onboarding process, making widespread adoption more difficult and less secure.', 'The issue of security in DeFi can never be overemphasized. If you have ever experienced theft, extortion, or cyberbullying resulting from the leakage of personal identifiable information,  you will understand  better.I like the solution proposed by the researchers, because in solving a problem it is always advisable to start from the root cause. Considering the problem from a wallet perspective will solve the vulnerability better. Relying on script blockers is like buying time to avoid a problem. In the end, the problem will resurface, somehow.As a second layer of security, it is advisable that anyone interacting with DeFi sites have a separate wallet for that. Taking this precaution can save one a lot of drama too.@Tolulope You did a good work here. I’m curious, it looks like only DeFi sites hosted on Ethereum blockchain were studied? Have an idea why? I was hoping to see other blockchains so as to get a bird’s eye view of the security threat. That doesn’t mean this is not comprehensive anyway.For the sites selected, were there criteria laid down for the selection?', 'Thank you for your comment @Cashkid18. I agree with you on the pros and cons of anonymity which you have mentioned. Would you agree that anonymity could better preserve people’s privacy and that people’s privacy is more important vis-a-vis money laundering and terrorist funding regulations?', 'Thank you for your comment @Ulysses. I believe the researchers may have decided to use DeFi as their sample population due to the fact that they found that most of the privacy and security issues in DeFi apps are those that can be found in other parts of the web. If you would like to read more on privacy issues in Blockchain generally, I think you may find this paper helpful - Blockchain Access Privacy: Challenges and Directions | IEEE Journals & Magazine | IEEE Xplore 1', 'Great work @Tolulope well i agree that for the purpose of thwarting phishing attacks, the creation of fake address is a good approach to this.Web 3.0, in my opinion, is a given, and generally speaking, there are more positive reasons than negative ones, security concerns and user data privacy aside. To participate in Web 3.0, businesses, users, and developers must be aware of the security issues involved.DeFi offers a wide range of possibilities. Regulators, investors, and the financial markets, however, also face significant risks and difficulties as a result.Consider in a DeFi undertaking is constructed inside the network. people work with each other at once, for reasons to navigate their economic choices. That said, the platform is susceptible to bugs inside the clever agreement, that’s a self-executing agreement that bureaucracy the premise of DeFi tasks. Well, DeFi platforms are encountering more hacks than ever some time recently as request for decentralized back (DeFi) applications rises. DeFi hacks have fetched clients more than $2.0 billion so distant in 2021, making token security more critical than ever. According to me, I would say there isn’t a straightforward way to bargain with such dangers.', '@Tolulope Because it is always appropriate to start by addressing the underlying cause of the problem, I simply appreciate the arrangement that the experts have suggested. The powerlessness will be much more clear when the problem is viewed from the perspective of the wallet. Although relying on script blockers is like buying time to avoid a problem, the problem will somehow reappear in the end.Web 3.0 addresses many of Web 2.0’s security concerns in terms of character, but secrecy and decentralization also have a downside. For starters, anonymity gives little to no security for buyers and makes it difficult to hold bad fictional characters accountable for their deeds.Additionally, concealment complicates management, changes money laundering, and disrupts funding for psychological demonstrators. Decentralized recognizable proof complicates present directions in terms of expansion.', 'Hi Tolu, for me privacy and anonymity are different things which serves different purposes.When we are talk about privacy, we are referring to a situation where the identity of the user is known but what the user does or doing is protected or not known. You can say that the user’s data is protected from the public eyes.Then, when it comes to anonymity, we are referring to a situation where the the identity of the user is protected and not known but what he does is seen. That’s where tools like tor and VPN comes in, to hide our identity but people will still see what we did on the internet but they won’t know who did it.If you are talking about preserving our privacy better then that’s where encryption comes in and it should be a very strong encryption at that in order to protect and encrypt users data better.For your second question, I don’t believe that privacy is more important when it comes to a case of using the privacy to commit crimes like money laundering and terrorist activities. Apart from such scenarios, privacy is very important and should be protected.', 'Thank you for your comment @WaterLily', ' Cashkid18:For your second question, I don’t believe that privacy is more important when it comes to a case of using the privacy to commit crimes like money laundering and terrorist activities. Apart from such scenarios, privacy is very important and should be protected.I agree with you. Thank you for your comment @Cashkid18', 'Great research summary  @Tolulope thanks for taking your time to write on this …diverting from this question Tolulope:What requirements do DeFi sites need to ensure user privacy and security?I would like highlight an important step users can take to stay safe using DeFiVerifying if a dApp and smart contract has undergone an audit, is an important step users should take.Numerous programs and websites analyze various dApps to look for security holes, reliable developers, or other potential problems. The following is a list of various auditing tools you can find helpful while looking into dApps:DeFi Safety is a website where reports and safety ratings for DeFi projects are posted.Solidity finance - website that determines whether a protocol has been audited or not is.Rugscreen is a website that can determine whether a protocol is a known rug pull or fraud.Coinsniper is a website that provides further details on how to avoid frequent DeFi frauds.Smart contract audit helps to prevent code vulnerabilities.', 'Thanks for the advice on the user side, @kingdamieth. To the original point that was part of the summary, however, do the developers of DeFi projects also have an obligation here? Would the user advice that you are providing here be sufficient to identify things like the third-party scripts being discussed here?', 'Thanks for taking me back to the main point of the summary … surely developers of DeFi projects have an obligation in relation to their privacy and security issues. Although answering this question, zube.paul:Would the user advice that you are providing here be sufficient to identify things like the third-party scripts being discussed here?It is quite insufficient. This is because both Users and Developers have to play their part as they both bear responsibilities for DeFi safety. Due diligence is the best strategy for investors to manage DeFi risks. Likewise, projects should pass routine security checks and finish the audit of their smart contracts before listing in order to reduce the danger of DeFi attacks. Additionally, technical companies release fresh patches and upgrades for DeFi systems so they may fix security flaws before hackers discover them. The better a project’s chances are of preventing a DeFi breach, the quicker it installs these security patches.Key security tips DeFi developers should take i to consideration2022-01-24-17.48.50-min1200×676 68.4 KB']"
                Open Peer Review Proposal | Request for Feedback              ,https://www.smartcontractresearch.org/t/open-peer-review-proposal-request-for-feedback/1660,"Grant Proposals, Bounties and Awards",42,[],"['about', 'discussion', 'governance']","['Open Peer Review Proposal | Request for FeedbackPeer review has historically been a mechanism within the scientific pipeline for filtering out low-quality research and providing feedback for improvement. Rigorous review has traditionally only been available to those striving for publication in a journal. However, many independent researchers, especially in Web3, do not have publication as a goal. This post outlines a proposal for an open peer review pilot to be implemented at the Smart Contract Research Forum (SCRF) that provides rigorous review. As part of this, we want to understand what crypto-native peer review would look like. We begin by paying our reviewers and will seek to explore how emerging technology could benefit the peer review process.Feedback on the process outlined below is requested and appreciated in order to help improve.The creation of this process involved consideration of the sources collected in this knowledge repository 4.Our goals with this pilot are to:Increase the avenues to feedback for independent researchersExplore the effects of financially incentivizing public peer reviewCollect and share data from the processIdentify how to improve our designGrow our international community of academics, researchers, developers, and industry expertsOur process:In the spirit of creating an OPEN peer review system, the author’s research and the peer reviews for that research will be publicly available on the SCRF Forum 1.Our process is divided roughly into three steps:Screen & SelectPost & ReviewSynthesize ResultsScreen and Select:In this step, we identify the researchers and corresponding reviewers who will be the core participants in our open review.Researchers: We will source web3 researchers from our network (and by our we really mean @eleventh19’s) who are working on problems in web3 such as DAO Governance, cryptoeconomics, and blockchain who have research ready for review.Reviewers: For each paper, we will recruit at least three reviewers who are knowledgeable about the subject area. We will tell the reviewer the author’s name so that they can self-report whether or not they have any conflicts of interest.Meta-reviewer: For each paper, we will recruit one meta-reviewer who is responsible for evaluating the quality of the reviews. Evaluating the quality of reviews is important so that we have a metric to determine how different variables influence the quality of a review.Post and Review:In this step authors share their research, reviewers share feedback, and authors respond.Authors will visit smartcontractresearch.org and make an account if they do not already have one. They will create a new topic in the ‘Peer Review’ category that includes a post with their research to be reviewed. Research can be posted directly on the forum as text, PDF, or as a link to where the research can be read.Reviewers are requested to follow the instructions in Appendix A, sign their review, and post it on the forum during a narrowly-defined window of time to ensure authors get feedback in a timely manner.Authors are requested to respond, rate the review, and follow the instructions in Appendix BMeta-reviewers are requested to respond, rate the review, and follow the instructions in Appendix C.Templates to use will be provided based on the items in the Appendix.We select reviewers to ensure that the authors receive quality feedback from multiple sources with deep insight. However, in the spirit of open peer review, we strongly encourage anyone to leave reviews on the papers.Synthesize Results:In the final step, we will aggregate reviewer ratings into an overall view of the paper and conduct interviews with all contributors. The purpose of the interview is to understand the contributors’ perspectives on the process we used, how the public nature affected them, and how much the financial or reputational incentive motivated them.Lastly, we will payout reviewers and meta-reviewers. In this first round, we will pay out $500 for their contribution to making better science. $500 was chosen as a baseline partly in order to be in accordance with the 450-movement 7, adjusting for inflation of course.To Conclude:This is the first round of open peer review. We plan to experiment with many variables (incentive structure, review format, the anonymity of authors & reviewers, etc.) in order to find a peer review process that works best for all parties and leads to the highest quality research. Throughout our efforts, we hope to provide peer review for independent researchers who would not normally have it, provide financial rewards to peer reviewers for their time, and to collect data that will help all peer review designers create an optimal process for their objectives. We look forward to working openly with the community throughout this process!Acknowledgment: We’d like to thank Dr.Nihar Shah for his time, advice, questions, and an excellent resource surveying peer review.This process is still a draft and we would sincerely appreciate your feedback to help us improve all parts of this process. Please leave any feedback about this process on this post. Appendix A: Instructions for Reviewers Thank you for reviewing a research paper as part of the Smart Contract Research Forum’s Open Peer Review Project! We appreciate your contribution to the scientific discourse of web3 research. We humbly request from all reviewers a moment of your time:Please familiarize yourself with these guidelines before writing your review:Please remember that we are not asking for an accept or reject decision as we are not a journal publication or a conference proceeding. We are instead creating a forum for scientific discussion and improvement of research.Please provide feedback on the scientific content of the paper.As a peer reviewer, your role is not to proofread or check for typographic errors but to understand the author’s scientific intent and any potential missteps.When you find flaws, please help the author overcome them by suggesting how to fix them or by suggesting alternative approaches.Please be empathetic and professional. As part of fostering a learning environment, we want to avoid any rudeness or arguments that attack the person rather than the position of the paper.Please keep in mind that peer review is not just beneficial for the author, but it is beneficial for you as well! Reviewers report that peer reviewing helps them:advance their own research by stimulating new ideas and becoming aware of new methodsstay informed with the latest research in their field before their peersreduce misinformation from being disseminated into the worldPlease remember that your reviews may be relied upon not just by the author of the paper but by readers of it in attempting to critically assess it.Bear in mind, that your reviews and your name will be publicly available to anyone reading the Smart Contract Research Forum. You can add your reviews to your CV since anyone can access them. We encourage you to take the time to make a review you will be proud of.Please share your review within two weeks after the research has been posted.Please include the following in your review:Disclosures of any competing or conflicting interests including but not limited to: organization affiliations, competing research theories, prior beliefs on the topic, funding sources, knowledge of the author(s), etc.A brief summary of the paper, its objective, and findings.Suggested revisions to improve the legitimacy and credibility of the substance of the paper.Strengths of the paper.Answers to the following multiple-choice questions. When answering these questions, copy and paste the questions into your review and delete the choices you do not select. Please provide a brief explanation of your selection.Please rate this paper on scientific correctness (does the research have an identifiable question or objective? Are sound methods and/or statistics used? are the conclusions logically based on the results?).I am confident that the paper is technically sound, and I have carefully checked the details.The paper appears to be technically sound, but I have not carefully checked the details.The paper has minor, easily fixable, technical flaws that do not impact the validity of the main results.The paper has major technical flaws.Please rate this paper on organization and clarity of language (is the writing easy to follow? are concepts well-explained? is the document well-organized?),The paper is well-organized and clearly written.The paper is well organized but the presentation could be improved.The paper is somewhat clear, but some important details are missing or unclear.The paper is unclear and very hard to understand.Please rate this paper on openness and reproducibility (is the data available? is the code? are the methods explained clearly enough to be replicable?)Key resources (e.g., proofs, code, data) are available and key details (e.g., proof sketches, experimental setup) are comprehensively described for competent researchers to confidently and easily reproduce the main results.Key resources (e.g., proofs, code, data) are available and key details (e.g., proofs, experimental setup) are sufficiently well-described for competent researchers to confidently reproduce the main results.Key resources (e.g., proofs, code, data) are unavailable but key details (e.g., proof sketches, experimental setup) are sufficiently well-described for an expert to confidently reproduce the main results.Key details (e.g., proof sketches, experimental setup) are incomplete/unclear, or key resources (e.g., proofs, code, data) are unavailable.How confident are you in your understanding of this paper? Why?I have checked all points of the paper carefully and was sure that I knew exactly what the author meant in their writing. I am certain I did not miss any aspects that could otherwise have impacted my evaluation.I checked the important points carefully and felt pretty sure I understood what the author meant in their writing. It is unlikely, though conceivable, that I misunderstood some aspects that could have impacted the quality of my evaluation.There’s a chance I misunderstood some aspects of the paper that I do not have the depth of knowledge required to confirm what the author writes is accurate. Moreover, I may not have carefully checked some of the details, e.g., proof of a theorem, experimental design, or statistical validity of conclusions.I am able to defend my evaluation of some aspects of the paper, but it is quite likely that I missed or did not understand some key details.This paper was very confusing to me. My evaluation is an educated guess.Any other questions or comments about the research which was not captured here?Please share any thoughts on how we can improve the peer review process in this forum post. Appendix B: Instructions for Authors Responding to Reviews Thank you for evaluating reviews of research papers at the Smart Contract Research Forum! We appreciate your contribution to the scientific discourse of web3 research. When responding to reviewers we humbly request that you do the following:Please familiarize yourself with these guidelines before writing your response:Please remember that reviewers are attempting to help authors improve research and have dedicated a significant amount of time to doing so.Please respond to each reviewer individually and make it clear which revisions you are implementing and where in the paper you have done so.Please be empathetic and professional. As part of fostering a learning environment, we want to avoid any rudeness or arguments that attack the person rather than the position of the paper.Bear in mind, that your reviews and your name will be publicly available to anyone reading the Smart Contract Research Forum. You can add your reviews to your CV since anyone can access them. We encourage you to take time to make a review you will be proud of.Please include the following in your response:Disclosures of any competing or conflicting interests including but not limited to: organization affiliations, competing research theories, prior beliefs on the topic, funding sources, knowledge of the author(s), etc.A response to suggested revisions and for each suggestion an explanation of if the revision was implemented and howAnswers to the following multiple-choice questions with a brief explanation of your ratings for each:Please rate this review on the level of understanding it showed of the researchThis review showed a thorough understanding of the research objective, methods, and conclusionsThis review mostly understands the research objective, methods, and conclusions but shows that it may have missed some small detailsThis review partly understands the research objective, methods, and conclusions but misses significant detailsThis review does not understand the research objective, methods, and/or conclusionPlease rate this review on toneThis review makes use of appropriate language and is written with complete professionalism and respect,This review is written with professionalism and respect but is occasionally condescendingThis review is written with some professionalism and respect but makes use of inappropriate language or is condescendingThis review is written with little professionalism and respectPlease rate this review on helpfulness (did it help you strengthen your paper?)This review provided clear areas to strengthen the substance of the paper and easy-to-understand descriptions of how to improveThis review provided clear areas to strengthen the substance of the paper and descriptions of how to improveThis review provided areas to strengthen the paper and/or descriptions of how to improveThis review did not provide substantive or clear areas to strengthen the paperAny other questions or comments about the review which was not captured here?Any thoughts on how we can improve the peer review process? Please share them in this forum post or in our final interview/surveyAppendix C: Instructions for Meta-reviewersThank you for evaluating reviews of research papers at the Smart Contract Research Forum! We appreciate your contribution to the scientific discourse of web3 research. When responding to reviewers we humbly request that you do the following:Please familiarize yourself with these guidelines before writing your response:Please remember that reviewers are attempting to help authors improve research and have dedicated a significant amount of time to doing so.Please be empathetic and professional. As part of fostering a learning environment, we want to avoid any rudeness or arguments that attack the person rather than the position of the paper.Bear in mind, that your reviews and your name will be publicly available to anyone reading the Smart Contract Research Forum. You can add your reviews to your CV since anyone can access them. We encourage you to take the time to make a review you will be proud of.Please include the following in your response:Disclosures of any competing or conflicting interests including but not limited to: organization affiliations, competing research theories, prior beliefs on the topic, funding sources, knowledge of the author(s), etc.Answers to the following multiple-choice questions with a brief explanation of your ratings for each:Please rate this review on the level of understanding it showed of the researchThis review showed a thorough understanding of the research objective, methods, and conclusionsThis review mostly understands the research objective, methods, and conclusions but shows that it may have missed some small detailsThis review partly understands the research objective, methods, and conclusions but misses significant detailsThis review does not understand the research objective, methods, and/or conclusionPlease rate this review on toneThis review makes use of appropriate language and is written with complete professionalism and respect,This review is written with professionalism and respect but is occasionally condescendingThis review is written with some professionalism and respect but makes use of inappropriate language or is condescendingThis review is written with little professionalism and respectPlease rate this review on helpfulness (did it help you strengthen your paper?)This review provided clear areas to strengthen the substance of the paper and easy-to-understand descriptions of how to improveThis review provided clear areas to strengthen the substance of the paper and descriptions of how to improveThis review provided areas to strengthen the paper and/or descriptions of how to improveThis review did not provide substantive or clear areas to strengthen the paperAny other questions or comments about the review which was not captured here?Any thoughts on how we can improve the peer review process? Please share them in this forum post or in our final interview/survey', '@UmarKhanEth I really appreciate you posting this and working toward your project in general.Overall, I think this is a good initial protocol, but I did have some clarifying questions that I hope you can answer.From the Screen and Select section, I’m interested in some of that selection process. It does make some sense that a convenience sample of researchers would be pulled in through a network connection, but I am curious how you will be selecting how reviewers and meta-reviewers will be selected.I’m also particularly interested in the meta-reviewers as I have seen systems like this elsewhere. Is there a training procedure they are going through or anything like that? The templates in the appendixes are likely helpful, but I wonder if getting to some type of consensus on how to use them is part of your plan?I’m also interested in how you might address “from the wild” type of reviews. There are planned reviewers and meta-reviewers in this proposal, but what happens to reviews from people not involved directly in the project? As this will be on the forum here, any forum user could presumably also use the templates and offer a review. Is there a plan for that type of interaction?', 'Do you have a sense of how incentivization would work with this model? I’m particularly interested in when you think it would be best to introduce compensation, would it be a sourcecred-like model where you’re rewarded periodically based on the performance to date (which might maximize engagement, etc) or would steer toward something more like an honorarium, paying them in advance for their work. Also interested in the scale of compensation, since this is historically something that’s considered almost like a monastic pursuit, do you think we should pay a fair market rate for people’s time or would this be more like a sweetner, a little extra to make the emotional labor a little more palatable.Aside from my questions, @UmarKhanEth I also wanted to thank you for bringing this to the forum and your obvious hard work on this! I’m really excited to see how this all plays out!', '@UmarKhanEth Echoing @zube.paul and @jmcgirk in thanking you for this contribution to the forum. You say in your opening paragraph… UmarKhanEth:Rigorous review has traditionally only been available to those striving for publication in a journal. However, many independent researchers, especially in Web3, do not have publication as a goal.I appreciate that conventional publication is something these authors might be trying to “rise above,” so to speak. But in that case, what exactly is their motivation, other than to improve their own thought and articulation process? Do they wish to short-circuit the protracted publishing process and get their work into the hands of industry more quickly for real world results, for example? Raise the level of community discourse and coherence? Please tell us a bit more.Regarding reviewer incentives, I thought you made it clear that you were paying $500 per review, starting immediately. Is there more to that part of the story?', 'Hey @zube.paul, @jmcgirk and @rlombreglia! Thank you all for your engagement and questions! zube.paul:It does make some sense that a convenience sample of researchers would be pulled in through a network connection, but I am curious how you will be selecting how reviewers and meta-reviewers will be selected.This is a great question that has been a challenge for us. At least for our first round of peer review, we’re planning on mimicking journals and doing this very similarly to how we’re selecting researchers: by leveraging existing relationships and network connections.  As a backup, we are also sending out a survey to gauge general interest from online communities. We’ll be posting this in group chats, forums, etc. Here is a link: https://forms.gle/xwBBpAGBf2UQKNJb7The more, the merrier :)What do you see as alternative ways of finding reviewers? zube.paul:I’m also particularly interested in the meta-reviewers as I have seen systems like this elsewhere. Is there a training procedure they are going through or anything like that? The templates in the appendixes are likely helpful, but I wonder if getting to some type of consensus on how to use them is part of your plan?We don’t currently have any training planned. But this is a great point and something we will need to think about doing. There is an interesting study that showed junior reviewers, when trained, had 30% of the highest-rated reviews (as determined by meta-reviewers) at a conference while other reviewers, not trained at that conference, only had 14% of the highest-rated reviews. The value of training to improve quality is clearly demonstrated.I wonder what a training program for meta-reviewers may look like. Have you seen something like this before? My initial thought is to provide examples of reviews and ratings they should receive to ‘calibrate’ meta-reviewers. zube.paul:I’m also interested in how you might address “from the wild” type of reviews. There are planned reviewers and meta-reviewers in this proposal, but what happens to reviews from people not involved directly in the project? As this will be on the forum here, any forum user could presumably also use the templates and offer a review. Is there a plan for that type of interaction?Yes! We encourage open reviews from anyone. The reviewer will qualify for a grant just like our recruited reviewers, so long as it is of the same quality. (i.e, spam reviews won’t be rewarded). jmcgirk:Do you have a sense of how incentivization would work with this model?We’re keeping it simple to start and offering a ‘Peer Review Grant’ of $500 to those who successfully complete a review. Meta-reviewers will also be given a grant, of the same amount, for every three reviews they review. jmcgirk:Also interested in the scale of compensation, since this is historically something that’s considered almost like a monastic pursuit, do you think we should pay a fair market rate for people’s time or would this be more like a sweetner, a little extra to make the emotional labor a little more palatable.This is a great point and a difficult one. So far, our thinking has been that individuals’ hourly rates can vary wildly. Some individuals will charge hundreds of dollars per hour of work and this may not be something we can support. We’ve tried setting a baseline of ‘fair’ compensation for the amount of work by offering $500. For high-earning individuals or those that do not want to be paid for review, they will have the option of sending this money to a charity of their choice.In the 450 movement, the author arrives at a conclusion that $50 for 9 hours of work or $150 for three hours of work leads to $450 both ways and would be a fair rate. This is in line with a study that shows, on average, a peer review takes 6 hours. A rate of 75 per hour for six hours also leads to $450. We’ve slightly increased the amount of funds to $500.Part of the difficulty with this question is also that there is historically no agreed-upon market rate for peer review. We’re throwing a bit of a shot in the dark but it’s the best target we have right now based on what we’ve been able to learn. I’d love to update this number based on what others’ think or our findings from the first round. rlombreglia:I appreciate that conventional publication is something these authors might be trying to “rise above,” so to speak. But in that case, what exactly is their motivation, other than to improve their own thought and articulation process? Do they wish to short-circuit the protracted publishing process and get their work into the hands of industry more quickly for real world results, for example? Raise the level of community discourse and coherence? Please tell us a bit more.Part of what we’ve seen is a lot of researchers posting directly online to pre-print platforms like arXiv or SSRN, or in some cases to blogging sites like Mirror or Medium. This is definitely a lot faster than going through a publication’s peer-review process which can often take months or years. Yet, exposing scientific analyses to criticism is a crucial part of the scientific process.This growing trend of directly posting papers online is something we’ve seen particularly in the Computer Science and Physics communities. Separately, we’ve also seen it among Web3 researchers who work at DAOs instead of Universities. While being published in a journal is something the academic system rewards heavily with things like tenure or grant funding, it doesn’t appear to have the same incentive in Web3. Here, it may be enough to get hired to send someone a link to your writing so they can become familiar with your thought process or thinking on a specific matter.I do hope researchers who participate in peer review on SCRF are motivated to receive feedback that helps them improve their research. This is really what it’s all about – the pleasure of finding things out, the seeking of more precise, more accurate truth. I hope I don’t sound too naive ', 'I’d like to share some literature summaries we’ve been working on, motivated, inspired, and pushed by the researchers on this forum.They can all be found (including links to the paper) on our knowledge repo 1.Here are some TL;DRs:Shah, Nihar B. Challenges, Experiments, and Computational Solutions in Peer Review.  Communications of the ACM, Vol. 65 No. 6, Pages 76-87TLDR:Peer review is supposed to be an objective, rational, and fair process for improving research and filtering out bad research. However, it is challenged by the biases of the humans involved and often has subjective or insufficient outcomes. Studies with fictitious manuscripts have found that peer reviewers only detect between 1/3 and 1/4 of errors on average.When running a peer review, it is difficult to find reviewers with the right expertise, to prevent dishonest behavior, and to account for the biases and differences of people involved. These biases include preferring to see positive results, especially those which confirm the reviewer’s prior beliefs, by top authors or institutions.There are some computational solutions being explored to solve these problems. However, there is often insufficient data because reviews are anonymized, not shared, and do not have a ground truth decision to compare to.List, B. Crowd-based peer review can be good and fast. Nature 546, 9 (2017).TLDR:A synthetic chemistry journal tests collaborative, crowdsourced peer review with 100 anonymous experts and finds feedback is faster and more comprehensive than traditional peer reviewScientific Autonomy, Public Accountability, and the Rise of “Peer Review” in the Cold War United StatesMelinda Baldwin Isis 2018 109:3, 538-558TLDR:Peer Review as we know it today is a relatively recent phenomenon. While journal editors have often asked other experts for advice and occasionally forwarded segments of criticism to authors since the late 1600s, it has not become standard practice to consider peer review the distinguishing factor between science and not science until the late 20th centuryThe founding of the NIH in 1948 and of the NSF in 1950 saw the amount of funding for scientific research increase by a factor of 25 in just five years. By the 1970s this greatly increased the scrutiny scientists were under, especially by Congress, regarding how they decided where to allocate grant funds.With a desire to continue influencing funding decisions, Scientists cast ‘peer review’ as “the crucial process that ensured the credibility of science as a whole.“2021. F1000 Research Article outlining the market rates for publishing scholarly articles. By Alexander Grossman and Bjorn BrembsTLDR:Journal subscription rates have risen drastically over the past decade. In response, the authors provide a quantitative approach for determining the costs of publishing a scholarly article based on step-by-step calculations and numbers shared publicly or privately by publishers and service-providersCosts range from less than $200 per article to about $1000 per article, depending on the size of the publisher, the use of pre or post-publication peer review, the rejection rate, and implemented technology.With average subscription pricing of around $4000, these results indicate that publishers typically spend around $2200 on non-publication costs in order to maintain their reported 30% profit margin or $1200 per article.Greaves, S., Scott, J., Clarke, M., Miller, L., Hannay, T., Thomas, A. & Campbell, P. (2006). Nature’s trial of open peer review nature international weekly journal of science.TLDR:Nature ran an open peer review trial in 2006 which gave authors the option to allow online, public comments on their paper to be a part of the editor’s consideration in publishing (in addition to ordinary peer review)While expressed interest was high, participation was low among authors and there were sparse comments. 5% of authors agreed to open peer review and 38/71 (54%) of those papers received comments(2006). Peer review and fraud. Nature, 444, 971-972.TLDR:After ‘Science’ published a fraudulent paper about human embryonic stem cells, Nature assesses the risk of fraud and how to prevent it. Could Open Peer Review have caught and prevented the publication?The answer is No, at least based on Nature’s 2006 trial in which participation was too low because researchers were more focused on writing papers than volunteering to leave open, public reviews of their peers', 'After ‘Science’ published a fraudulent paper about human embryonic stem cells, Nature assesses the risk of fraud and how to prevent it. Could Open Peer Review have caught and prevented the publication?What do you think of the idea of working in some kind of credibility score into the system? DARPA was working on a machine learning model designed to predict whether papers would be reproducible or not - https://www.darpa.mil/program/systematizing-confidence-in-open-research-and-evidence', 'This is super cool. Thanks for sharing! I’m really curious to know how they implemented this.I’m in favor of credibility scoring – I think when done right it can make a subjective peer review process much more objective, especially when combined with AI/ML. I would love to implement something like this here but it would require a dataset of peer reviews and scores to train on. I’m aware of few such public datasets. This is one of the biggest problems facing peer review – a lack of trusted anonymization technology to allow those who run the peer review process to release the reviews. Maybe something ZKP will be able to help with one day (EDIT: or more open review!)', 'This proposed peer-review practices really held my attention. I am particularly interested in the role of the meta-reviewer. This aerial view will require special skills in both deconstruction and synthesis/integration.Has there been any discussion about a system for protection of human participants in research or is that presumed to be the author’s responsibility? Please forgive the oversight if discussed above, but I’m taking in lots of new information at once.', 'Hello @kdouglass! Thank you for your comment and question!As you’ve highlighted, the metareviewer is a challenging role to fill and one of the most critical.There has not yet been much discussion about protecting human participants in research. This is one area we could do more to encourage reviewers to be critical of the author. What sorts of protection do you think we should expect of researchers?I think it would be fair to leave the responsibility to protect human participants to the researcher and then ensure they are behaving ethically. Perhaps, to be proactive, we can share some guidelines with the researcher based on what other orgs have already come up with.', 'Sharing guidelines is a good start. That brings awareness to researchers they should at least be thinking about how they collect data or use existing datasets.The following question helps me frame this conversation: what type of infrastructure best structures the Forum’s relationships with researchers, institutions (potential partners) and with the broader community?We can think of a research-intensive, academic institution, e.g., as a ready-made infrastructure for the person who conducts research. The institution equips the researcher with a range of tools:institutional reputation,data collection and analysis software,meeting spaces,collaboration tools,grant administration andtravel funding.This infrastructure also includes tools to protect human participants in the researcher’s research. An apparatus and protocols for protection of the human participants in research also protects the researcher. These tools protect researchers from personal lawsuits, for example.The research protection tools support the relationship between broader community and the institution as it carries out its “third mission”. Good policies and practices are good public relations.While researchers affiliated with SCRF could assume some responsibility for protections of human participants, this is highly subjective from person-to-person. Since many institutions and other online entities have already instituted systems of practices and protocols, SCRF can outsource protections for human participants. This outsourcing would need to involve a clear understanding of whose guidelines/process will be followed and how the processes will be documented with SCRF. This would provide the clear pathway/infrastructure researchers need to do their work. They would have clarity on, for example, the definition of consent and clarity on when they might be unintentionally involving human participants in research, without the participants’ consent.Being clear on the process for protecting human participants in research, even if outsourced, is a proactive risk mitigation strategy. It would allow SCRF to self-regulate. Also, it would protect the organization from being blind-sided with abuse claims, although researchers are acting independently. Finally, having a predictable, reliable process would give SCRF access to data about how protection of human participants in Web 3.0 research is developing.', 'Hi Umar, excited to see this open peer review project being facilitated! One thought I have in my mind about the incentive UmarKhanEth:We’re keeping it simple to start and offering a ‘Peer Review Grant’ of $500 to those who successfully complete a review. Meta-reviewers will also be given a grant, of the same amount, for every three reviews they review.So maybe could we have a tracking system on how many work one has reviewed and being reviewed, and maybe the reputation they build in the open peer review system is also some incentive for the work?', '@kdouglass This is a great framing for how to build a policy for protecting human participants in SCRF-affiliated research. This kind of policy would also protect us, and the researcher. It would make research much easier to start and stop because there will be clear, best practices that should be followed. Peer reviewers can use that policy as a reference to check if researchers are in violation.I think this also would have wide-reaching implications for SCRF, beyond this peer review project, and ultimately inform a clear set of expectations for SCRF-funded research. I’m curious to dig deeper into examples of institutions we could outsource protections to and examples of effective policies. Although outside the scope of this project, I could see this being a valuable area of inquiry for SCRF to document and propose an approach for future research – especially research relating to studies of ethnography, organizational design, or governance as it’s practiced within existing DAOs with real people. We should continue this conversation outside the peer review thread – I can reach out on discord!Hi @GUA thanks for your comment and question! GUA:So maybe could we have a tracking system on how many work one has reviewed and being reviewed, and maybe the reputation they build in the open peer review system is also some incentive for the work?Yes! Social incentives may even be stronger than financial incentives. One way we could build this incentive is a reputation tracker for reviewers which aggregates and display somewhere the ratings they receive from meta reviewers. They could then show this off, for example on a CV.Publons is an example of a system like this for existing journals, that verifies the number of reviews a researcher has completed. It’s received mixed results – many academic departments do not reward peer review even if verified. Within the open science community, Publons being bought by Clarivate was seen as shifting the intention of the site from realigning incentives to gathering data.It’s notable that where Publons rewarded and tracked the number of reviews, we would focus on number and quality. Hopefully, this would optimize for better scientific outcomes.']"
"                Research Summary - Convergence of Blockchain, IoT, and AI              ",https://www.smartcontractresearch.org/t/research-summary-convergence-of-blockchain-iot-and-ai/579,Scaling,151,['#link-3'],"['scalability', 'iot', 'summary']","['TLDRThe authors provide a non-technical overview of the benefits of blockchain, Artificial Intelligence (AI), and the Internet of Things (IoT), and discuss their complementary properties and interactions.They present the role of these technologies using a layered framework: IoT is used for data generation, the blockchain provides an infrastructural backbone via immutable smart contracts, and AI can be used to optimize rules and processes.The authors argue that as these systems intersect, the use of blockchains may prove beneficial. Smart contracts can provide an instrumental mechanism for IoT devices to interact with one another predictably.CitationSandner P, Gross J and Richter R (2020) Convergence of Blockchain, IoT, and AI. Front. Blockchain 3:522600. doi: 10.3389/fbloc.2020.522600LinkFrontiersConvergence of Blockchain, IoT, and AI 26Blockchain, IoT, and AI are key technologies driving the next wave of the digital transformation. We argue that these technologies will converge and will allow for new business models: Autonomous agents (i.e., sensors, cars, machines, trucks,...Core Research QuestionHow can the convergence of blockchain and AI make up for the drawbacks of traditional IoT?BackgroundAI, IoT, and blockchain are innovations with massive implications for business and industry.Blockchains: increase trust, security, and transparency.IoT: increases automation and user-friendliness of industries.AI: improves processes through pattern detection and outcome optimizationThe interconnection between these fields has been neglected as a field of study.In the future, they will converge.Connection: IoT provides data, blockchains provide infrastructure and rules of engagement, while AI optimizes and processes rules.Convergence may allow for optimized data management and business process automation.SummaryThere are two general storage options for blockchain-based data: on-chain storage and off-chain storage. On-chain data can be restored from full nodes at any time, but storage requirements are significant, which can lead to “blockchain bloating” (i.e. large quantities of on-chain-stored data that hinder throughput and scalability). Off-chain storage stores data off-chain and only keeps aggregated metadata on-chain; it is considerably more scalable than on-chain solutions but decreases data transparency.Blockchain may improve the infrastructure of IoT devices via smart contracts, especially when it comes to data immutability.The true potential of AI, IoT, and blockchain may only be accessed if and when combined.As these technologies intersect, when it comes to data management, the design goal is to improve standardization, enable privacy, and increase the security and scalability of systems where this technology is relevant.A considerable challenge for traditional IoT is the massive amount of data it collects.This data is traditionally stored on centralized servers, which face a lack of standardization, privacy, security, or scalability. Cross-platform data is hard to access and difficult to explain.Blockchain offers access for multiple parties and stores data in one format, increasing interoperability.Another challenge is that data submitted to the cloud is not encrypted and therefore does not ensure privacy.Privacy technologies being pursued in the context of blockchains may lead to better privacy if implemented correctly. If implemented properly in a combined system, this may increase the privacy assurances of IoT devices.Security: Cryptography and Consensus mechanismTrade-off: privacy vs. control of illicit activities.Security may be improved by AI performing data analysis (IoT is a source of big data).AI benefits from big data produced by IoT devices; the larger the quantity of data, the better the AI performance.IoT can’t store big data effectively, but blockchain combined with AI can.Certain blockchains lack scalability but may be increased with different consensus algorithms and with AI.Data Management: Authentication via a Blockchain-Based IdentityBlockchains can manage IoT device identity and authenticate IoT participants on the network.Identity: individuals, companies, devices, or machinesTransactions between two entities may be processed efficiently (high speed, low cost).2025 estimate: more than 20 billion IoT devices will be connected to the internet.Identity management will play an extremely important role.Blockchain allows protection of data, organization of ownership, and monetization of dataIdentities are secure as blockchain data is extremely difficult to forgeAutomatization via Smart ContractsThe convergence of blockchain, AI, and IoT is promising for automation.Smart contracts are the main connection between these three innovations.The main limitation of smart contracts is their crypto prerequisite that companies are often unwilling to leverage.Typical crypto value too volatileStablecoins: unregulatedIT and accounting systems are typically based in fiatSolution for smart contracts: fiat-denominated cryptocurrencies that flow through the smart contract, e.g. DLT-based digital EuroIoT devices could make micropaymentsEnterprise Resource Planning (ERP) systems could record every blockchain transactionCheap or no cost of currency conversionCompliant with regulatory requirementsIssued by banks, e-money institutes, unregulated institutions, or central banksThe distinction between central-bank-issued Euro and e-money-provider-issued Euro is important in times of crisis, hence why an official digital currency may be needed.Discussion & Key TakeawaysThe authors argue that convergence of IoT, AI, and Blockchain can and will happen. When it comes, it will begin a new age of digitization. Most critiques of blockchain’s limitations from ten years ago have already been remedied. Scalability limitations may be addressed with newer architectures. There are still challenges that have yet to be overcome.General Data Protection Regulation’s (GDPR) enumerated right for data to be forgottenIntegration with legacy systemsThe authors argue that these issues will sooner or later be overcome.ApplicabilityThe authors provide a case study with a network of IoT lamps. Each lamp has a blockchain-based identity. It operates using a digital Euro. Micropayments made to the lamp allow for it to turn on. AI may leverage the data about the lamps’ usage and automatically perform needed maintenance based on blockchain-provided metrics. It may also optimize downtime by optimizing the maintenance process. Lamps may be tokenized and availed to investors, allowing for building and autonomous maintenance for a network of lampsTokenization may be performed for all IoT devices (e.g. sensors, cars, or cameras) that are connected to the Internet and a blockchain network.Thanks to @Cindy for helping to create this summary.', '@Gearlad what was the purpose of the IoT lamp case study? Were they making a case for an autonomous business model – is that what happens at the convergence of IoT, AI, and blockchain? It’s an amazing field, one application being investigated in California is wildfire detection 5. What kind of work are you doing with IoT, AI and blockchain at NTU?', 'Neat summary @Gearlad. I’m completely sold on the idea of tokenizing the ownership of IoT devices. It’ll enable a host of niche IoT applications, like the one @jmcgirk mentioned, to gain access to funding. Using blockchains to protect data, manage device identities, and automate business processes (via smart contracts) also makes a lot of sense. However, I’m skeptical of the role AI plays outside of data analytics. What exactly do the authors mean when they claim AI can be combined with blockchain to improve scalability and data storage?Finally, I didn’t quite understand the point about companies shying away from smart contracts because they are fueled by cryptocurrencies. I get that official DLT-based digital currencies would be less volatile than typical cryptocurrencies and more compliant than stablecoins, but I don’t see how that affects the experience of interacting with a smart contract. As far as I know, governments aren’t building their CBDCs using blockchains [1], let alone the ERC-20 tokens required for Ethereum smart contracts [2]. That means government digital currencies would need to be tokenized (i.e. baked into stablecoins) in order to be used with smart contracts. One way or another companies will have to buy and sell crypto tokens. Am I missing something here? Anyone feel free to correct me If I’m wrong.[1] D. Shah, R. Arora, H. Du, S. Darbha, J. Miedema and C. Minwalla, “Technology Approach for a CBDC”, Bank of Canada, 2021. [Online]. Available: https://doi.org/10.34989/san-2020-6.[2] N. Reiff, “What Is ERC-20 and What Does It Mean for Ethereum?”, Investopedia, 2020. [Online]. Available: ERC-20 Definition', 'This research definitely covers new grounds and makes for a compelling investigation due to its novelty.  I personally find the authors’ lamp analogy ambiguous as to its real-world applicability and impact.  What are your thoughts?  I like the example that @jmcgirk gave here.With regards to the core research question, it’s notable that IoT devices and AI systems stand to gain through integrating blockchain infrastructure in many respects.  Centralized IoT systems have lots of inefficiencies.  With the quantity of IoT devices continuing to surge, data inflation can place restrictions on scalability and drive up maintenance overhead, just to name a few of the problems [2].  Blockchain systems do offer decentralization that can remedy nuances that arise in centralized IoT systems, but I side with the authors’ position - merging just these two technologies will not grant optimization.  Something more is needed.IoT-blockchain systems satisfy the need for transparency, accountability, secure transactions, lower costs (central auditors are essentially taken out of the picture), etc.  Throughput still remains an issue, though.  AI addresses this concern by automating processes and enabling self-sufficient models.  According to Sandner, Gross, and Richter, AI algorithms advance through data availed by IoT [1].  Further, “the more data is used to train the AI algorithm, the better the performance of the algorithm” [1].  Based on this, the more that IoT expands, the better AI is able to meet growing demands, increase throughput, maximize efficiency, and fuel economic growth.It seems that the three technologies will inevitably exist as symbiotic cohorts.  Still, why is there widespread hesitance to adopt new frameworks like these if the benefits ultimately outweigh drawbacks?  In your summary, you state that “smart contracts are the main connection between these three innovations.”  Can you elaborate on this?[1] P. Sandner, J. Gross, and R. Richter, “Convergence of Blockchain, IoT, and AI.” Frontiers in Blockchain, vol. 3, pp. 1-5, Sep. 2020, doi: 10.3389/fbloc.2020.522600. [Online]. Available: Frontiers | Convergence of Blockchain, IoT, and AI.[Accessed Jul. 22, 2021].[2] S. Verma. “How blockchain and IoT is making supply chain smarter.” IBM. Available: https://www.ibm.com/blogs/blockchain/2019/11/how-blockchain-and-iot-is-making-supply-chain-smarter/. [Accessed Jul. 22, 2021].', 'It’s great to point to convergence AI, IoT, Blockchain. Blockchain is like a bridge to connect IoT and AI. I also believe in the future is coming soon. I think all the technical solutions are provided already to companies. As the summary mention that Gearlad:The main limitation of smart contracts is their crypto prerequisite that companies are often unwilling to leverage.Typical crypto value too volatileStablecoins: unregulatedIT and accounting systems are typically based in fiatI read through one of the board of governors of the federal reserve system thinks about stablecoins from this paper Taming Wildcat Stablecoins 2. This paper point out three commnets:First, the use of private bank notes was a failure because they did not satisfy the no-questions-asked (NQA) principle.Second, the U.S. government took control of the monetary system under the National Bank Act and subsequent legislation in order to eliminate the private bank note system in favor of a uniform currency—namely, national bank notes.Third, runs on demand deposits only ended with deposit insurance in 1934.I think the most difficult part is how to convince the rule makers.', 'The author is convinced with the idea of the Convergence of Blockchain, IoT, and AI, and his lamp analogy is interesting and visionary. The definition of IoT is a system of interrelated computing devices or machines that are provided with unique identifiers (UIDs) equipped with the ability to transfer data over a network without requiring human-to-human or human-to-computer interaction. According to the Internet, there are 2.5 quintillion bytes of data being created every day, it is expected that the volume of data is going to double every two years. Additionally, we are entering the area of AIOT, therefore; the estimation of the data generated every other year will be one time more might be underestimated. Trained with such a huge amount of data, artificial intelligence will undoubtedly thrive, but it’s impossible to store the volume of data on this scale with traditional method; left alone each device should be equipped with the ability to communicate with each other and make the appropriate decision in time, which is the essence of AIOT. This is when blockchain comes into play, not only because of the scalability it provides, but also most importantly its immutability feature.Just imagine years after now, you’re sitting in an autonomous car, each car can acquire data via ways of means lidar, traffic data, communication with others vehicles, etc., and then make the best decision to arrive at the destination in the shortest time. The communications between agents become essential, however, it also becomes a vulnerability of the framework. If someone sends the target car some fake traffic data making the vehicle takes a detour, therefore they can hijack it. This scenario points out blockchain is an inevitable part of the development of AIOT.', 'The IoT lamp case was actually just an example to show that truly any arbitrary IoT device may leverage the proposed AIoT framework in a beneficial way.I personally feel that the authors could have provided more use cases. The one you describe in this article, PANTHER, is great - in which wildfires are detected by an artificial nervous system, of sorts, where an assortment of sensors - heat, humidity, wind, fuel, and cameras - are connected in a node that connects to the cloud, the data of which can subsequently be used by machine learning tools.Our team of Deane @Albert @fmendoz7 @TurtleHead @eleventh and @Cindy are now in the beginning stages of our AIoT Blockchain research project. I plan to work on the research summaries “Multi-Layer Aggregate Verification for IoT Blockchain” and “Efficient Attribute-Based Smart Contract Access Control Enhanced by Reputation Assessment” with Cindy. Albert is working on the summary for “SoK: Applying Blockchain Technology in Industrial Internet of Things”. Francis is tackling the summary “An analysis and evaluation of lightweight hash functions for blockchain-based IoT devices”.Teamwork makes the dream work!', 'The technical questions of crypto that have come about and that are bound to come in the future will always eventually have a solution. But answering this- Sean1992076:I think the most difficult part is how to convince the rule makers.-remains the key in the future of crypto!', 'What are some immediate business use cases for the convergence of IoT, blockchain and AI? @Albert @fmendoz7 @TurtleHead @eleventh and @Cindy – would this be a military technology primarily? I can imagine swarming drone detection lines or smart mines. What other things come to mind?', 'There are already some examples that startup companies are applying  AIoT and blockchain technology to the agriculture industry. With the IoT sensors, producers can gather data on a range of metrics and send back information for decision-making. Using the data, artificial intelligence (AI) can improve the growing and selling processes with the parameters put in their model, help farmers determine which crops to grow to maximize the profit or anticipate potential threats by cross-fitting with historical information. Then, the producer can use robots to get rid of the weak plants to create more space for the healthy plants and deploy drone spreading capsules with the eggs of natural enemies targeting certain types of pests before they cause damage. Meanwhile, the data will be uploaded to the blockchain, creating secure, transparent records, which is valuable for the agriculture industry to create smart contracts tracking the food from its origins to grocery stores. The data can also be uploaded to the blockchain via oracle to create an insurance smart contract. Farmers can purchase agricultural derivatives. If the condition goes south, the derivative will compensate for the losses.', 'Although the trend is just starting, it’s guaranteed that it will gain popularity in the recent future.', 'One good application scenario might be with surveillance systems that use cameras or motion detectors in an IoT network. All data is uploaded and stored (at least for a specified amount of time) to a database (off-chain storage). Suspicious activity would be detected by AI and flagged data could be uploaded as on-chain storage for both immutability and a longer-term (perpetual) storage. Essentially in this case we have AI to select which data goes onto the blockchain.', 'Smart contracts are what governs autonomous processes in an AIoT blockchain system. The authors describe a pay-per-use scenario in which, based on the terms of the smart contract, lamp(s) in the network turn on for a specified time after receiving a micro-payment. Moreover, the lamp’s system status data such as power or time of usage may be uploaded to the blockchain and AI could use the uploaded data to optimize business processes (periodic maintenance, minimization of downtime, et cetera).I personally think that a better example than the authors gave would be in the process of renting a car. In the same way, each car would then have its own wallet and micropayments would allow for it to be driven for a specified amount of time. Due to a constant stream of data uploaded to the blockchain, if the driver speeds or if for example one of the parts malfunctions, the timestamp of when this event occured could be used to prove misuse or misconduct.', 'That’s a great point on tokenization and shared ownership. More and more infrastructure in an AIoT blockchain network could be publically owned, rented out for daily use, and auto-monitored/regulated.In terms of AI increasing blockchain scalability, the authors cite a case from another paper titled “Performance optimization for blockchain-enabled industrial internet of things (IIoT) systems: a deep reinforcement learning approach”. In this paper, a DRL-based system allows for higher level of throughput by using dynamic selection of certain blockchain parameters (block producers, block size, consensus algorithm, and block interval).', ' Albert:Just imagine years after now, you’re sitting in an autonomous car, each car can acquire data via ways of means lidar, traffic data, communication with others vehicles, etc., and then make the best decision to arrive at the destination in the shortest time. The communications between agents become essential, however, it also becomes a vulnerability of the framework. If someone sends the target car some fake traffic data making the vehicle takes a detour, therefore they can hijack it. This scenario points out blockchain is an inevitable part of the development of AIOT.This is another great example. Decisions made by AIoT devices do need to be explainable, especially when there are such immense safety concerns as in the case of autonomous vehicles. We can have blockchain to prove specific data existed and to justify the AI’s course of action. This could then be accessed by law enforcement entities or car insurance companies, for example.', 'I really enjoyed this summary, I think this convergence is going to have huge implications on the future of industry. The fact that it will behoove a business to have every aspect be connected via IoT from the whole supply chain to the coffee maker in the break room and be logged, evaluated, and optimized.I like to think the more and more sensors added are only going to give further insights and novel discoveries of how to benefit from the system whether it be large or small. For example, on a global scale, the more data we have about the climate will crack open new models that will help to predict what might happen better than something we might have never been able to predict ourselves/without AIoT.Wave-induced Atmospheric Variability EnterpriseHome | Wave-induced Atmospheric Variability Enterprise 2Wave-induced Atmospheric Variability Enterprise Wave-induced Atmospheric Variability Enterprise | NASA DRIVE Science CenterNext generation space weather prediction WAVE Science CenterEst. reading time: 2 minutesThese researchers are looking into Wave-induced Atmospheric Variability and have only recently been able to get some interesting results due to the availability of more sensors/data.Systems using blockchain data will be able to make leaps and bounds as they can communicate effectively and efficiently interesting correlations due to the standardized datasets.It is only a matter of time before blockchain AIoT is going to take over as they are perfect for each other.', 'Hey @willcon, thanks for the comment. I just checked out the WAVE project. Climate prediction is just another great example of what AIoT is capable of. It just so happens that these three innovations - AI, IoT, and blockchain - are set to change everything. The converged fields of AIoT and IoT Blockchain have already become major fields of research. In the same way, the converged fields of AI Blockchain and AIoT Blockchain are likely to become major topics of future research.Incidentally on some news, just today Facebook released its first set of smart glasses (Ray-Ban Stories), another great example of AIoT technology. I’ve pre-ordered Dr. Kai-Fu Lee’s book “AI 2041: Ten Visions for Our Future” which holds some astounding predictions for the future. One interesting one is that AR and VR technologies will allow for virtual and physical meetings to become practically indistinguishable from each other.', 'This has been a great thread to read - a lot of research has focused on more incremental improvements and points regarding to mechanism or design, but real world use cases are not isolated, research that focuses on connecting the dots and utilizing all elements of emerging tech such as the articles cited in this thread will be the applications we’ll see.A couple points that may be of interest to this threadHelium, a telecom based project may be a good example of connecting IoT with blockchain. Traditionally IoT networks have been difficult to develop because of the startup and incentive costs to maintain a network, Helium has used blockchain economics and incentive structures to help with this (miners and validators process and host their local network for IoT device in exchange for token rewards)Other shifts to think about that all are developing in parallel - the 5G rollout will enable far higher throughput of information transfer / compute and lower latency and the transition onto the Cloud for most applications will complement AI / IoT use cases (and many would argue is the infrastructure needed to enable the next generation of industrial or city-wide AI/IoT)Although the use cases of blockchain are exciting, one small critique in the paper would be that the researchers don’t exact answer the question of why a blockchain (as opposed to just a normal database) would be the best solution to some of these problems. We’re seeing for example the development of private LTE networks for smart cities or for utilities to monitor their service deliver with IoT, with blockchain potentially being a slower and more expensive solution due to consensus needs and node hosting. One would argue with the need for security that instead of a public blockchain a permissioned blockchain would be utilized, but at that point the line blurs between what one may define a blockchain to beAn interesting paper regarding 5G and Blockchain is this one here 3 for anyone interested in a read:https://www.researchgate.net/publication/335518169_Blockchain_for_5G_Opportunities_and_Challenges 3', 'Great summary. The topic brought by this paper is interesting yet somewhat vague. Despite all the difficulties yet to be overcome, there are still a lot of research potentials with systems that already exist. Introducing (new) IoT devices combining existing Blockchain and AI systems could be a great commercial achievement; Pulishing a recommandation map for different IoT devices to different Blockchain protocols using different AI applications in between along with all the tradeoff discussed could be a great academic achievement. It’s good to know inspiring paper  like this one.', 'Very nice summary.Hence reading through the comments ebbs and flows, I can’t help but to ask whether “standardizing” incumbent IoT data and providing “authenticity” would be the appropriate approach to automate data collection and preprocessing for AI applications. Forcefully making end users or consumers abide by a specific blockchains Smart contract might as well de-incentivize adoption of blockchain technology on the IoT layer of data collection.In my opinion, blockchain can provide data authenticity and basic preprocessing via the Smart contract and that would be a huge advantage for streamlining the end to end of an AI applications development. Along side with block chain providing a more secure decentralized but also centralized data infrastructure, at the end of the day, providers of such blockchain infrastructure are still there to make money. Denying a means to obtain data and enforcing a more complex interfacing by the end user would be the life and death of said budding blockchain maintainers.Then there is the issue of what it truly means to be “on chain”. With such large volumes of inflow, inevitably, there needs to be a form of a single source of truth. To my knowledge, a purely on chain approach to that “single source of truth” is yet to be available. I know of several Graph based technologies that may facilitate  such “single source of truths” but even then, there are still drawbacks and compromises.I also do concur that there needs to be further improvement on the existing network layer architectures. Lightning network for bitcoin, cardano being quite dismissive on doing anything more than networking on chain, and IOTA tangle being an aspiring resolution to the “on chain” data scalability issue.I have this presentation for a past partnership proposal I tried to pitch to a blockchain company on the integration of Data science and Blockchain for aspiring Paas companies.docs.google.comData Analytics For Blockchain 4Datascience for Platforms Task was to give direction to a datasphere project so here it isPlease do tell me your thoughts about it.-Tony']"
                Discussion Post: Taming Wildcat Stablecoins              ,https://www.smartcontractresearch.org/t/discussion-post-taming-wildcat-stablecoins/708,Mechanism Design and Game Theory,64,[],"['defi', 'discussion', 'regulatory']","['CTA:   “In these threads, we attempt to further the discussion of a key problem in this category and evolve our understanding of the domain space where research work has not yet answered the specific problem or question being considered. These posts are living documents, and it is our hope that the community will continue to contribute to their structure and content.”CitationGorton, G. B., & Zhang, J. ""Taming Wildcat Stablecoins”. SSRN. 19 Jul 2021. Available online: Taming Wildcat Stablecoins 14.Desan, C. (2014). Making money: coin, currency, and the coming of capitalism. Oxford University Press, USA.Chen, J. (2021). Demand Deposit. Investopedia. Retrieved from Demand Deposit DefinitionKagan, J. (2021). Savings Account. Investopedia. Retrieved from What Is a Savings Account?Smith, T. (2021). Convenience Yield. Investopedia. Retrieved from  Convenience YieldFRS v. Dimension Financial Corp., 474 U.S. 361 (1986)Corporate finance institute (CFI). (n.d.). Bank Run- Learn About Liquidity & Causes of Bank Runs. CFI. Retrieved August 19, 2021, from Bank Run - Learn About Liquidity & Causes of Bank Runs 2Gensler, G. (2021). Remarks Before the Aspen Security Forum. U.S. Securities and exchange commission. Retrived from SEC.gov | Remarks Before the Aspen Security Forum 1LinkTaming Wildcat Stablecoins 14Why Central Bankers Invoke Free Banking 1The no-questions-asked (“NQA”) principle 3Demand Deposit DefinitionWhat Is a Savings Account?Convenience Yield474 U.S. 361Bank Run - Learn About Liquidity & Causes of Bank Runs 2SEC.gov | Remarks Before the Aspen Security Forum 1BackgroundStablecoins: “are a digital form of privately produced money where each coin is supposed to be backed one-for-one with ‘safe’ assets. ‘Depositors’ buy stablecoins and, for each dollar deposited with the issuer, they receive that number of stablecoins in exchange.” (Gorton & Zhang, 2021)Important properties of money: “the three most commonly stated ones being a store of value, a unit of account, and a medium of exchange.” (Mankiw, 1997; Desan, 2014)The no-questions-asked (“NQA”) principle 3: “requires the money be accepted at par in a transaction without due diligence on its value.” (Holmström, 2015)Convenience yield: “A convenience yield is the benefit or premium associated with holding an underlying product or physical good, rather than the associated derivative security or contract.” (Smith, 2021)Bank run 2: “A bank run occurs when many customers withdraw all their money simultaneously from their deposit accounts with a banking institution for fear that the institution is, or might become, insolvent.” (CFI, n.d.)Demand deposit: “A demand deposit account (DDA) is a bank account from which deposited funds can be withdrawn at any time, without advance notice.” (Chen, 2021)Saving deposit: “A savings account is an interest-bearing deposit account held at a bank or other financial institution.” (Kagan, 2021)Free Banking Era in the United States:“The closest analogy to stablecoins is found in the Free Banking Era, when entry into banking was relatively easy and banks could issue their own banknotes. Starting in 1837, some states changed the way that bank charters were granted. These states allowed free banking—that is, anyone could open a bank. However, there were rules. Banks had to back their note issuance one-for-one with state bonds that were deposited with the state treasurers (the banks received the coupons from these bonds). Each state specified the exact bonds that were eligible to back notes. But, the private bank notes, whether issued by chartered banks or free banks, did not trade at par away from the issuing bank. The market was an ‘efficient market’ in the sense of financial economics, but varying discounts made actual transactions (and legal contracting) very difficult. It was not economically efficient.” (Gorton & Zhang, 2021)National Banking Era in the United States: “The National Bank Act was passed in 1863, establishing national banks in the United States. These banks could issue national bank notes, but they had to be backed with U.S. Treasury bonds deposited with the U.S. Treasury. Subsequent legislation imposed a prohibitively high tax on bank notes other than national bank notes. In other words, the era of privately issued bank notes was over. For the first time in U.S. history, there was a uniform currency that satisfied the NQA principle.” (Gorton & Zhang, 2021)Section 2(c) of the Bank Holding Company Act of 1956: defines a bank as any institution “which (1) accepts deposits that the depositor has a legal right to withdraw on demand, and (2) engages in the business of making commercial loans.” (Gorton & Zhang, 2021)Legal Tender Act in 1862: ""authorized the creation of paper money not redeemable in specie (‘greenbacks’). "" (Gorton & Zhang, 2021)Section 21 of the Glass-Steagall Act: “prohibited entities [that were] not banks from taking deposits. Section 21(a) of the Glass-Steagall Act is still on the books today.” (Gorton & Zhang, 2021)Title VIII of the Dodd-Frank Act: “In Title I of the Dodd-Frank Act, Congress created the Financial Stability Oversight Council (‘FSOC’) to combat the risk of systemic nonbank financial companies…The FSOC could designate stablecoin issuance as a systemic payment activity under Title VIII of the Dodd-Frank Act. The statute states: The purpose of this subchapter is to mitigate systemic risk in the financial system and promote financial stability by— (1) authorizing the Board of Governors to promote uniform standards for the— (A) management of risks by systemically important financial market utilities; and (B) conduct of systemically important payment, clearing, and settlement activities by financial institutions; (2) providing the Board of Governors an enhanced role in the supervision of risk management standards for systemically important financial market utilities; (3) strengthening the liquidity of systemically important financial market utilities; and (4) providing the Board of Governors an enhanced role in the supervision of risk management standards for systemically important payment, clearing, and settlement activities by financial institutions.” (Gorton & Zhang, 2021)12 C.F.R. § 5.20(e) by Office of the Comptroller of the Currency (“OCC”): “In 2003, the OCC promulgated a rule that set forth its authority to grant a bank charter to any entity engaged in at least one of the three core banking functions: receiving deposits, paying checks, or lending money.” (Gorton & Zhang, 2021)The Federal Deposit Insurance Corporation (FDIC): “The Federal Deposit Insurance Corporation (FDIC) is an independent federal agency insuring deposits in U.S. banks and thrifts in the event of bank failures. The FDIC was created in 1933 to maintain public confidence and encourage stability in the financial system through the promotion of sound banking practices.” (Gorton & Zhang, 2021)Money market funds: “A money market fund is a mutual fund that invests solely in cash and cash equivalent securities, which are also called money market instruments. These vehicles are very liquid short-term investments with high credit quality.” (Gorton & Zhang, 2021)Systemic risk: “Systemic risk is the possibility that an event at the company level could trigger severe instability or collapse an entire industry or economy.” (Gorton & Zhang, 2021)Key Problem / Topic AreaAre stablecoins issuers violating laws that regulate banks or currencies issuing such as the National Bank Act of 1863?How do policy makers address the systemic risks created by stablecoins?What is the best design for central bank digital currencies (CBDCs)?Specific Question or Problem StatementAre Stablecoins “Money”?Are Stablecoins “Demand Deposits”?Are Stablecoin Issuers legally “Banks”?Could a stablecoin issuer become a bank in practice?Should the sovereign have a monopoly on money issuance?What is the best design for central bank digital currencies?Gorton & Zhang’s this article points out that there are two ways to design central bank digital currencies:(1) as a digital currency token and (2) in the form of a deposit account with the central bank. (2021) Which one is better? Or is there any choice better than these?Approach / MethodologyIn order to understand the legal framework governing stablecoins, Gorton & Zhang’s article makes a historical review of money market funds, which were deemed not to be deposit-taking institutions. (2021)In order to propose how stablecoin issuers could be regulated, Gorton & Zhang’s article goes back further in time to the 19th Century and describes the Free Banking Era, the consequences of porous regulation, and the eventual demise of the system via the National Bank Act. (2021)Conclusions / Key TakeawaysGorton & Zhang’s article argues that for a stablecoin to be “Money” in addition to being a store of value, a unit of account, and a medium of exchange, a stablecoin must also satisfy the requirements of the NQA principle. Currently, stablecoins can’t satisfy the NQA principle. Thus, stablecoins are not money. (Gorton & Zhang, 2021)Gorton & Zhang’s article argues that because holders of stablecoins are not the owners of the stablecoin issuer, many stablecoins could be considered deposits. Thus, stablecoin holders are essentially creditors of their own depositories. However, some stablecoin issuers like Tether could be treated like money market funds because their contractual relationship with stablecoin holders is equity, instead of debt. If the redemption process for stablecoins were unencumbered, then that would mean stablecoin issuers were accepting demand deposits. (Gorton & Zhang, 2021)Gorton & Zhang’s article argues that stablecoin issuers are not “Banks”. According to the Competitive Equality Banking Act of 1987, an institution is considered a bank for the purposes of the Bank Holding Company Act if it is either (1) an FDIC-insured institution or (2) an institution that accepts demand deposits and makes commercial loans. Stablecoin issuers are not FDIC-insured institutions. Given the narrow scope of the term “commercial loan” applied by the Supreme Court’s decision in FRS vs. Dimension Financial Corp. (474 U.S. 361), stablecoin issuers would not be considered to provide commercial loans. Thus, stablecoin issuers would not be considered banks under the Bank Holding Company Act. (Gorton & Zhang, 2021)Gorton & Zhang’s article argues that a stablecoin issuer cannot be a bank in practice. Reserve Banks can decide which institutions receive master accounts regardless of whether the institution has a charter from the OCC or from a state like Wyoming or Nebraska. Thus, in a practical sense, stablecoin issuers cannot become banks simply by receiving a charter from the OCC or from a state banking authority. (Gorton & Zhang, 2021)CTA: Future Work / RFPFollowing Nobel laureate Paul Krugman’s tweet “This, by Gorton and Zhang, is very good: stablecoins are just a modern version of free banking, in which private banks issued their own notes supposedly backed by specie. That system was crisis-ridden, and the same will be true no.”As the SEC Chair Gary Gensler said, “We already have an existing stablecoin market worth $113 billion 1.” (Gensler, 2021) Do stablecoins constitute a revival of the free banking era? Is that really a problem? If so, how can we manage it?Is there any way to encourage the global wild stablecoins to fit into the NQA principle legal frameworks?In order to facilitate the fine development of stablecoins, it is important for those who are expecting its future to think about these questions.Gorton & Zhang’s article points out that “there are three main takeaways from our historical experience. First, the use of private bank notes was a failure because they did not satisfy the NQA principle. Second, the U.S. government took control of the monetary system under the National Bank Act. Third, runs on demand deposits only ended with deposit insurance in 1934. In this context, the researchers suggest that the government has a couple of options: (1) transform stablecoins into the equivalent of public money by (a) requiring stablecoins to be issued through FDICinsured banks or (b) requiring stablecoins to be backed one-for-one with Treasuries or reserves at the central bank; or (2) introduce a central bank digital currency and tax private stablecoins out of existence.” (Gorton & Zhang, 2021)Gorton & Zhang’s article is in favor of the design that a central bank digital currency would be issued as a digital currency token. It allows the central bank to avoid the operational tasks of opening accounts and administering payments for users, as private sector intermediaries would continue to perform retail payment services. It can also avoid the consequence of designing central bank digital currencies in the form of a deposit account with the central bank that the Federal Reserve would be engaging in fiscal policy with all the political ramifications that would entail and jeopardize its independence. (Gorton & Zhang, 2021)', 'Why does the researcher suggest the government transform stablecoins into money? Is it necessary to do so to meet the NQA?It’s also curious to me why the government should be playing a role in enforcing the nature of stable coins at all. How does he or she justify the government’s role?', 'I think the researchers mean that if stablecoins can’t satisfy NQA, then it can’t be deemed as money. Being money may result in many concerning regulations. On the other hand, stablecoins are issued by private sectors, they are not backed by the national treasury, so traders who use particular stablecoin may need to take another step to check if that stablecoin is well back so that their interests could be safe. This additional step will produce a lot of costs in an entire society. Maybe these are why the researchers regard NQA matter in the discussion of stablecoin’s nature.The researchers suggest the government issue digital fiat currencies which money backed up by the national treasury, so-called CBDC, that are issued by central banks and are different from stablecoins which are issued by private sectors. I think some fiat currencies may not satisfy NQA because their countries are in huge debt or at war what will diminish their back, and people still need to check their value. So I think fiat currencies or CBDC might not definitely satisfy NQA.', 'George Selgin from the CATO Institute argues that the comparison with the US’s wildcat banking years is flawed — it had nothing to do with consumers. “If it had,” he said, “there’d be no need for a punitive 10 percent tax to force state banks to quit issuing their own notes.” (from Sebastian Sinclair at CoinDesk 1). Who would really be benefiting from forcing stablecoins to adhere to NQA standards?', 'It reminds me of a mystery story I have heard about Die Rothschild Familie. Though I’m not sure if it is true and if this topic has anything to do with them. On the other hand, I believe levying on issuing notes would be beneficial to the government. Think about the battle of currencies between countries can tell.', 'Yeah, I agree it is undeniable that the United States has benefited from a strong currency (especially now), I think the quotation above is aimed more at the idea that the federal government is benefiting at the expense of the people it serves… which is a constant struggle in American history (states rights vs. federal). It’ll be very interesting to see what happens with El Salvador and to see who comes out ahead.', 'Recently, SEC’s Chairman Gary Gensler mentioned the wildcat banking era of 1837-63 in the U.S. when he talked about cryptocurrencies (Kiernan, SEC’s Gensler doesn’t see cryptocurrencies lasting long 1, Wall Street Journal, 2021). It looks like that the regulation on cryptocurrencies will become harsh.', 'Coming to this subject as a naive reader, I have to ask why anyone would issue a private Stablecoin in the first place. Unless you’re trying to profit disreputably by facilitating money laundering or otherwise acting as a “shadow banking system,” what could be the source of the utility?What Gorton and Zhang say is: “Stablecoins are a new form of private money that can add value in cross-border transactions for firms and banks.” I.e., Stablecoins can serve as a common denominator for countries using different currencies.This seems a modest but valid role for Stablecoins. It does NOT require them to “be like money” or to satisfy the NQA principle across an entire society.At the other extreme, we have 1) Facebook—a company that has always shown the naked ambition to become or replace society—deciding to conduct its business within an FDIC-insured bank and thus make its Diem coin ultimately indistinguishable from the US dollar…Or 2) the issuer of another prominent Stablecoin, Tether, found to be less than honest about the backing of their virtual currency, with the apparent motive NOT of “adding honest value” to a transaction but playing digital three-card-Monty and moving “the value” around so quickly that no consumer can say for sure where it really is at any given moment.I agree that the interests of the Feds may at times be in conflict with the interests of the States, but if the sovereign can’t dictate the terms of the money supply, there’s no basis for being the sovereign.So my naive question is: Beyond the function of “adding value in cross-border transactions,” what is the utility of issuing and using Stablecoins in the first place?', 'The easiest answer to this question that is a “legitimate” usage would be to facilitate wholesale purchasing giving “club” members the power to purchase things wholesale while building the organization’s buying power without directly making purchases at any given time.  Effectively, creating a private stable coin amounts to a “treasury” that the community or organization can decide how they want to allocate the funds.  Some use the pool as a means to provide liquidity to the stablecoin, while others use the funds to facilitate businesses operating by covering the costs and then reimbursing the pool with the profits from the business.The problem with framing the argument against Tether or any other stablecoin is ultimately the Federal Reserve has succumbed to the same problems and thus it undermines the notion that Tether’s fraud potential is somehow any different than the fraud potential the currently exists in fractional-reserve banking.Ultimately, the main utility is pooling funds to give the capacity to direct the reserve.  That is a desirable outcome for any organization.  Stablecoins make that easier.  On the other hand a floating token then provides another avenue for “speculation”, whereas stablecoins are much harder to arbitrage themselves on speculative trading.', '@Larry_Bates Regarding “the notion that Tether’s fraud potential is somehow any different than the fraud potential that currently exists in fractional-reserve banking,” I would love to understand this better via some specific examples and/or suggestions for further reading.Again, I was referring to Tether’s initial claim that 75% of its tokens were backed by “cash and cash equivalents,” a claim later found to be fraudulent. Is it fair to say that the Fed has “succumbed to the same problems”?On your point that “the main utility [of Stablecoins] is pooling funds to give the capacity to direct the reserve,” my (admittedly) naive question still stands: “Why not just do that with dollars?” How exactly do “Stablecoins make that easier”?', 'So, I think it’s reversed:  Tether succumbed to the same problem the Federal Reserve has had over multiple decades.  Fractional reserve banking has shown to be a problem multiple times in the Federal Reserve’s distribution of funds to national banks.  In other words, Tether just followed the same practices of the Federal Reserve, not the other way around.“Why wouldn’t people just use US Dollars?”Considering Americans only make up a small percentage of the global population, establishing the US Dollar as the global reserve currency is clearly a means of maintaining US dollar dominance, effectively giving the US government (Federal Reserve) a lot of influence over global monetary policy.Getting away from a USD peg legitimately strips influence over the global economy from the US government.  That alone is a reason for an organization not interested in the US government’s goals to not hedge their capital in USD.Assume users of stablecoins are not necessarily American, and the motivations become much more clear.', '@Larry_Bates When I described myself as “admittedly naive,” I probably should have said “willfully naive” — i.e., for the purposes of getting down to first principles.I completely get your point about “legitimately [stripping] influence over the global economy from the US government.” That certainly sounds like a bigger-sounding issue than “pooling funds to serve the interests of a group.”If I were answering my own question about “Why not use US dollars?”, I’d probably home in on the issue of decentralization.Do you have any specific posts or summaries, etc. that focus on decentralization specifically, and that we could use to generate deeper discussion of this key issue?', 'While the language of “pooling funds to serve the interests of a group” sounds overly reductive and seemingly insignificant, what is a government beyond “pooling funds to serve the interests of a group”?Effectively, every government operates at that level with a varying degree of influence by a varying minority/majority of the group on how the funds are allocated.  I try to use language that applies at the macro/micro level so that it is more likely to be “correct” in alignment with reality than trying to use extremely scenario-specific language.I will point to the “Prize-Linked Savings Pools 2” post as to why it is a good idea to diversify the number of pools in which people are contributing to retain value as not to cause a central point of economic failure.  Effectively, diversifying the currencies in which a person is saving while pooling savings to create market liquidity mechanisms seem to be an element of stablecoins that make it easier for someone to create an interest-bearing mechanism beyond the poor yield associated with a simple USD savings account.In other words, the yield mechanisms associated with USD specifically are not as good of a prospect for higher return rates than using a stablecoin-based PLS account.  When you get into the post, you start to see the outsized returns on those types of accounts compared to traditional USD savings accounts.  There is nothing political about it.  It is provably more profitable to use a PLS stablecoin account than to use a traditional USD savings account.', '@Larry_Bates  Thanks for your reply and the explanation of your use of “macro/micro” language. Larry_Bates:While the language of “pooling funds to serve the interests of a group” sounds overly reductive and seemingly insignificant, what is a government beyond “pooling funds to serve the interests of a group”?Yes, I see your point more clearly now.BTW, in following your link to PLS accounts, I came upon a succinct thread on “Why Web 3 matters” 4 (recommended by the co-founder of PoolTogether) that quickly captured (for me, anyway) the importance of decentralization.It’s also succinctly answers my (deliberately naive) question: “Why not just do that [pool funds] with dollars?”', 'As the Financial Stability Board (FSB) (2021) observed in Regulation, Supervision and Oversight of “Global Stablecoin” Arrangements: Progress Report on the implementation of the FSB High-Level Recommendations, “At present, stablecoins are being used primarily as bridge between traditional fiat currencies and other crypto-assets, which in turn are primarily held and traded for speculative purposes” (p.1)Stablecoins still playing the role of haven in the cryptocurrencies world currently. As the largest capitalization stablecoin, it will be a tragedy if Tether breaks out confidence risk. Thus, the Zeke Faux’ investigation that @jmcgirk posted in the comment of On the Economic Design of Stablecoins aroused concerns again.While I don’t argue that we must tame these wildcats by over-tax to diminish them, moderate regulations are required. FSB’s recommendation and implementation of jurisdictions are notable.Recently, Bank for International Settlements (BIS) released Application of the Principles for Financial Market Infrastructures to Stablecoin Arrangements (2021). According to its considerations for determining the systemic importance of a Stablecoin Arrangement (SA), Tether may be considered as systemic important at least because of its size and being used in cross-border payments. If so, being a SA, Tether has a much longer way to go to meet the requirements BIS set, including providing clear and direct lines of responsibility and accountability, developing appropriate risk-management frameworks, aligning technical settlement and legal finality, providing the transparency and remedy policies of settlement gaps, and the most related to the discussion in On the Economic Design of Stablecoins, “no credit or liquidity risk”, which requires transparency of its reserve to the public as the first step. When we are discussing what’s the truth behind its reserve sources and it’s still buzzing, it’s quite far from the needed confidence degree of a systemic important financial infrastructure.', 'I thought I’d bring this thread to life given the recent depegging of TerraUSD and the plunge in the value of Luna (which has since recovered). Feel free to add your thoughts in here. You can watch the community discuss this issue in real time – Commonwealth', 'This seems to be the most active thread - Commonwealth also check out Do Kwan’s thread for some interesting and valuable context - https://twitter.com/stablekwon/status/1524331171189956609 @Astrid_CH it would be really interesting to get your input.Another interesting thread - the attack on the UST system: https://mobile.twitter.com/OnChainWizard/status/1524123935570382851', 'This piece on algorithmic stablecoins in Wired claims that the “clever architecture” of UST/Terra “did not and could not work.”And of course the whole debacle gives regulators an opportunity to rush in 1.', 'Vitalik Buterin sent a couple of tweets yesterday expressing strong support for “coordinated sympathy and relief for the average UST smallholder.” “The obvious precedent is FDIC insurance (up to $250k per person),” he said. “IMO things like this are good hybrid formulas.”Vitalik was responding to a tweet thread where the poster made an analogy to the Madoff ponzi restitution which also prioritized making the average investor whole, not the wealthiest.SCRF Community: Do you agree or disagree with this idea?', 'I think there is insurance available for some crypto deposits, so it’s certainly feasible. One of the things that makes the crypto ecosystem so exciting and dynamic, however, is the lack of regulation and I worry that restitution and other external authorities looking in would eventually hurt the hothouse environment that’s creating all of the innovation in crypto. Still, I wouldn’t mind a little bit of safety!']"
                Research Summary: Blockchain is Watching You: Profiling and Deanonymizing Ethereum Users              ,https://www.smartcontractresearch.org/t/research-summary-blockchain-is-watching-you-profiling-and-deanonymizing-ethereum-users/564,Privacy,32,['https://arxiv.org/pdf/2005.14051.pdf'],"['summary', 'network-security', 'privacy', 'ethereum', 'iot', 'zero-knowledge']","['TLDR:From a privacy perspective, Ethereum’s account-based model is less secure than Bitcoin’s unspent transaction output model (UTXO). Several privacy enhancing overlays have been deployed, such as noncustodial, trustless coin mixers and confidential transactions.The authors claim to be the first to propose and implement Ethereum user profiling techniques based on quasi-identifiers to test these overlays.Three different deanonymizing techniques were tested:  Quasi-identifiers and node-embedded ranking algorithms were applied on regular transactions. Trustless mixers such as Tornado Cash were also applied. Finally, the Danaan-gift attack was used to fingerprint addresses, relinking shielded transactions.The anonymity set of a certain address can be effectively reduced by a factor of 2^1.6 ~= 3.0314 bits on average.Core Research QuestionFor an account-based paradigm such as Ethereum, can we come out of attacks that reduce the anonymity set to a certain extent and deanonymize accounts in the future?What are the limitations of such attacks?How can users, devs and core devs keep the transactions private, to further increase the anonymity and privacy of the Ethereum network?CitationsBéres, Ferenc, et al. “Blockchain is Watching You: Profiling and Deanonymizing Ethereum Users.” arXiv preprint arXiv:2005.14051 (2020). 14Twitter threads by the author of the paper. 7Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs (CIKM 2020).Code repository used in the paper. 1BackgroundMixers and confidential transactions are well-used on the Bitcoin blockchain, which is based on a UTXO paradigm. Ethereum differs from Bitcoin in that address reuse is enforced on a protocol level. This paper is the first (to the authors’ knowledge) to propose and implement Ethereum user profiling techniques based on quasi-identifiers. They use graph-representation learning to analyze privacy, specifically anonymization on Ethereum.SummaryAccount-based Ethereum is inferior to UTXO-based Bitcoin from the perspective of privacy, as one has to reuse an account in order to spend any remaining funds. This enables quasi-identifiers for profiling, identifying and linking the person behind accounts.The paper gave a few novel quasi-identifiers: temporal activity, transaction fee (gas oracle source), and location in the transaction graph.The authors correlated Ethereum Name Service (ENS) addresses to a certain user via Diff2Vec and Role2Vec. They crawled ENS addresses and corresponding Twitter handle/address pairs via a public Twitter API, as an “answer” or “ground truth”, to evaluate the performance of the method.They attacked mixing solutions such as Tornado Cash, and pointed out common misuses, which make it possible to deanonymize and relink deposited accounts to withdrawn accounts.They also describe a Danaan-gift attack, where sending a small but identifiable balance to a target (0.000123456789, for example) could fingerprint the account.The paper collected and open-sourced their dataset for further research purposes.They introduce two existing approaches (albeit primitive): analyzing smart contracts, and analyzing addresses.Smart Contracts were examined for unsupervised clustering techniques, and contract code reuse.Addresses were crude clustered into four different groups; analyzed based on airdrops/ICOs; stylometry was used to deanonymize contract developers.The authors offer a brief introduction to a non-custodial mixer called Tornado Cash, Ethereum Name Service (ENS), and node embedding algorithms in Machine Learning.Methodology of dataset collection: here the authors explain the relationship between ENS, addresses, and humans were collected from Twitter, Humanity DAO, addresses that interacted with Tornado Cash’s mixer contract, and Etherscan.At this perfunctory stage, the authors discovered that many public addresses could already be linked to real people, and those addresses exposed sensitive activities, such as gambling and adult services.The paper focused on reducing the anonymity set, rather than finding an exact identification. The paper aims to be the cornerstone for “future privacy” protection.Treating evaluation as a classification task in ML, AUC (area under curve) as the average rank of results from selected algorithms (harmonic average of Diff2Vec and Role2Vec).Quantity/efficiency is expressed as entropy gain, where the paper inferred an empirical probability distribution from the former result, to judge the deanonymization power of the algorithms.Accounts were linked via three quasi-identifiers: temporal activity, transaction fee (gas oracle source), and location in the transaction graph.Temporal activity of a certain account is represented as μ, x̃, and std in vectors.Gas price was chosen manually by the user, or automatically by the wallet client -  therefore a gas profile (slow, normal, fast) could be constructed for each Ethereum user. Also represented as μ, x̃, and std in vectors.The pre-processing of raw accounts and transaction data were described, in order to apply node embedded algorithms onto it.Euclidean feature vectors are calculated with the above-mentioned quasi-identifiers. Different granularity of data range and different algorithms are applied.The result shows that the harmonic average of rank of Diff2Vec and Role2Vec most promising, and yielded 1.6 extra bits of additional information on the account owner (2^1.6 of anonymity set reduced).They explain k-anonymity and the concept of anonymity set for native mixing services on Ethereum, using Tornado Cash as an example.Utilizing gas quasi-identifier and reused deposit/withdrawal account address pairs to deanonymize them, the paper relinked 218, 110, 60, and 7 addresses for 0.1 ETH, 1 ETH, 10 ETH, 100 ETH contracts.The paper found that users who reused their addresses were more likely to consecutively deposit and withdraw within <24 hours. Thus, a temporal quasi-identifier can further be used to deanonymize addresses if the majority of users are assumed to not leave their funds in the mixer contract for more than 3 days. Also, the account used to withdraw was not fresh i.e., without previous transactions, for most users.They describe performance evaluation methods. Performance comparison between quasi-identifiers and Diff2Vec. The best result with regard to entropy gain is the concatenated results of both Diff2Vec and quasi-identifiers.They suggest correct usage for mixers to maintain intended privacy properties. Three best practices:Leave funds in a contract for more than a week, even better if this includes a randomized deposit and withdraw interval;Use a relayer to withdraw to fresh addresses;Use a mixer after every on-chain transaction in order to prevent user behavior profiling.The authors introduced the Danaan-gift attack on Ethereum, where sending crafted, unusual amounts to victims can be used for fingerprinting addresses.Taking Aztec’s shielded transaction as an attack example.The authors discuss future directions of deanonymization techniques, such as combining on-chain data and off-chain data for analysis.More quasi-identifiers are waiting to be found, such as wallet fingerprints (different ways to calculate gas fees between clients).Network layer privacy should be considered on Ethereum. The use of relayer could also be potentially harmful to privacy.Metamask and mobile clients should be further researched in order to prevent fingerprinting.Quasi-identifiers on Ethereum could also be a possible attack vector on UTXO-based blockchains. UTXOs could be mass-clustered, and are vulnerable to deanonymization attacks.Part I. Deanonymization Methods, Quasi-IdentifiersData PreparationThe dataset included linkable relationships between ENS, addresses, and humans. This public data was used as ground truth to evaluate the performance of deanonymization techniques. The dataset was collected from Twitter, Humanity DAO, and addresses that interact with Tornado Cash’s mixer contract. The corresponding raw transaction data was gathered via Etherscan.Raw transaction data of addresses was pre-processed into Euclidean feature vectors (and represented as a transaction graph) via Karate Club, an API-oriented open-source Python framework for unsupervised learning on graphs.AssumptionThe paper devised three quasi-identifiers, such as temporal activity, transaction fee (gas oracle source), and locations in the transaction graph. The performance of the three would be compared in the later paragraph of the paper.Algorithmic AdjustmentThe average rank and AUC (area under curve) returned by the node embedding algorithms cannot be used directly for evaluation on deanonymization, since it did not produce the priori and posteriori probability distribution directly for us to infer entropy gain := privacy loss := reduction of the anonymity set, after applying different deanonymization techniques.In order to make the results of different quasi-identifiers “comparable”, please refer to Section V. in the paper for the proof and the method authors used to generate the empirical probability distribution of entropy gain from the returned rank of candidates for the node embedding algorithms.The size reduction of the anonymity set is therefore represented as entropy gain, a metric we are concerned with for deanonymization techniques.EvaluationQuasi-identifiers:Temporal activity of a certain account is represented as μ, x̃, and std in vectors.Gas price is chosen manually by the user, or automatically by the wallet client -  therefore a gas profile (slow, normal, fast) can be constructed for each Ethereum user. Also represented as μ, x̃, and std in vectors.Diff2Vec and Role2Vec, the two node embedding algorithms were applied on the transaction graph to rank the similarities between ground truth and the raw transaction data.ResultsThe average rank for temporal activity:The average rank for gas price:The harmonic rank average of Diff2Vec and Role2Vec combined, for the transaction graph:One can see that Diff2Vec and Role2Vec are superior to the other two quasi-identifiers.When it comes to entropy gain:圖片321×682 86.8 KBThe result is very clear, where Diff2Vec and Role2Vec combined results excel on every aspect.Part II. Deanonymization Methods, Mixers such as Tornado Cash.Data PreparationThree heuristics were proposed:If there is an address from which a deposit and also a withdrawal have been made, then we should consider these deposits and withdrawals linked.If there is a deposit-withdraw pair with unique and manually set gas prices, then we should consider them linked.Let d be a deposit and w be a withdrawal address in a Tornado Cash (TC) mixer. If there is a transaction between d and w (or vice versa), we should consider the addresses linked.These linked addresses are used as ground truth pairs, for performance evaluation.It is worth mentioning that such simple heuristics already deanonymized a significant portion of accounts, as shown in the following table:圖片739×163 62.8 KBEvaluationThe same methods in Part 1. are also applied here.Ground truth addresses were separated into three sets, one when the deposit was within the past day of the withdrawal, another when within the past week, and the unfiltered full set, in order to understand the impact on anonymity for consecutive deposit and withdrawals.ResultsDifferent deanonymization methods, ordered by rank:圖片747×346 57.4 KBThe paper concatenated the feature vectors of Diff2Vec and daily activity (temporal activity) to yield the best result. A detailed description was not easily available.In terms of entropy gain:圖片761×257 41.9 KBIf we were to know the rank trend versus time i.e., after deposit, how long should we wait before withdrawal:圖片756×261 77.7 KBWhere one should at least wait for 7 to 15 days for the deposit transactions to increase, increasing the anonymity set.Part III. Deanonymization Methods, Shielded Transactions such as Aztec.A common phenomenon in Ethereum is observed. Since 1 ETH = 10^18 wei, but most wallet clients’ gas units are in gwei = 10^9 wei. That is, 98.1% of the wallet clients do not modify the last 9 digits of the account balance, at all.Danaan-gift attack, where an adversary sends a unique small amount to the victim’s address to fingerprint the account by exploiting the above-mentioned phenomenon, can be conducted on layer 2 shielded transactions such as Aztec protocol, to relink confidential transactions.Such a fingerprint could survive or disappear after more transactions have been conducted by the victim’s account, with the following probability:圖片743×177 64 KBBe advised that such an attack is a proof-of-concept at the moment, the paper did not actually conduct one (yet).Discussion and Key TakeawaysThe authors of the paper were stunned that so few empirical studies had been done on account-based privacy issues. Three quasi-identifiers and one evaluation method (Diff2Vec and Role2Vec combined) were provided in the paper, where on average 2^1.6 entropy could be gained after applying the aforementioned deanonymization techniques. Future privacy can be improved on the results of this work.A few suggestions were provided to users on different applications and occasions:Do not use sensitive applications with your public, already used address.Do not reuse addresses if possible and feasible.Manually decide your gas amount/use a different gas oracle when doing transactions.When using mixing services, wait at least a week before withdrawal, and do not reuse addresses. Use a relayer to withdraw with a fresh address.For the ecosystem as a whole, the paper suggested:Wallet clients should implement a certain degree of gas randomization and use a different gas oracle feed.Mixers such as Tornado Cash should improve the UX design for users to better use the mixing service as intended, in order to maintain a healthy size of the anonymity set.A network layer security (libp2p) on Ethereum should be further considered, in order to better achieve/protect future privacy/forward privacy.Implications and Follow-upsA research paper called PERIMETER 5 was published, describing a passive network-layer attack that would de-anonymize transactions on Bitcoin and Ethereum with 90% accuracy by intercepting 50% of connections. But there has been little other attention focused on network-layer privacy on the Ethereum blockchain. The Ethereum core development team is focused on improving the throughput of ETH 2.0 and this doesn’t seem to be a high priority ticket yet: https://github.com/ethereum/eth2.0-specs/issues/2285 2.', '@Jerry_Ho Could this kind of analysis also be used against account-based L1s like Solana and DOT?', 'In order to apply the strategy, more things have to be considered:First, is solana/polkadot famous enough that (no offense), we can gather enough public data (person-accounts pairs) as ground truth pair for evalution of the algorithm?That is, we shall check closely that how is solana/polkadot fundamentally different for their network layer behavior, such as the validator design, different transaction format, p2p nodes behavior, to name a few.One example:In solana, transaction fee is, in a sense, deterministic. And the way to obtain it, is vastly different from the behavior on Ethereum.Therefore, gas oracle as a quasi-identifier no longer work on solana, or it has to be heavily tweaked.Ref:docs.solana.comTransaction Fees | Solana Docs 1Subject to change.docs.solana.comDeterministic Transaction Fees | Solana DocsCalculating feesdocs.solana.comState-validation Transaction Fees | Solana DocsSubject to change.Personally, I don’t know much about solana and polkadot; but one should always be cautious when applying exciting new techniques - you gotta know what you’re doing, or it could be garbage-in-garbage-out.P.S. please correct me if my understanding to solana gas fee is flawed; it’d be very appreciated!', 'Has anyone actually used a Danaan-gift attack yet? How it work in the wild, I wonder', 'It was originally aimed to attack zcash. After a brief search, I cannot find any attack on other blockchain.The discussions, tools and papers by Alex Biryukov and his team in University of Luxembourg can be found here, in chronological order:github.com/ZcashFoundation/GrantProposals-2017Q4Empirical analysis of the Zcash blockchain 1        opened Sep 15, 2017          feddan35                  coding                  research                  security                  complete-submission                  awarded        CryptoLUX Research Group, University of Luxembourg application for Zcash foundat…ion grants.Our proposal is the empirical analysis of the Zcash blockchain. Despite Zcash positioning itself as a privacy-preserving blockchain, still about 80\\% of all the transactions are transparent, which makes them suitable for analysis. Until the Sapling update is released, and private transactions are made mandatory, there can be unintended vulnerabilities concerning the privacy of the users, and those problems may surface with an empirical analysis. We propose the following:- Investigate existing approaches for Bitcoin analysis, and apply and extend them to the Zcash blockchain- Use existing or our own tools. Extend these tools with Zcash specific functions, and release it to the public- Investigate JoinSplit transactions between z- and t-addresses, whether there are cases of deanonymization for the z-address holders- By empirical analysis provide a more accurate prediction of the transaction fees for wallet implementations, as currently users are usually either overpaying, or waiting too long for their transactions to be accepted, while with a more accurate estimate these problems could be mitigated- Analyze big clusters of addresses, like marketplaces, exchanges, especially in cases of transactions between z- and t-addresses, as Zcash users are generally conscious about their privacy, and personal data disclosure based on blockchain analysis would harm the reputation of Zcash- Investigate the privacy implications of SPV wallets in Zcash, as Bloom filter based light wallets in Bitcoin can lead to privacy related attacks (On the Privacy Provisions of Bloom Filters in Lightweight Bitcoin Clients, ACSAC 2014)- Analyze the network properties of the blockchain as well, as our team has expertise in this topic. (Deanonymisation of clients in Bitcoin P2P network, ACM CCS 2014)- The work would lead to a working paper of our findings- The work will be done in ethical way, no individual data would be deanonymized, results of experiments on real data would not be stored. For demonstration purposes  we will deanonymize our own transactions. We have prior experience with our studies of  privacy in Tor and Bitcoin on how to carry such work in an ethical way.The team that would work on the project would consist of PI Alex Biryukov, and two of his PhD students, Daniel Feher and Sergei Thikomirov. Our budget would be 24000\\$, which would cover around 9 person months of work.GitHubGitHub - cryptolu/BlockSci: A high-performance tool for Zcash blockchain...A high-performance tool for Zcash blockchain science and exploration - GitHub - cryptolu/BlockSci: A high-performance tool for Zcash blockchain science and explorationgithub.com/ZcashFoundation/GrantProposals-2018Q2Advanced Zcash Blockchain Analysis 1        opened May 18, 2018          closed Jul 30, 2019          feddan35                  security                  invited-full                  full-submission                  grant-winner        CryptoLUX Research Group, University of Luxembourg application for Zcash foundat…ion grants.Our proposal is to continue the empirical analysis of the Zcash blockchain. As we have described in our previous results ([link](https://github.com/ZcashFoundation/GrantProposals-2017Q4/issues/24)), the t-to-z and z-to-t transactions in Zcash are linkable in a quite large proportion, mostly caused by the requirement for block rewards to be first converted to a z-address. This alone leads to at least 80\\% of shielded transaction linkability. Considering the previous results, we still have areas, that seem valuable to investigate in detail. These are the following:- Investigate optimal transaction fee prediction.- Extend previous analysis and heuristics for other Zcash based currencies.- Investigate payment structures both in public and hidden transactions as well.- Further miner analysis, fine tuning the miner deanonymization, remove any false positives, try to find the marginal miners/pools as well as improve coverage of the history of Zcash (currently 92% in earlier months).- Study further the interaction of exchanges with the blockchain.- Investigate the possible false positive rates for the existing heuristics, analyze the remaining 2,000 hard unlinked transactions per 10,000 blocks.- Analyze recommended best practices (schielded ecosystem) and make new recommendations.- We are also open to ideas and proposals from the community on what else would be of interest to explore.The team would consist of PI Alex Biryukov and PhD Student Daniel Feher. The work would take 4-6 months.The paper directly cited by Blockchain is Watching You:cryptolux.orgZcash.pdf829.43 KBAnd the refined results:Biryukov, Alex, and Daniel Feher. “Privacy and linkability of mining in zcash.” 2019 IEEE Conference on Communications and Network Security (CNS) . IEEE, 2019. 1Biryukov, Alex, Daniel Feher, and Giuseppe Vitto. “Privacy aspects and subliminal channels in Zcash.” Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security . 2019. 2', 'I would like to add some notes about the Danaan-Gift attack for those who aren’t familiar with Aztec/ZCash like me. To use those 2 protocols, users will turn (shield) a public balance into a private balance and split it into many packs which people cannot see the balance and where it goes.([2]) Then, Danaan-Gift attackers assume users will turn (deshield) the private balance back to the public balance to use it in other ways, this is a reasonable assumption. For many people, the most convenient way is deshielding the whole account, therefore attackers can send a transaction with malicious value to fingerprint an account and discover the deshielding behavior.  [3] mentioned the depreciation of the t-address pool (I think they mean transparent value pool) will solve the problem, but may not work for Aztec as Aztec uses ETH thus needs for deshielding is too hard to eliminate.As I understand, the anonymity set is highly related to the number of users of a chain/mixer. [1] mentioned the ETH 2.0 may help to increase anonymity, so I’m curious about the relationship between it and network capability/famousness (as mentioned above), application (mixers) activity. How will the anonymity set increase with the growth of the other parts (e.g. TPS) of Ethereum (and Solana, Polkadot), and (how) will it affect the reduction?Y. Zhou, J. Wu and S. Zhang, “Anonymity Analysis of Bitcoin, Zcash and Ethereum,” 2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE), 2021, pp. 45-48, doi: 10.1109/ICBAIE52039.2021.9389894.Williamson, Z. J. (2018). The Aztec protocol. URL: https://github. com/AztecProtocol/AZTEC.Biryukov, Alex, Daniel Feher, and Giuseppe Vitto. “Privacy aspects and subliminal channels in Zcash.” Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security . 2019.', 'https://eprint.iacr.org/2020/627.pdf 5 was an incredible read that speaks to the challenges already discussed.Transaction likability attacks such as ITM or Danaan-Gift are seem to be predominantly enabled by bad balance (UTXO) management on the wallet side. Through better input selection management, one could break down a shielding deposit into many transactions and potentially trick those observing the transaction graph.For example, one could spoof a “change” UTXO before depositing into a shielded pool. Doing so would make it difficult to trace the links of each individual deposit as it breaks the common-input heuristic used to trace transactions. You could then shield transactions from transparent address to z-address. Another transaction from z-address to another z-address would shield the entire transaction.On your way out of the shielded pool, you could do the same instead of a single transaction, you could make several deposits from shielded to unshielded. While it is true that someone could still trace all permutations of value, the transaction graph would be very large and make it impractical.It would be great if wallets were to automatically implement these mechanisms to reduce the possibility of linkage in systems of shielded and unshielded balances. It would make transacting more expensive, but it would likely substantially decrease the potential for timing/sizing/fee tracing attacks.', ""Recently I’ve discovered some interesting changes in IoTA - they chose to move from account-based to UTXO-based in IoTA 1.5, Chrysalis.Although this is more of a scaling discussion rather than a privacy discussion, I think it could still be somehow related to this thread, as it brings ideas about account-based vs UTXO based paradigm.The design rationale can be found here:IoTA cafe, 2019IoTA 1.5 update noteIt is said that due to its peculiar/interesting take of DAG instead of blockchainning, the existence of coordinator were too centralized and was harmful to the longetivity of the network.If we were to follow the explanation and the context of the above articles, it seems more than neutral for IoTA to switch from account-based to UTXO-based, as theydon’t have smart contract yetit really makes the resolve of transactions fasterOther references before the question:Accounts and not UTXOs, Ethereum design rationaleIoTA RFC: Transaction PayloadHere comes the questions, where I couldn’t quite figure them out alone.Is it going to harm IoTA’s potential on implementing turing complete smart contracts in the future version of IoTA?If the answer to 1. is no, then what’s ethereum 2.0 (specifically sharding)'s take on this issue i.e., why won’t ethereum encounter similar problem as IoTA did?"", '@Jerry_Ho (or @flyinglimao or @cipherix) - a couple of follow-up questions: is there a distinct deanonymization lib/framework that’s actually in-use, and not just on paper/on theory? Also, is the same degree of anonymity is achievable on chains other than the ones(BTC/ETH) discussed in the paper? Would be more or less anonymous?', '@Jerry_Ho, wonderful work…just thinking about the implications of the security issue in ETH under General Data Protection Regulation.', '@Jerry_Ho , out of curiosity to find out more about anonymity detection on the Ethereum Blockchain network, I came across a tool called Tutela, which is an Ethereum and Tornado Cash anonymity detection tool, built as a response to a Tornado Cash C . This allows Ethereum and Tornado Cash users to see when they potentially revealed themselves on-chain and it does this by the following ways:1.By connecting their individual Ethereum addresses,their Tornado Cash deposits and withdrawals being connected.Was wondering if you know something about it and how effective you think it is in the detection of anonymity on the Ethereum Blockchain, especially on how effective it will be in profiling and detecting fraud activities and terrorists who move funds using the Blockchain.']"
                Research Summary: Dissimilar Redundancy in DeFi              ,https://www.smartcontractresearch.org/t/research-summary-dissimilar-redundancy-in-defi/1579,Auditing and Security,53,['https://arxiv.org/abs/2201.12563'],"['summary', 'network-security', 'testing', 'scalability', 'defi', 'flash-loans']","['(Joint work with @danhper)TL;DRThe meteoric rise of decentralized finance (DeFi) has been accompanied by a plethora of frequent and often financially devastating attacks on its protocols. There have been over 70 exploits of DeFi protocols, with the total of lost funds amounting to approximately $1.5 billion USD.In this paper, we introduce the notion of “dissimilar redundancy” for smart contracts. The core idea is that a program logic is implemented more than once, ideally using different programming languages. For each implementation, the results of the execution should match before the state of the blockchain is allowed to change.This research is inspired by and has clear parallels to the field of avionics, where on account of the safety-critical environment, flight control systems typically feature multiple redundant implementations. We argue that the high financial stakes in DeFi protocols merit a conceptually similar approach, and we provide a novel algorithm for implementing dissimilar redundancy for smart contracts.CitationPerez, Daniel, and Lewis Gudgeon. “Dissimilar Redundancy in DeFi.” arXiv preprint arXiv:2201.12563 (2022). [2201.12563] Dissimilar Redundancy in DeFi 14Core Research QuestionCan smart-contract risk (i.e., the risk of an exploit or bug) be meaningfully reduced through the use of multiple redundant implementations of the same program?BackgroundSmart contracts: Smart contracts are program objects that live on blockchains. The crucial property of smart contracts that we rely on in this work is atomicity - namely that a transaction either succeeds or fails in its entirety - as this ensures that the blockchain cannot be left in an invalid or inconsistent stateScaling solutions: Given the high cost of undertaking transactions on Ethereum, many scaling solutions have emerged that try to solve this issue.Smart contract vulnerabilities: There have been many different forms of smart contract exploits over the years, often leading to significant losses of money. In this paper, we focus purely on the technical security of smart contracts, and specifically logical bugs, which alone have resulted in millions of dollars of losses to DeFi protocols.SummaryDeFi protocol hacks are frequent - with more than 70 to date totalling losses of more than $1.5 billion. For a financial infrastructure that is purportedly going to replace traditional finance, this is worrisome.The severity of the issue of hacks is exacerbated by the non-custodial nature of DeFi systems. Unlike traditional financial systems, where in the event of financial disaster there are often safety-nets such as the state or insurers, in DeFi there are no such safety provisions at scale.Moreover, while there is a nascent insurance market for DeFi insurance, such solutions provide only a (necessary) second-best solution: providing coverage in the event of a DeFi system failure. A first-best solution is to prevent the failure in the first place.In aviation, the safety-critical nature has led to the emergence of a practice of implementing multiple separate and redundant flight control systems.For example, the Boeing 777 features a Fly-By-Wire flight control system that had to meet extremely high levels of functional integrity and reliability.To do this, it has three separate primary flight computers, with each computer containing three dissimilar internal computational lanes.The lanes differ in terms of compilers, power supply units and microprocessors, with, for example, lane 1 using the AMD 29050, lane 2 the Motorola 68040 and lane 3 the Intel 80486. Within each of the three flight computers, two of the lanes acted as monitors while the third lane was in command.In this way, the flight computer features a form of redundancy that is dissimilar, with the multiple lanes being resistant to bugs induced by microprocessors or compilers.We apply the core of this idea to smart contracts. We implement and detail a system based on a proxy pattern which relies on dissimilar implementations of a DeFi protocol, and cross-checks one against the other before effecting any on-chain state change.On Ethereum, this approach has already been taken for client implementations, with the community maintaining multiple open-source clients, developed by different teams and using different programming languages.The purpose of this approach is to strengthen the network and make it more diverse, with a view to avoiding a single client dominating the network in order to remove single points of failure. We extend this concept to the smart contract layer itself.MethodThis paper presents a protocol that enables the approach of dissimilar redundancy to be used in the context of Ethereum, and the Ethereum Virtual Machine.On Ethereum today a standard programming pattern is the proxy architecture pattern. With a proxy pattern, all message calls to a contract first go through a proxy contract. With this pattern, the former contract contains the actual implementation logic while the latter contract provides a storage layer. At present, a common use of this pattern is to provide contract upgradeability. Although contracts cannot be directly upgraded once deployed, using this pattern allows upgradeability to be mimicked. Simply, the proxy contract can be instructed to use a different logic contract, while retaining the same storage.In our work, we expand on this pattern by allowing the proxy contract to delegate to multiple underlying contracts at once. This is in contrast to the standard pattern which only permits delegation to a single implementation at a time. Our core idea is to enable a proxy contract to sequentially call two different implementations - supposedly identical in logic and ensure that the data returned by function calls to each implementation as well as return values from an arbitrary number of checks provided by the contract developer match.Although the overall idea is straightforward, the actual implementation requires several technical difficulties to be overcome.The figure below outlines the proxy pattern we develop.726×932 14 KBResultsWe evaluated our approach with reference to a smart contract that implements a simple auction system. We implemented the auction contract with identical behaviour in both Solidity and Vyper, but purposefully introduced two implementation bugs in the Vyper implementation of the contract logic.We tested our approach both locally and in a real-world deployment.In a local environment, we use Python in combination with Brownie and the Hypothesis testing library to generate test cases. The details of the testing are in the paper, but with this approach, locally we find that with only a few lines of code it was possible to have extensive coverage of a tested function that was able to automatically find discrepancies between two implementations.In a real world environment, we deployed to Polygon. An important strength of our approach is that it is possible to use two implementations not only during development and during testing, but also after the contract is deployed, ensuring that all the transactions executed will be consistent between the different implementations provided. Now considering gas costs, we provide the estimated USD cost of our approach in Table 2 below.StartFirst bidSubsequent bidFinalizeProxied0.02290.00920.0060Solidity0.00940.00450.0029Vyper0.01080.00450.0029Discussion and Key TakeawaysDeveloping the approach above necessitated overcoming a number of technical challengesThe first such challenge is that of how program implementations could be called with the same state. Our approach requires both implementations to be called with the same initial state. However, typically sequential program calls would modify the state. We describe our solution in detail in the paper, but in a nutshell we overcome this by having the proxy delegate first to itself, passing in call data as an argument, along with the implementation and call checks to perform.The second challenges centers on how the checks that we want to be performed after each program execution should be encoded. A check is a call to a contract that needs to be consistent after each execution.The full code of the proxy contract can be found on GitHub. In particular, the solution to our first problem is around line 168 and the encoding used is described from line 29.Our approach comes with strong benefits in relation to smart contract reliability, but has some limitations.Transaction fees: This approach will always at least double the cost of transactions, and potentially increase it further if many calls need to be performed. There is no direct way to prevent this or improve it significantly at the implementation level. This means that the viability of this approach will depend on the willingness of users to support different levels of transaction fees.Development cost: The approach implies greater development costs, as the same program intent needs to be implemented more than once. However, in the same way that the Ethereum Foundation funds external teams to maintain different clients, a well established protocol could potentially do the same.Storage layout: Our approach requires implementations to use the same storage layout, since the proxy contract will be storing all the states of the contract. This imposes some potentially undesirable rigidity on alternative code implementations.ApplicabilityThis work is applicable to anyone developing smart contracts who is seeking to reduce bug risk and improve quality assurance.In particular, we believe that this approach is particularly well suited for protocols that have well-defined specs.It allows multiple teams to independently work on the implementation of the spec and ensuring that the resulting implementations are indeed equivalent.', 'Welcome to the forum @fskforte – it’s always a real treat having original authors in the forum fsxforte:This research is inspired by and has clear parallels to the field of avionics, where on account of the safety-critical environment, flight control systems typically feature multiple redundant implementations.Do you know of other models used for mission-critical systems used in other industries?', 'Welcome to the forum @fskforte and thank you for the excellent summary!The proposed proxy architecture is really interesting. I’m curious about how it would handle execution mismatches if, for example, the execution outcome of the Solidity implementation leads to a different state transition relative to its Vyper counterpart. A lot of popular proxy and admin implementations, like Open Zeppelin’s, have pause() functions that can effectively halt the application.Would that be the desired outcome in the event of a mismatch?', 'Oh wow, this is so cool! Thank you fsxforte for this research.I have a question about its potential applications in the future. I can’t determine that if this idea is “nonsense”, “doable but not realistic”, or “could be a direction that’s worth working on”.Currently there’s a lot of contract code (templates) being reused, like the ones from openzepplin - and it’s a good thing because it makes the foundation of contract security safer by reusing battle-tested code.Under the dissimilar redundancy paradigm, we want to introudce redundancy to the ecosystem, and we believe that it can be a good security practice for everyone.Do you think that it is possible to (say, on some extermely cheap L2), design an incentive pattern for people to code and deploy modularized contract parts for different but minimalistic function on the network?Ideally, by competition(between different implementation contracts of the same minimalistic logic) and regulated calling (spec-wise) of redundant contracts, can we make this a standard practice that is both cheaper than auditing (CANNOT REPLACE AUDIT COMPLETELY, it’s complementary), and as a result the whole ecosystem is safer?Sorry if I worded my imaginary situation wrong or I got your research result wrong - it’s quite fascinating!', 'Following up on (or adding to) @jmcgirk’s and @cipherix’s questions:I enjoyed the detail you provided on the Boeing 777 Fly-By-Wire system, specifically the three different “lanes” using various power supplies and processors. In that example, the “redundancy” seems very reliably implemented in the physical world. If there is any discrepancy in the lanes, that would be known virtually instantly given the speed of semiconductors, and the plane (presumably) could not take off.Everyone agrees on the need for secure smart contract implementation, and your avionics analogy sounds promising. But unless I’m misunderstanding, it’s more an analogy at this point than reproduction of a literal model. For one thing, smart contracts occur in a complex, decentralized network which adds enormous complications that we don’t see in the physical airplane model. You also mention that transaction fees would “at least” double, and this is in systems already beset by great gas costs and scalability issues.Could you walk us a bit more through the similarities to the avionics “three lane” analogy and how they would or wouldn’t apply to networked smart contracts?', 'Hey @jmcgirk - thanks for the welcome!There are quite a few areas/industries where other models are used for mission-critical systems. Rail is one (e.g. https://web-archive.southampton.ac.uk/deploy-eprints.ecs.soton.ac.uk/8/1/fm_sc_rs_v2.pdf) (quite an impressive track record for bugs in the abstract!).The label ‘N-version’ programming can also be used here, with applications also extending to electronic voting.', 'Thanks @cipherix!The idea would be that if there is a mismatch, the execution is halted. Due to atomicity of Ethereum transactions, we can be sure that such a halting would not result in some inconsistent state, so the transaction could revert. The counterpart to this idea is that a transaction only executes if all program versions agree with the return value/state change', 'Hey @Jerry_Ho!This is an interesting idea. It sounds like you are effectively suggesting a market for modularized contract parts, where different developers / teams of developers would be in a tacit competition to produce reliable code, in the same way vendors are in tacit competition to produce reliable widgets. It’s a cool idea.I agree that competition and regulation (in terms of defining interfaces, etc) would be important. Thinking through the economic design would also be important. For the developer of a modularized smart contract component, could there be some sort of reward mechanism for creating the most robust implementation of a specific component?', 'Hey @rlombreglia, thanks for this reply!I think this is more than just an analogy. While it is true that smart contracts live in a complex decentralized world, the EVM itself does exist as a single entity comprised of every node in the network. (I’m limiting things to the Ethereum world here). Since transactions are atomic, it is possible to enforce that multiple implementations have to agree in order for the transaction to be fully executed. This is what we do in the paper, essentially. While of course the networking does add complexity, I think of this as complexity at the network layer/layer zero rather than the blockchain layer / layer one!', 'Heyo @fsxforte, it is great to find you here! Hope I will be seeing you during SBC’22 in person with @danhper.I think this is such an amazing design, however, the gas + development overhead might have extremely exclusionary effects. One reason for this is that, in my memory, most protocols that were hacked are early projects with large demand. Since they are early projects, most teams are swamped with other tasks. Therefore, for these projects, implementing this design will not just be a technical decision but also a strategic/cultural one (will Gryostable implement this design?). It will slow down their Go-To-Market (GTM) due to technical overhead and user acquisition due to high gas costs.So, after publishing this paper, what have you learned from chatting with early-stage founders and what are their reception on this? Also, I am curious what the late-stage protocols (Uni, Maker, Comp, Aave) think about this design. Do you see any of them experimenting with this any time in the near future (12 months)?Thank you so much for coming by! Amazing work.', 'Welcome to the forum @Kydo and thank you for joining this fun discussion! You bring up a good point on this architecture’s impact on users.As @fsxforte discussed in section V of the paper, transaction fees in fact make this design prohibitive in networks that face congestion and high fees.From the paper:As we have seen in the previous section, this approach will always at least double the cost of transactions and potentially increase it further if many calls need to be performed. There is not any direct way to prevent this issue or improve it significantly at the implementation level. This means that using such an approach on a expensive network, such as Ethereum, is almost unfeasible, as the price increase for transactions would likely be unacceptable for many users. However, more and more layer two solutions are being developed, with transaction fees orders of magnitude lower than what can be seen on Ethereum mainnet. This is for example the case of Polygon mainnet, which we used for the evaluation earlier. As we can see in Table II, although the costs have been doubled, this represents an increase in the order of a cent in the worst case. As Ethereum is moving towards a layer 2 future [17] and transaction fees become negligible, the fee overhead of our approach might become less and less of a concern.Indeed, the doubling of transaction fees in Ethereum as it stands does impose an adoption barrier. But perhaps this architecture could be solely applied to critical functions within the application, like a custody contract, where redundancy and stronger guarantees around state transitions are key.I’d also be interested in hearing what the feedback from the community has been. Given the prevalence of DeFi exploits, could founders leverage this architecture to build versions of their applications suitable for power users?In other words, is there a scenario where an application has a “version” that implements dissimilar redundancy for power users willing to pay a premium in fees for increased security?', 'Hey Kydo! Yes, should see you in person at SBC!Agreed that the gas and development might have exclusionary effects given typical current development approaches to smart contracts. I think there’s a debate to be had about whether it should have these effects in an ideal approach to development, and what the ideal GTM speed should really be.Regarding chatting with early-stage founders, this paper is only just out and we’re just starting to talk with people now about this approach. Very interested indeed to hear what people make of it at both a technical and philosophical level. And perhaps we can continue this discussion at SBC!Thank you, and thanks for the comments!', ' cipherix:solely applied to critical functions within the application, like a custody contract, where redundancy and stronger guarantees around state transitions are key.Sorry for the long wait ser. I totally agree with this and I think this is quite an interesting design. @fsxforte what do you think?', ' fsxforte:Since transactions are atomic, it is possible to enforce that multiple implementations have to agree in order for the transaction to be fully executed.Thanks for the clarification (and sorry for my late response). Yes, I think I missed the point (when reading your summary) that the atomicity of transactions made this possible at the blockchain layer.', '@fsxforte always glad to see more novel ways of protecting the space from hacks, losses and exploits.The procedure seems solid with potential real-world applicability.However, I have one reservation…You mentioned in the takeaways that the process will always at least double the cost of transactions, potentially increasing it further if many calls are made.This point reminds me of the never-ending trade-off blockchain users make between security, scalability, and performance.In the end, it is still a solution worth its troubles ', 'This post was flagged by the community and is temporarily hidden.']"
                Notable Works in Decentralization metrics              ,https://www.smartcontractresearch.org/t/notable-works-in-decentralization-metrics/828,Mechanism Design and Game Theory,54,[],"['notable-works', 'summary', 'discussion', 'defi', 'game-theory', 'governance']","['Notable works on “Decentralization Measurements” (Historical order) :“Is Bitcoin a Decentralized Currency?”Source: https://www.researchgate.net/publication/270802537_Is_Bitcoin_a_Decentralized_Currency  8Citation: Gervais, Arthur & Karame, Ghassan & Capkun, Vedran & Capkun, Srdjan. (2014).Summary: This paper describes centralization bottlenecks in Bitcoin and proposes some possible improvements.The authors identify different concepts which tend to centralize Bitcoin such as the role of clients/client developers and the emergence of ASICs, mining pools, and tainted coins. They also describe improvements such as more decentralized mining pools, the diversification of available services (exchanges, wallets, etc.), and transparent decision-making from developers during client maintenance and implementation.Tags: consensus, bft, ethereum“The Meaning of Decentralization”Source: https://medium.com/@VitalikButerin/the-meaning-of-decentralization-a0c92b76a274 7Citation: Vitalik Buterin (2017).Summary: This post discusses the definition of decentralization by describing three axes of “software decentralization” (architectural, political, and logical decentralization) and applies them to the blockchain.The author studies blockchains’ properties (fault tolerance, attack resistance, collusion resistance) and their relations with ‘software decentralization’ axes. He identifies centralization bottlenecks in blockchains and suggests ways to improve them (better diversity of developers and implementations, proof of stake over proof of work, disincentive various collusion models)Tags: consensus, bft, proof-of-stake, proof-of-work, ethereum“Quantifying Decentralization”Source: Quantifying Decentralization. We must be able to measure blockchain… | by Balaji S. Srinivasan | news.earn.com 6Citation: Balaji S. Srinivasan and Leland Lee, (2017)Summary: This post introduces the Nakamoto Coefficient, a quantitative measure of the decentralization of a blockchain system.The authors explain that decentralization measures can help compare blockchains, but also monitor and optimize current architectures. They propose the Nakamoto Coefficient based on tools to measure non-uniformity within a population (Gini coefficient and Lorenz curve). They identify that blockchains are generally composed of six so-called “sub-systems” (mining, client, developers, exchanges, nodes, ownership) and that if a subsystem is centralized, the system is centralized. They apply their measures to Ethereum and Bitcoin sub-systems and discuss the pertinence of the results. In this response to the post, Buterin brought clarification to some of the subsystem metrics and discussed the limitations of using Gini.Tags: consensus, bft, proof-of-work, ethereum“Decentralization in Bitcoin and Ethereum Networks”Source: [1801.03998] Decentralization in Bitcoin and Ethereum Networks 2Citation: Adem Efe Gencer, Soumya Basu, Ittay Eyal, Robbert van Renesse, Emin Gün Sirer (2018).Summary: This paper provides a comparison of the decentralization of Bitcoin and Ethereum based on node capacities and usages. They discuss known biases and limits of their metrics and dataset before suggesting improvements.The authors focus on the resource capabilities using metrics such as the structure of the network (latency/geography), provisioned bandwidth, distribution of mining power, mining power utilization (pruned/uncle blocks network ratio), and fairness (pruned/uncle blocks miner ratio). To collect a consequent amount of data as accurately as possible, the authors deployed a “probe” connected to different Ethereum and Bitcoin nodes for one year. This research was one of the first large-scale, peer-reviewed, formal studies in the field of decentralization metrics.Tags: consensus, bft, proof-of-work, ethereum“Measuring Blockchain Decentralization”Source: Measuring Blockchain Decentralization | ConsenSys Research | ConsenSys 5Citation: Everett Muzzy and Mally Anderson (2019)Summary: This paper introduces several categories and sub-systems composing a blockchain system. They identify pertinent decentralization measures for each sub-system.The authors take a step back from their previous article by concluding that, due to their specificities (notably concerning consensus algorithms) and always-evolving architectures, blockchains are difficult to compare in terms of decentralization. They focus their research on the categorization of decentralization metrics that can be used regardless of the type of consensus algorithm. They describe 19 sub-systems within four categories of metrics present among any blockchain architecture. They apply these metrics to Ethereum and discuss the results.Tags: consensus, bft, proof-of-work, ethereum“Taxonomy of centralization in public blockchain systems: A systematic literature review”Source: Taxonomy of centralization in public blockchain systems: A systematic literature review - ScienceDirectCitation: Ashish Rajendra Sai, Jim Buckley, Brian Fitzgerald, Andrew Le Gear (2019)Summary: This publication provides a systematic literature review of centralization in blockchains focusing on 89 identified papers (from 2009 to 2019). They build and iterate on a taxonomy through feedback acquired from experts interviews. The taxonomy provides centralization metrics and measurements but also highlights research gaps.Authors describe in-depth the methodology leading to the selection of publications, metrics, and measurements (systematic literature review, interviews, cross-validation, etc.). Their taxonomy relies on a categorization of blockchains’ layers provided in this publication and being extended by adding additional layers etc. The authors highlight the fact that centralization has distinct aspects (13 identified) and discuss the significance (or the lack) of measurements. They compare different aspects of centralization in Ethereum and Bitcoin. This publication is one of the most recent, formal, and complete studies/literature reviews on decentralization.Tags: consensus, bft, ethereum“Measuring Decentrality in Blockchain Based Systems”Source: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9205256Citation: S. P. Gochhayat, S. Shetty, R. Mukkamala, P. Foytik, G. A. Kamhoua and L. Njilla (2020)Summary: This paper identifies the emergence of centralization in three “layers” of blockchain systems (governance, network, storage). They identify and apply layer-specific metrics to Ethereum and Bitcoin and discuss the results.The authors first provide a summary of decentralization-measurement related works. Then they describe their own categorization of metrics before applying and discussing the experimentation results. They conclude that with time BTC and ETH nodes tend to centralize.Tags: consensus, bft, proof-of-work, ethereum“A Coefficient of Variation Method to Measure the Extents of Decentralization for Bitcoin and Ethereum Networks”Source: http://ijns.jalaxy.com.tw/contents/ijns-v22-n2/ijns-2020-v22-n2-p191-200.pdfCitation: Wu, K., Peng, B., Xie, H., & Zhan, S. (2020).Summary: This paper describes a quantitative method based on a coefficient of variation (relative standard deviation) to evaluate the degree of decentralization for a blockchain system.The authors provide a measurement method which can be applied to any metrics to estimate distribution/variability of centralization in blockchains systems. They apply their metrics to address balances (top 100 addresses) and blocks mined (a 7-day period in October 2018) in Bitcoin and Ethereum networks before concluding that, according to the chosen samples, Bitcoin’s mining and wealth is more distributed than Ethereum’s.Tags: consensus, bft, ethereum“Measuring Decentralization in Bitcoin and Ethereum using Multiple Metrics and Granularities”Source: [2101.10699v2] Measuring Decentralization in Bitcoin and Ethereum using Multiple Metrics and GranularitiesCitation: Qinwei Lin, Chao Li, Xifeng Zhao, Xianhai Chen (2021)Summary: This article compares Bitcoin and Ethereum decentralization using three metrics and several temporal granularities.The authors use the Shanon entropy, Nakamoto coefficient, and the Gini coefficient with different time periods (days, weeks, months) on mining power distribution. They conclude that the degree of decentralization in Bitcoin is higher, while it is more stable in Ethereum.Tags: consensus, bft, proof-of-work, ethereum“Against overuse of the Gini coefficient”Source: https://vitalik.ca/general/2021/07/29/gini.html 4Citation: Vitalik Buterin (2021).Summary: This post discusses the limits of using the Gini coefficient as a measure of inequality in the blockchain context and suggests alternatives.The author highlights that inequality in an internet community can come from a lack of interest (as opposed to a lack of resources in a geographic community). He suggests alternatives (Nakamoto coefficient, Herfindahl-Hirschman index, Theil L/T index) and tries to distinguish between the concentration of power and the lack of resources in the metrics.Tags: ethereum', 'Sami, I’m also very interested in the subject of decentralization … in terms of technology, but also politically in the sense that large centralized organizations quickly become ungovernable and ultimately corrupt. You’ve posted a very interesting collection of materials here. I’m curious about what’s going through your mind. Would you mind sharing a few of your thoughts about where you’re going with this? What is your ultimate purpose in collecting all of this information about decentralization? Are you developing a project?', 'Ralph, the purpose of performing this state of the art about decentralization metrics was to build the foundations for an original research project which aims at exploring the solutions required for blockchains to improve decentralization. The goal is to abstract all the aspects of decentralization and highlight research gaps to provide directions for researchers to improve decentralization in their field of expertise. Discussions and collaborations will be stimulated around the expansion of the taxonomy provided in Sami_B:“Taxonomy of centralization in public blockchain systems: A systematic literature review”Source: Taxonomy of centralization in public blockchain systems: A systematic literature review - ScienceDirectCitation: Ashish Rajendra Sai, Jim Buckley, Brian Fitzgerald, Andrew Le Gear (2019)The idea would be to follow the same exploratory research approach to describe threats to decentralization, how to measure those threats, and what could be the solutions to those threats. In this vein we will try to make the already existing taxonomy more up-to-date and complete to integrate any innovations and not covered aspect of decentralization.Anyone who wants to collaborate is free to contact me!', '@Sami_B can you tell us a little bit about why this project would be useful? Why is it useful to develop these kinds of metrics? Would this be a tool for investors or would there be an engineering use for this project?', 'Such research are useful because decentralization can be considered as the core property of blockchains. However centralization emerges under numerous forms (from voluntary trade-offs to gain on scalability but also from unanticipated situations) and there is currently no consensus on the decentralization’s terminology in the context of blockchains (some consider address balances as a representative metric while others may focus on miners’ concentration).My vision is that blockchains’ decentralization has distinct aspects which still need to be explored (to identify centralization bottlenecks, their measurements, and their potential solutions). However we do not need to compare blockchains to improve their decentralization. Therefore the project will not aim to provide a scale of comparison or an audit tool but rather aim to highlight the risks of “centralized” approaches in order to provide direction for better models and usages.As the research in this domain is still nascent, the research approach will be an “exploratory research” to define the scope of future research.I will present the project and provide more details soon in a discussion post.', 'We have open bounties available for anyone who can suggest additions, deletions, corrections or updates to this page. Please see forum 5 for details.', 'Decentralization, while meaning different things to different people is a common concept in blockchain. This awareness is the first step on the road towards harmonizing what it is and how it can be measured. This is why this list of notable works in decentralization metrics put together by @Sami_B are vital.On that note, here are a few suggestions for additions to the list.Mechanism Design and Game TheoryIs Decentralisation Even Real?Source: https://hackernoon.com/is-decentralisation-even-real 1Authors: Sharmini RavindranDescription: This article looks at decentralization and how it features in decentralized finance (DeFi).Relevance: This work, as a balance to other research focusing on the technical aspects, examines the economics aspects.Citation: Sharmini Ravindran “Is Decentralisation Even Real?” in Hackernoon, https://hackernoon.com/is-decentralisation-even-real 1.Tags: decentralization, game theory, regulatoryMechanism Design and Game TheoryDecentralisation in the blockchain spaceSource: https://policyreview.info/glossary/decentralisation-blockchain-spaceAuthors: Balázs Bodó, Jaya Klara Brekke, and Jaap-Henk HoepmanDescription: The paper reexamines what decentralization means in the wake of accelerated adoption.Relevance: This work is important because it addressed the common misconception of using “decentralization” and “distributed networks” as synonyms.Citation: Bodó, B. & Brekke, J. K. & Hoepman, J.-H. (2021). Decentralisation in the blockchain space. Internet Policy Review, 10(2). https://doi.org/10.14763/2021.2.1560.Tags: decentralization, game theoryMechanism Design and Game TheoryDefining Decentralization for LawSource: https://lex-node.medium.com/defining-decentralization-for-law-58ca54e18b2aAuthors: Gabriel ShapiroDescription: This article seeks to examine what decentralization means for law.Relevance: This work is relevant as it suggests some metrics useful for measuring decentralization for legal purposes.Citation: Gabriel Shapiro “Defining Decentralization for Law” in Medium, Defining Decentralization for Law | by _g4brielShapir0 | Medium.Tags: decentralization, game theory, regulatoryMechanism Design and Game TheoryBlockchain technology and decentralized governance: Is the state still necessary?Source: https://www.researchgate.net/publication/315919685_Blockchain_technology_and_decentralized_governance_Is_the_state_still_necessaryAuthor: Marcella AtzoriDescription: This paper explores the potential of blockchain technology being used to decentralize societal governance.Relevance: This work is one of the earliest works that researches the use of blockchain for societal governance and concludes that while technology could decentralize societal governance, the State would not be replaced, rather technology would complement.Citation: Atzori, Marcella. (2017). Blockchain technology and decentralized governance: Is the state still necessary?. Journal of Governance and Regulation. 6. 10.22495/jgr_v6_i1_p5.Tags: decentralization, game theory, regulatoryFrom @Sami_B’s list and the new suggestions, it appears that there are three categories in which decentralization can be considered: as it relates to (1) the protocol, (2) the token economics and (3) the users. All of these raise novel legal questions and must be the reason James @jmcgirk said he is “curious about the legal reasons for wanting decentralization metrics.”There are several legal reasons for wanting decentralization metrics as seen in addressing issues relating to liability, jurisdiction, ownership and so on. There is a reason why the US Congress can summon Mark Zukerberg to answer for Libra, but not Satoshi Nakamoto to answer for Bitcoin. It is because, one is deemed to be centralized (Libra) and the other “sufficiently decentralized”.On the issue of liability, decentralization could mean the absence of legal personhood and thus the infeasibility of identifying who should be held liable. On the flip side, a lack of decentralization means certain individuals or entities can be identified and held liable.On the issue of jurisdiction, decentralization could mean no single terrestrial jurisdiction can be invoked. Nevertheless, it is possible to embed decentralized justice within any decentralization category.On the issue of ownership, in the absence of legal personhood, it would be difficult to assign intellectual property rights to no one. Generally, there is usually a resort to making all intellectual property open source.These issues are just a tip of the ice berg. I look forward to this research making significant progress and exploring more issues that corroborates the reason why knowing what decentralization is and being able to measure it is vital.', 'Thanks for the suggestions @Fizzymidas, I already had a few from your list in stock and will work to incorporate them.It’s interesting because your addition clearly illustrates the lack of legal aspect in the 1st version of the notable works.Concerning these legal aspects, an interesting concept is the CRC one, which proposes a framework (an evaluation grid) to determine the proximity between a token and a security (noting that this framework has no legal value). They assign a score to the tokens based on scale from 1 to 5; 5 being the closest to a security. While this score has no judicial value, it allows to start “measuring” certain metrics and provide some guidance. Framework’s methodology/measurements try to answer to the “Howey Test” (test determining if an asset is a security) which is also mentioned in this publication.', 'This appears to be decentralization as it relates to tokeneconomics.I came across CRC last year through Dr Phillip Sander, one of the CBDC panellists. Although, I have not truly studied it.Some lawyers have argued that the Howey Test is not an appropriate litmus test for determining if tokens are securities or not. But, I am not aware of any alternative. Moreover, different laws apply in different jurisdictions.', 'I would be interested to see the rationalization as to how the Howey test would not be appropriate for determining if something is a security, as that is its specific purpose.  SEC has released guidelines for token sales, but obviously, their rules do not apply across the globe.', 'As a compilation of resources on “measuring decentralization” in blockchains (mostly Bitcoin + Ethereum), it is interesting how little scholarship is captured on this, and how most of it comes out of the blockchain community (Buterin, Balaji, Gun Sirer, etc).Also, do people have more thoughts on the relative conclusions (i.e. ""tends to centralize, or decentralized vis a vis’ more stable)?', 'Yes many articles are not academic but the trend is reversing, as it has been with blockchains in general for some years. I think this pattern appears when the concepts are still immature, we see this in many innovations like DeFi or NFTs for example.Concerning your question about conclusions, Ethereum and Bitcoin are mainly used as 2 references to execute such comparisons. Without taking any position, most publications show that Bitcoin is more “decentralized” than Ethereum on most of the used metrics. I will try to summarize some of the findings from the Notable Works list: Sami_B:“Measuring Decentralization in Bitcoin and Ethereum using Multiple Metrics and Granularities”Source: [2101.10699v2] Measuring Decentralization in Bitcoin and Ethereum using Multiple Metrics and GranularitiesCitation: Qinwei Lin, Chao Li, Xifeng Zhao, Xianhai Chen (2021)This article concludes that over time, ETH and BTC nodes tend to centralize, they also provide some comparison such as : Sami_B:“A Coefficient of Variation Method to Measure the Extents of Decentralization for Bitcoin and Ethereum Networks”Source: http://ijns.jalaxy.com.tw/contents/ijns-v22-n2/ijns-2020-v22-n2-p191-200.pdfCitation: Wu, K., Peng, B., Xie, H., & Zhan, S. (2020).This research concludes that Bitcoin is 27.3% more decentralized than Ethereum concerning block mined (and 16.5% concerning address balance). Sami_B:“Measuring Decentralization in Bitcoin and Ethereum using Multiple Metrics and Granularities”Source: [2101.10699v2] Measuring Decentralization in Bitcoin and Ethereum using Multiple Metrics and GranularitiesCitation: Qinwei Lin, Chao Li, Xifeng Zhao, Xianhai Chen (2021)In this one they conclude that ‘the degree of decentralization in Bitcoin is higher, while the degree of decentralization in Ethereum is more stable’ according to their multiple metrics and granularities:‘Compared with Bitcoin, Ethereum tends to be more stable but less decentralized in terms of the measurements of the Shannon entropy.’‘Compared with Bitcoin, Ethereum tends to be less decentralized, as revealed by the results of the Nakamoto coefficient.’‘Compared with Bitcoin, Ethereum tends to be significantly less decentralized in terms of the measurements of the Gini coefficient, which might be affected by the huge difference of the block production rates between Bitcoin and Ethereum.’ Sami_B:“Decentralization in Bitcoin and Ethereum Networks”Source: [1801.03998] Decentralization in Bitcoin and Ethereum NetworksCitation: Adem Efe Gencer, Soumya Basu, Ittay Eyal, Robbert van Renesse, Emin Gün Sirer (2018).They conclude that, according to their metrics and data from 2016/2017:‘Bitcoin has a higher capacity network than Ethereum,but with more clustered nodes likely in datacenters’ , ‘Ethereum nodes are geographically further apart than Bitcoin’.‘In Ethereum, the block rewards have less variance than Bitcoin’s.’‘Ethereum has a lower mining power utilization than Bitcoin, likely due to the high block frequency.’ There are more pruned blocks in Ethereum which means the power is, in a sense, less efficiently used than in Bitcoin (but they recognize that their data may be incomplete). Sami_B:“Taxonomy of centralization in public blockchain systems: A systematic literature review”Source: Taxonomy of centralization in public blockchain systems: A systematic literature review - ScienceDirectCitation: Ashish Rajendra Sai, Jim Buckley, Brian Fitzgerald, Andrew Le Gear (2019)State of centralization940×388 129 KBConsensus centralization940×373 114 KBGithub Contributions935×395 98.8 KB', 'Thanks for sharing this resource, Sami.', 'Decentralization is an interesting topic that fascinates me both technologically and politically, as massive centralized organizations always devolve into unmanageable, corrupted entities. The materials you’ve placed here are all quite intriguing, with great sense, and for sure it’d benefit both old and new members of this forum. Thanks @Sami', 'I agree with your posit on decentralization! Freakytainment:centralized organizations always devolve into unmanageable, corrupted entitiesFrom this point, do we say this does not happen in DeSoc?How do we measure this in DeSoc?']"
                CommunityRule refactor now online              ,https://www.smartcontractresearch.org/t/communityrule-refactor-now-online/1567,Governance and Coordination,18,[],"['governance', 'summary', 'discussion', 'dao']","['TLDR:With the support of SCRF, a refactor of the CommunityRule codebase is now online, poised for much more ambitious future projects.BackgroundLast year, Smart Contract Research Forum provided a grant 2 to CommunityRule, an open-source Web app led by MEDLab at CU Boulder meant to serve as an interface for community governance design. The project was led by Deacon Rodda at SQGLZ, and I have already shared 1 Deacon’s brilliant reflections on the project. Drew Hornbein took on the development work to implement Deacon’s recommendations. As I reported earlier, Asher Farr did some experiments with reimagining CommunityRule in Blockly.CommunityRule was initially developed as a demonstration project by me. While it achieved some exploratory adoption, the (badly designed!) technical underpinnings limited its ability to be modular and adaptable to new challenges.ResultsThe SCRF-funded project enabled some important advances:Refactoring the application code from vanilla JavaScript to the much more advanced Vue.jsMoving the database from raw HTML to JSON/YAML, enabling vastly improved data portabilityRedesigning the authoring interface to be more self-explanatory, responsive, and flexibleEncouraging custom module design as the default, rather than relying on pre-written modulesSmall adjustments in nomenclature and usabilityDiscussion and Key TakeawaysThese changes bring us closer to the goal of connecting a) a visual design interface with b) programmable governance software. For example, we hope that CommunityRule can be used in the future for designing and implementing governance processes using DAO technologies and Web2 services like PolicyKit 2 and Modpol 2. Moving to a JSON-based data model enables us to much more easily do things like:Add arbitrary data fields on governance modulesDeploy custom instances of CommunityRule with custom module setsTranslate modules into working governance codeOne important insight during this process, aided by the Blockly experiment, was recognizing the need for governance modules to be much more precisely specifiable, rather than simply being described in natural language text. As we move closer to enabling computable rules on CommunityRule, we need to gradually lessen our dependence on natural language and support appropriate machine-readable data models.Implications and Follow-upsWe will continue to develop the basic software for usability and new features. This process will be spurred by much larger pending grant projects, in partnership with the Metagovernance Project 1, which we hope will fund CommunityRule development for use by open-source communities and integration with PolicyKit, respectively. Thanks to the groundwork provided by this phase of development, we can hit the ground running on those efforts.Thank you, SCRF, for your support! And we invite anyone interested to give the new CommunityRule a try 10 and let us know what you think.', 'Thank you for the update, @ntnsndr. This is exciting progress!I am particularly interested in the observation that natural language text ended up being a barrier of sorts. It certainly makes sense that as CommunityRule is implemented across platforms that machine-readability would be important, but I am hoping that you would be willing to explain a little more about this insight.', 'Thanks for this. Perhaps I overstated the case, as natural language explanations are really important. It’s just that they are inadequate for precisely defining how a module should function. I think the long term goal for CommunityRule should be to include space for both precise specification and NL description—like well-commented code. So a module might look like:majority_binary_vote = {  description = ""Positive result if over 50% of members vote yes""  threshold = > members*.5  time_limit = nil}In a case like that, having to specify both in natural language and code is revealing. In NL we might say “it’s a majority vote.” But explaining it to a computer requires answering some other questions.Is it a majority of all members, or just of voters?Is there a time limit on voting?What happens if the threshold is reached?So in addition to greater machine readability for going beyond NL, there is the likelihood of greater clarity and specification.', 'Congratulations on finishing the refractor @ntnsndr and thank you for updating us on everything! I’m excited to see everything come together. Just using your template was really helpful to me in designing governance for a project I’m working on. Actually, that makes me wonder, @Sunnypizza213 whether you might find CommunityRule useful for designing your agriculture DAO(if anyone is curious about Sunny’s project check out - Project Agriculture DAO)', 'Thanks James! I am happy to do CommunityRule sessions with any communities interested.', 'Thank you.We will check it out for sure.', 'Are there any projects that are experimenting with new types of organizations within companies and organizations? I’ve been very surprised at how rigid and dogmatic the culture of agile sprints, proposals and project management in tech is compared to other industries. I understand it comes from running decentralized teams against hard deadlines, but whenever read about exciting new governance structures, I have to wonder about life for the contributors and specialists working for the decision makers.', 'Murmur 1 is an interesting example of a platform for building governance structures for teams in existing organizations that are meant to be forkable and copyable.Great question about quality of life. I’ll have to think about how that might become incorporated in interfaces.', ' ntnsndr:Great question about quality of life. I’ll have to think about how that might become incorporated in interfaces.Maybe someone could write “Microserfs 3.0” (a la Douglas Copeland’s 1995 novella/Wired piece)', 'I too am fascinated by the tension between natural language and machine-readable text. One possibly illustrative example is a non-profit I’ve mentioned in this forum before, Project Haystack, which has created standardized semantic models for describing physical locations, machinery, and the data streaming off IoT sensors—all for the purpose of achieving interoperability.To give the simplest taste, here’s a familiar location described in Project Haystack terms:id: @whitehousedis: ""White House""sitearea: 55000ft²tz: ""New_York""weatherStationRef: @weather.washingtongeoAddr: ""1600 Pennsylvania Avenue NW, Washington, DC""geoStreet: ""1600 Pennsylvania Ave NW""geoCity: ""Washington D.C.""geoCountry: ""US""geoPostalCode: ""20500""geoCoord: C(38.898, -77.037)yearBuilt: 1792It gets more complicated from there, of course, but the point is that if all our wildly diverse manufacturers adopted the semantic standards advocated in the Haystack model, we’d be a huge step closer to implementing, say, a smart city.The rub is that most manufacturers refuse to use these standards—I think mostly because “Not Invented Here”—and of the companies that do use them (in whole or in part), most refuse to publicize the fact that they do.Thus, after a decade-plus of meticulous development, Project Haystack’s dream of interoperability goes unfulfilled for human political reasons. So much for coding utopia!', 'Exciting project! Thank you @ntnsndr ! I checked out the website and the product, it seems that we’re able to create our own CommunityRule with predefined modules. 3 things hereI wonder how did you decide to set these modules, is there any academic work or references on these definition and categories?I see some overlap and connection between the four categories: culture, decision, process, and structure; and the modules in each category. Like would Do-ocracy decision making process would somehow conflicts with the Board structure? It would be interesting to think about what would be the whole picture of the organization with each of these combinations; and could we have the CommunityRule structures in a more scalable way, like part of the org is under certain CommunityRule while these small parts come together following a different CommunityRule?some module documentations are not finished yet, I would be very interested in helping out with those, lmk!', 'Thanks so much for this! Especially your interest in helping with documentation:) I’m actually unsure about whether to maintain those large text-based descriptions of modules. As I’ve reported to SCRF earlier 1, I’m interested in moving CommunityRule toward less of a text-heavy design and more toward configuration options that help explain to users how a module might work. Still, if you find those docs useful and would like to contribute, I would love that, and would be happy to make sure they’re readily available on the platform. Here they are in the repo.In terms of the module set, I am actually planning to simplify the set of default available modules and encourage more module customization. One piece of feedback I’ve gotten a lot is that a lot of the names of the modules are opaque and unfamiliar, and this makes the platform harder to use. For instance, I think it might be better, rather than having 5 different voting modules, to just have one voting module that can be configured in many different ways.I’m also not sure, if we have a shorter module list, if we need to maintain the categories of modules. There is certainly overlap, and it is hard to draw clean lines. But if anything, I think of the categories in terms of prompting users to make sure they have all those things—e.g., don’t forget about culture!I’d love to hear any further thoughts you have! If you’re interested in discussing your participation in the project, I’d be happy to set up a call.']"
                Adding a DeSci Section to SCRF?              ,https://www.smartcontractresearch.org/t/adding-a-desci-section-to-scrf/1563,Meta,137,[],"['discussion', 'governance']","['OverviewDecentralized Science (DeSci) is a topic that has seen a strong growth in interest in the first part of 2022. There are a lot of exciting projects and activities coming together to explore how to improve science with web3. As part of that, there’s a question of where some long tail discussions around DeSci can take place. This presents the SCRF community with an opportunity to add a section to the forum to house some of this conversation. This would also entail needing to bring on additional resources to support the engagement and moderation for this section.QuestionsThe idea came up to add a section on the forum for ‘Decentralized Science’ or ‘DeSci.’ As part of this, we need to consider:Should we add a section to the forum for this topicIf yes, what types of posts do we want (are thoughts on desci ok? Project overviews? How are you using web3? What problems are being solved in DeSci?)What types of posts are not allowed?Community discussionsGovernance decisionsWhat additional resources need to be added to ensure that this section remains active?The benefitsThere are two ways to explore the benefits, one is to the DeSci community and the other is to the SCRF community. For the DeSci community, this could provide a forum with operational support related to it to house long-tail discussions around DeSci. This could also provide an opportunity to extend some writing grants to DeSci community members and could extend an opportunity or two for part-time work supporting the DeSci section.The drawbacksTwo main drawbacks come to mind. One is that this is an application domain of web3, not a research domain area. Would adding a section on the forum that calls for non research related content too distracting from our core mission? Is there a way to provide clarity on what content is sought after on the forum to find a middle ground?The other is whether this adds operational strain to the content or engagement teams. If yes, is it possible to bring on one or two people for part-time grants to support engagement or content?My personal viewI know I’m biased, but I do think that providing this resource could be valuable to both the broad DeSci community and to the SCRF community. I think it would make sense to engage 1-2 people to support with this new section (already speaking to one person on it). I also think we would have to provide some focus to posts (e.g. what is DeSci, what problems are being addressed, how are web3 tools being used). I do NOT think general DeSci governance decisions (aka should the DeSci community plan something for a specific conference).DiscussionWhat do people think about the above questions?', 'Considering DeFi is more mature and we do not have a section dedicated to that, I would be wary of dedicating an entire section to DeSci.  I would lean towards creating a tag and accumulating enough work in that area to validate the need for an entire category.  I’m not sure DeSci is yet mature enough to warrant an entire section on the forum.I am not yet certain what “DeSci” is beyond “science + Web3”.  In that context, it would make sense to add a ""Web3 section and make “DeSci” a subsection of Web3.  I have not seen a clear enough definition of DeSci that goes beyond just the self-referential “decentralized science”.  Until DeSci gets a more clear definition, I would assert that it might be a considerable danger to dedicate many resources to something that may never come to agreement on its own definition.', ' eleventh:If yes, what types of posts do we want (are thoughts on desci ok? Project overviews? How are you using web3? What problems are being solved in DeSci?)I think this would be a great subcategory in the forum (maybe in governance?), but I worry it might be too specific for a new category, given how technology focused the rest of the forum is. And generally echo Larry’s concerns above. If this is part of a general change toward more intercategory discussion, I think it would be a great first step… but DeFi, NFTs etc. would be more immediately pertienent.As for posts, I think case studies and interviews of existing projects would be really interesting to add to our existing summaries and discussions. For example Brian Armstrong’s new think tank might be something to look at, especially if we can pull him in (he’s also involved with ResearchHub, if I’m not mistaken). Overall it would be nice to see more discussions generally.', 'I searched “DeSci” on the forum, and got 7 results:  Search results for ‘desci’ - Smart Contract Research ForumConversely, I searched “DeFi” and got 50+ results: Search results for ‘defi’ - Smart Contract Research Forum“Web3” yielded 41 results: Search results for ‘web3’ - Smart Contract Research Forum“NFT” yielded 47 results: Search results for ‘Nft’ - Smart Contract Research ForumJust purely based on the numbers, it would not seem logical to add a DeSci category if not intending to add categories for subjects that have more presence on the forum.', 'TLDRI think we should have a DeSci Section because it helps us be involved in a community that aligns with our interests and missionThe category should have similar content types as other categories to avoid some of the downsides, but there are some other types of post we should encourage in that section.Good observations, @Larry_Bates, but I think I disagree a little on whether or not a space needs to be mature in order for it to be a section on the forum. I appreciate the call out to the defi space though. I have some thoughts on that as well, but I’ll try to keep them organized below!Why DeSci Before XYZ Category?I have been spending time thinking about the forum categories lately and some of the observations that @Larry_Bates brought up are part of those thoughts. The initial forum categories were created in late 2020 as an initial attempt to organize some of the emerging overall topics in web3. Obviously, such an initial system is not going to keep up with an industry that moves as quickly as ours does. There is some attempt to capture this quick movement through the Terms Glossary and Content Tags 1 thread where our community suggests needed additions to the forum like a DeFi tag. Tags alone aren’t always great information organizing tools, however. It does seem like there is need for a community discussion about our categories overall and potentially a need to do some reorganizing of them. That seems like a growing priority based on what both @jmcgirk and @Larry_Bates pointed out. I look forward to being involved in that effort.I think that effort can happen concurrent to the adding of DeSci as a category to the forum. I am also interested in adding a Community section to the forum, but I don’t see that as getting in the way of any of the reorganization work being discussed above. As @eleventh pointed out above, there is a growing community interested in DeSci and SCRF has an opportunity here to not only be a space where DeSci can discuss and mature, but the intersection between web3 and DeSci seems to strongly align with SCRF’s mission 1 of advancing web3 research.In my mind, having a section on the forum creates the space for DeSci to mature as a field and application. I wouldn’t want our forum to only be a place where the most highly researched sub-fields of web3 get representation. Many of these emerging trends have need for a meeting space between “industry and academia.” I think by having a section on our forum for DeSci, we have an opportunity not to just be involved in exploring this emerging trend, but we also have an opportunity to shape and influence it. To the questions in @eleventh’s original post, we as a community would have the ability to set some types of guiding sections and constraints on content in the category.Structuring the DeSci CategoryIn @eleventh original post, it was asked: eleventh:One is that this is an application domain of web3, not a research domain area. Would adding a section on the forum that calls for non research related content too distracting from our core mission? Is there a way to provide clarity on what content is sought after on the forum to find a middle ground?Being an application and not a domain does mean, to me at least, that there should be some differences in the DeSci category than some of the other research categories. That said, I think there is a lot of similarity that we would want to retain and encourage in order to help guide discussions in a “SCRFy” direction.Areas of SimilarityAbout the DeSci category** Each category on the forum should have an about section. I think there is more to do with these in the future, but minimally there should be something like About the Consensus category.Notable Works in DeSci** This is an incredibly important part of every category, and I think that much more so for DeSci. Many of the questions being wrestled with in the DeSci community have notable works throughout history. “Science” is not some monolithic system. There are notable works in the philosophy of science that would be valuable to have access to as well as the application of web3 solutions to DeSci that are foundational reading for people learning about and doing work in the DeSci space as well as the web3 space.Key Problems in DeSci** This is a post type that I particularly like because it seems so mission aligned with SCRF. I actually thought it was part of the needed to have in the different categories already, but it somewhat looks like @lnrdpss  may have created this of his own accord in the Auditing 1 category. Regardless of the history of the post type, I think this is a requirement for me for the DeSci category if we are going to adopt it. It gives people a central space to concentrate on what are the next hurdles and might help avoid the downsides of “off topic” conversation that @eugene identified in the original post. @jringo recently posted essentially a think piece 1 that I think starts to populate this section already.Research Summaries/Discussion Posts** These are essential for SCRF, so I think it almost goes without saying these would exist in the DeSci category. Just the other day, @UmarKhanEth referenced a peer review 1 study during a community call that could heavily influence decisions about mechanism design and approaches to DeSci/Decentralized peer review. I think that should be summarized and I would like to discuss it on forum. I like the paper, but there are areas to discuss. Exactly what our forum is for.Areas of DifferenceExperiments in DeSci** DeSci gives us an opportunity to actively run and trial “experiments” right here on the forum. This is not something we really get to achieve in many of the other research categories on the forum. Specifying that a thread is an experiment in how to do something, like peer review, would make sense to have a specific home on the forum.Project Updates** This isn’t really that dissimilar, but I am putting it here because I believe we would see more activity like this. DeSci is likely to be a little more project oriented as it grows and matures as a field, so having updates on projects in the category makes sense. We have some models for this on the forum already. For example, @Mr.Nobody recently did an update post 1 on a project they are working on. There is likely some refinement we could do, but I think this would be a valuable type of content for a DeSci section.There are probably some other types of content we would want to encourage in the DeSci category if we adopt it that I would love to continue to flesh out. It might even help us think about content in the other categories as well.To me, if we had content type expectations like these, then we would be able to avoid the downsides mentioned in the posts above while also being involved in and influencing DeSci. I think it would attract an active community to forum while also helping SCRF accomplish its mission.I hope this post gets as much engagement as the SourceCred one did, by the way. This is a great discussion to have!', 'My initial though is absolutely add a DeSci section, but how would it be used? DeSci in my mind is a marketing term. So are we posting about other DeSci projects? Are we having conversations about DeSci conceptually? Is the DeSci category only to be used to discuss meta-analysis? I’m not seeing what kind of content would fall there.@eleventh can you elaborate a bit? Or give examples of content?This may be a good category to start discussing overall organizing of the DeSci community; decentralized peer review; or could be where DeSci research itself (from DAOs) is published.Should we add a section to the forum for this topic - only if we are defining how it should be used. I’m not intuitively seeing what kind of content would go there.If yes, what types of posts do we want (are thoughts on desci ok? Project overviews? How are you using web3? What problems are being solved in DeSci?) - I really see DeSci as a marketing/informational/educational term. I think this goes to the larger question of who SCRF wants to be within the DeSci community, and make the section about that! What is the strategy here?What types of posts are not allowed? - Follow existing SCRF rules here. No shilling, etc.What additional resources need to be added to ensure that this section remains active? - May make more work for the Moderation team (depending on usage) but will need folks monitoring the category in the same way. May be a good category for cross-pollitators to have domain over.', 'I’m in favor of adding a DeSci section to SCRF because doing so would be mission-aligned, builds on existing relationships, and prevents FOMO.Mission AlignedSCRF’s goal of connecting academia and industry to make Web3 research more actionable is in harmonic resonance with what DeSci is attempting to do. DeSci is trying to make scientific and academic learnings more accessible to people outside of universities. It also hopes to allow those without credentials to be a greater part of discovering new knowledge. This includes independent researchers, industry experts, and crypto punks.By creating and scaling online networks of researchers based not on degrees but on contribution, DeSci hopes to create a collaborative system that delivers on the promises of Open Science. Some great examples of what this could look like are covered in this TED Talk by Dr. Michael Nielsen 1 (who also authored Reinventing Discovery: the New Era of Networked Science 1). One notable advantage DeSci has that Open Science did not is cryptocurrency’s ability to align incentives between independent actors. This could play a significant role in solving some of the problems we face today 3.Building on Existing RelationshipsSCRF played big roles in the last two (and also first two!) organized DeSci conferences 1 as a host and speaker. In doing so, SCRF created relationships with organizations building a new possible infrastructure for research to occur. This includes visitors to our community call like ResearchHub, Ants-Review, and DeSci Labs. By creating a DeSci category, we invite organizations like these to deepen our relationship by sharing their thoughts, questions, and results.Preventing FOMODeSci seems like a far-off and sometimes intangible goal. Yet, what if it works? What if the inspired individuals in this space succeed in making a networked system of science built on incentives aligned using smart contracts? What if SCRF does not play a role in making this possible?This site is an example of a successful, engaged forum where Web3 research is discussed. Now is the time to build inroads and bridges which further the mission. To me, it feels natural for SCRF to foster a space where people can have conversations about how to make decentralized research possible and where the output of that research can be shared.', 'Timing wise, there isn’t much D content on the forum yet. IMAO make content before creating a category not the other way around.Can we start fromCreating a community section, and posting whatever DeSci content to that sectionRef: zube.paul:I am also interested in adding a Community section to the forumIf there is any DeSci content, we create a tag for itOnce there is a regular influx of content, we come back and discuss if we want a category for itI currently see no objection to the fact that there are a lot of content that deserve a better category to be in. It’s also the case that as web3 is evolving so quickly that this should be the first and not the last debate we have on whether to have a category on.Therefore we need a sustainable process of deciding that category X deserves a section to itself, category Y only gets a tag etc. My suggestion is everyone starts posting from the community category and we move on based on moderation efforts.Note: I see a lot of comments above expressing the interest of creating a category to incentivize content. I’m unaware that having a category on the forum is a perquisite for content creation. If you are interested in influencing content direction to ask for more resources to create DeSci content, consider doing a Research Improvement Proposal.', 'My 2 cents:.01: Make tags instead of categories. The forum is currently structured with categories related to root principles of DLT. Mechanism design, consensus layers, cryptography, oracles, governance, and scaling. Add to those categories.If I develop a mechanism design based on solving a problem in science, I should add it to mechanism and design and tag it “science”. If I develop a mechanism design based on solving a problem in finance I should post it in the same category and tag it “finance”. And so on with all the fields in which DLT is relevant. The different designs are probably actually related and the authors are more likely to interact if they post in the same category. I should be able to search “science” if I want to find DeSci related discussions..02: Stay away from “DeSomething” terms for headers. It’s marketing and language simplification for discussions. It brings to mind money and profit before technology.', 'In your thread that contains key questions that DeSci could address, my initial response was more inclined with what is being proposed here.  I think with the list of questions that you have created as a starting point, answering some of those questions about DeSci with tagged summaries could go a long way to validate the need for an entire category dedicated to that subject.  I do tend to agree that we should stay away from “De” anything as a main category in preference of tags.  I think if people that were inclined to contribute could make it more of a community effort so a single person doesn’t have to do all the content creation.', ' Twan:Timing wise, there isn’t much D content on the forum yet. IMAO make content before creating a category not the other way around.I absolutely agree. I think we need more DeSci content before considering a category, unless we’re planning to have special grants applications or some other new form of content in there.', 'TL;DR: DeSci and SCRF have considerable overlap in the field of Meta-Science. Topics like Peer review, publication, funding, open access, reproducibility, replicability and more are going to be critical for both spaces in the future. Let’s work together to build out these concepts in Web3Hi all,First time SCRF poster from the DeSci community. I just want to start out by saying thank you. Regardless of whether or not a DeSci section is the ultimate decision, we all appreciate Eugene and the SCRF community. The DeSci community would not have been able to grow this quickly without SCRF. We recognize and appreciate everything you’ve done so far.To the points @zube.paul and @ UmarKhanEth have made, there is considerable overlap between DeSci’s direction and SCRF’s mission. DeSci and SCRF are focusing on flip-sides of the same coin. DeSci aims to use Web3 technologies to build better science. SCRF aims to use scientific practices to build a better Web3. Perhaps a way to join these two missions would be shifting discussion from a DeSci category to a meta-science category?Here are a few discussion topics (in addition to what @zube.paul already mentioned) that would benefit both SCRF and DeSci within those parameters:Replicability and Reproducibility: A better system of science needs to focus first and foremost on creating and promoting replicable, reproducible science. The concepts and technologies being created in DeSci will be a catalyst for Web3 research. We can work together to build the systems and incentivize this behaviorOpen Access: Web3 is the perfect place to talk about the implementation of open access principles. Numerous projects are working on this concept in DeSci as we speak (for example @jringo and his work on Gridcoin). SCRF’s Web3 research is going to need accessible information and DeSci is building those tools. SCRF using and providing guidance during that development would be invaluablePeer Review: A better system for both science and Web3 needs to have peer review at the center. Right now, Web3 research is mostly presented through blog posts. It works for now but is not a scalable process moving forward. This could be a place to discuss best practices for technologically enabled peer reviewPublication: New technologies create new ways of disseminating information. SCRF has the potential to be a pioneer of new publication methodologies as Web3 researcher are more accepting of a new innovative system. DeSci tooling can be at the center of this. The DeSci community could have a lot to contribute to these conversationsFunding: Concepts like micro-grants and impact certificates are just a few interesting directions that Funding in Web3 for scientific research could follow. This is a topic that will be important to Web3 research as it scales. SCRF can be the cutting edge of those discussionsTheory of Change: This section can be used to answer the question “What impact can Web3 Science have?”. Having DeSci and SCRF contributing to a Theory of Change document together will create a more robust hypothesis with a broader, more complete vision. It can serve as the investment thesis for both groups in the futureMeta-Science Projects: Updates on projects in the DeSci community focused on building meta-science tooling. GridCoin, ResearchHub, DeSci Labs and many others are going to be relevant for SCRF as they facilitate research into Web3. It would be good to keep tabs on themOnboarding Scientists: How do we go about onboarding Web2 scientists into Web3 efficiently. Both DeSci and SCRF need the brilliant minds of Web3 to help us solve problems. This section can facilitate discussion on standard practices for onboarding scientists into Web3 for both groupsJust a couple thoughts here. Hopefully it gives a better picture of what this category could be.', 'I think that adding a category will simplify and encourage some content production around DeSci.The suggestions that @zube.paul has made could be a good starting point, using Notable works to create foundations as previously done, but with topics a bit more open than in existing categories such as Key Problems, Experiments, Project updates etc.We can, for example, easily ask existing DeSci projects to come to summarize their concepts but also to come together to define key problems or topics/categories that need to be discussed by the community.An interesting topic could also be the list of projects 1 started and curated by @eleventh , this list was disseminated and used across the DeSci community and I think it can be a good vector/overview of the ecosystem to engage various discussions, identify potential sectors (or lacking sectors) in DeSci.I agree that it could also deviate from SCRF’s first concepts but I think this is a good opportunity to capitalize on the already done work (conferences etc.) and the synergy around DeSci. Of course we have to provide some templates/guidelines (as discussed during a community call) to avoid project shilling etc but I think this could be a good mix between industry and academic as, for now, the focus of the forum is a bit more on vulgarization of academic publications than discussions between both sides leading to innovations.As raised by Paul, the discussion on Sourcecred was a good example of how the SCRF community can instigate/discuss the application of an industry tool to a specific problem.It could also be the chance for the DeSci community to highlight the difference with the existing concepts of Open Science and help to remove the hyper-financialization/speculation from the discussion.I don’t know how easy it is concerning the Discourse tool and the re-organization of posts, but we may also iterate on categories to see if creating a category is really encouraging content production or not.', 'I echo a lot of what’s been said about wanting to see more DeSci content on the forum. I would like to mention that of the 12 existing categories, 4 of them have 8 or less topics within (Consensus, Scaling, Cryptography, Grant Proposals). Larry_Bates:I searched “DeSci” on the forum, and got 7 results: Search results for ‘desci’ - Smart Contract Research ForumIn addiction to being a place where existing threads could be gathered, while we add content, a new DeSci category would be a useful place for us to house the Open Peer Review Project we’re running. We want authors and reviewers engaging with one another in public and creating threads on this forum would be a great place to do that. I’m not sure what category we could put these topics if not DeSci or Metascience.', 'In response to that, I am of the mind that the categories that have not had much activity should be removed.  There was an original intention of using a coverage map to diversify content across those categories, but that initiative sort of fell to the wayside.Concerning a general category to capture peer review, I would be wary of trying to have a catch-all for open peer review in that there are going to be different types of research being done; and using one forum to capture all those peer reviews will eventually become unnavigable.  Academic articles are usually categorized by subject matter or the tags which the author has chosen to describe the content of the research.  A “catch-all” peer-review/desci may work initially but it would quickly become inadequate to capture the nuances associated with different types of research.Out the gate, if research involves human subjects it will be completely different than something that is just looking at data that is gathered.  Further, is the peer-review taking place on Github or the forum?  If the peer-review is actually taking place on Github, it feels unnecessarily redundant to reproduce the entire process on the forum for the sake of perceived transparency when Github is already transparent.  Is the forum meant to capture the post-mortem on the peer review, or the peer review itself?  I’m not entirely sure how the open peer review would be captured in the forum based on what has been presented so far.Ultimately, we do have some content categorization issues that need to be addressed.  I’m not saying that we shouldn’t add DeSci altogether, but I do believe that if we add DeSci we may need to remove other categories and additionally add others such as DeFi or NFTs to accurately represent the forum content.  If categorization is not making indexing and access easier, it makes it more difficult.  I’m not sure the current categorization is the “best” it could be, and that is an entire conversation that seems to be more necessary as the organization evolves.I just don’t want to “add a DeSci” section when this is clearly an opportunity to reorganize categories around content with the understanding that the original content categories were themselves experimental.  I am not sure why the DeFi, NFT, and Web3 categories would not be implemented simultaneously if there is going to be one new category without addressing the old categories.Are we using categories to articulate areas which deserve attention, or using categories to articulate what SCRF is going to actively cover?  At this stage in the organization’s development, if we do not have a clear logic as to what our categories represent, then it will be difficult to actually come to a logical decision on whether a category should be added or not.  In other words, do our categories represent the industry or represent our coverage goals?That should be clear before we can really have a sincere and logical outcome to this debate.To be clear, I’m NOT against “adding a DeSci category”.  I am against “inconsistent usage of categorization”.One last thing:  I am not sure our categories reflect “academic” categorization and are more in line with industry categories.  It is not a huge problem to have industry-focused categories; however if our larger mission is to connect industry and academia then continuing to use industry jargon instead of intermixing academic jargon risks ostracizing academics.  I understand there is an impetus to experiment where there are perceived problems in academia, however completely disregarding academic jargon for industry jargon is not necessarily the best response.', 'This detail convinces me that a category might be the way to go. The highlighted topics could be tags within the category. I also agree that categories should be consistent, so I’m not convinced DeSci as a category fits in with the currently layout. I think it depends on what the goals of the forum are.', 'My understanding from all the replies above is that no one is against adding DeSci to the forum, however, thoughts differ on the where, when and how. zube.paul:In my mind, having a section on the forum creates the space for DeSci to mature as a field and application. I wouldn’t want our forum to only be a place where the most highly researched sub-fields of web3 get representation. Many of these emerging trends have need for a meeting space between “industry and academia.” I think by having a section on our forum for DeSci, we have an opportunity not to just be involved in exploring this emerging trend, but we also have an opportunity to shape and influence it. To the questions in @eleventh’s original post, we as a community would have the ability to set some types of guiding sections and constraints on content in the category.WhereShould it be a section, a category or a tag?Do we have any post on the forum that explains what makes up a section, a category or a tag? I know there is an attempt in the Terms Glossary and Content Tags post, but perhaps a more comprehensive explanation of how sections, categories and tags come to be. @zube.paul already talks extensively about why this organization/reorganization is needed at this time. I also noticed the words ‘section’ and ‘category’ being used interchangeably. More reason for clarity on what makes up a section, a category and a tag.WhenIs now the right time? Considering DeSci is still a budding area. And just as @Larry_Bates highlighted, we probably have other topics that need more attention. (That was some great data analysis by the way!)I agree that now could be the right time, in order for SCRF to have an opportunity to shape and direct discussions in the fledging area.HowThis should be easy to sort once the where and how are sorted. This means considering how DeSci content is presented on the forum. @jmcgirk made great suggestions like interviews, research summaries, and generally content that aligns with SCRF’s mission. @zube.paul has also highlighted more content types like project updates, ‘key problems’, etc.', ' eleventh:Two main drawbacks come to mind. One is that this is an application domain of web3, not a research domain area. Would adding a section on the forum that calls for non research related content too distracting from our core mission? Is there a way to provide clarity on what content is sought after on the forum to find a middle ground?@eleventh, please could you explain what you mean by an application domain and research domain. It seems a paradox to me in that doesn’t research lead to applications of the findings/solutions from the research and application leads to further research and on and on? This is why case study summaries are accepted on the forum?', 'As several people have said above, terms in the crypto/web3 “space” tend to be fragile, imprecise, impermanent, and often blatantly marketing-inspired, and “DeSci” doesn’t seem exempt from this. Further, even if “DeSci” were a term for the ages (not likely), slicing a forum into many root categories usually indicates a flawed taxonomy.After reading all the comments above, I think @eleventh’s original question (echoed by @Larry_Bates and others) captures it best for me: The thing presently called “DeSci” is the result of applying web3 techniques to scientific inquiry—which includes all the implied commitments to “greater fairness” in organization, governance, rewards, ownership, etc.Therefore, for this forum, “DeSci” is clearly not a root category. It makes better sense to create a DeSci tag, accumulate relevant content under that tag, and revisit the issue in a year or two.As has also been mentioned above, the deeper question is whether our existing root categories make sufficient sense. Do “Tooling and Languages” and “Mechanism Design” really exist at the same level as “Cryptography”? If our mission is to be a conduit for research between the academy and industry, then “Cryptography” makes as little sense as a category for “Computerization.”', 'Maybe we need an application category or something like that, I’ve also long advocated for a hardware category even though we don’t get much research in that area, I’ve been told that there’s plenty coming down the pipe. @cipherix what do you think? Should DeSci get a separate category?']"
                Research Summary and AMA with Giulia Fanti (Carnegie Mellon) on Analyzing Blockchain Incentive Mechanisms with Deep Reinforcement Learning              ,https://www.smartcontractresearch.org/t/research-summary-and-ama-with-giulia-fanti-carnegie-mellon-on-analyzing-blockchain-incentive-mechanisms-with-deep-reinforcement-learning/855,Mechanism Design and Game Theory,65,['https://twitter.com/giuliacfanti'],"['summary', 'discussion', 'governance', 'oracles', 'flash-loans', 'game-theory', 'mev', 'front-running']","['Dr. Giulia Fanti, an assistant professor of Electrical and Computer Engineering at Carnegie Mellon University, sat down with Chainlink Labs to discuss her co-authored paper “SquirRL: Automating Attack Discovery on Blockchain Incentive Mechanisms with Deep Reinforcement Learning”.She has also offered to answer questions about the paper here on the Smart Contract Research Forum for a limited time, so please don’t hesitate to ask!Description:This recording is the first episode of a new Chainlink Research Report series which features short presentations of exceptional working papers by blockchain scholars around the world.In this episode, Dr. Giulia Fanti, an assistant professor of Electrical and Computer Engineering at Carnegie Mellon, discusses her paper “SquirRL: Automating Attack Analysis on Blockchain Incentive Mechanisms with Deep Reinforcement Learning 10.” This paper uses deep reinforcement learning 2 to detect security flaws related to blockchain incentive mechanisms.Full Video: Take-aways:Incentive mechanisms play an essential role in permissionless blockchains.Designing incentive-compatible mechanisms, in which expression of true preferences are utility maximizing, is challenging.Little is currently known about properties of incentive mechanisms currently operating on large-scale blockchains, making it difficult to test their behavior.Deep reinforcement learning can identify new attack strategies and replicate known strategies such as selfish mining, helping to identify and improve upon weaknesses that were previously unclear.Dr. Giulia Fanti:Dr. Giulia Fanti is an assistant professor of Electrical and Computer Engineering at Carnegie Mellon University. She received her Ph.D. in EECS from U.C. Berkeley and her B.S. in ECE from Olin College of Engineering. She is also an academic partner with the Chainlink Labs research team and was previously awarded a Chainlink research grant 1 to further her work.Her research interests and publications focus on the algorithmic foundations of blockchains, distributed systems, privacy-preserving technologies, and machine learning. Giulia is also a fellow for the World Economic Forum’s Global Future Council on Cybersecurity, and has received a best paper award at ACM Sigmetrics and an NSF Graduate Research Fellowship.Some of Giulia’s work:SquirRL: Automating Attack Discovery on Blockchain Incentive Mechanisms with Deep Reinforcement Learning 3. C. Hou*, M.Zhou*, Y. Ji, P. Daian, F. Tramer, G. Fanti, A. Juels. [Replication Code on Github].The Effect of Network Topology on Credit Network Throughput 1. V. Sivaraman, W. Tang, S. B. Venkatakrishnan, G. Fanti, M. Alizadeh.Communication cost of consensus for nodes with limited memory. G. Fanti, N. Holden, Y. Peres, G. Ranade. [Replication Code on Github]Routing cryptocurrency with the spider network. V. Sivaraman, S. Bojja Venkatakrishnan, K. Ruan, P. Negi, L. Yang, R. Mittal, G. Fanti, M. Alizadeh. [Replication Code on Github].Design choices for central bank digital currency: Policy and technical considerations 1. S. Allen, S. Capkun, I. Eyal, G. Fanti, B. Ford, J. Grimmelmann, A. Juels, K. Kostiainen, S. Meiklejohn, A. Miller, E. Prasad, K. Wüst, and F. Zhang.On the Privacy Properties of GAN-generated samples. Z. Lin, V. Sekar, G. Fanti.Relevant links for Giulia:Email: gfanti@andrew.cmu.eduGoogle Scholar: \u202aGiulia Fanti\u202c - \u202aGoogle Scholar\u202c 2Twitter: https://twitter.com/giuliacfanti 2University Website:  https://www.andrew.cmu.edu/user/gfanti/ 1', 'I think this is a really cool implementation of RL, because RL is usually unstable and hard to train, but in this case, it is looking for strategies of exploiting profits, so stability would not be an issue.It also makes me really curious about why the Nash equilibrium for 3 players and above is for them to be honest.Could it be relying on the assumption that every player has about the same computing power? Because if so, does that apply to real life?', 'Thanks for the question–I want to stress that this is just a conjecture, so we don’t have any theoretical proof that honest behavior is a Nash Equilibrium for 3+ parties–it may not be the case! However, we did run some preliminary experiments a while back with varying number and fraction of hash power, and saw the parties appear to converge to honest mining in those settings too. So there is at least some empirical evidence to suggest that if the conjecture is true, it is not only true when all parties have the same hash power. It would be very interesting to see if this can be proved formally, even for e.g. only 3 parties.', 'Dr. Fanti, welcome to the forum! It’s a real treat having you here in the Forum. (and a big thank you to @jasonanastas for hosting this thread). You mentioned that this kind of selfish mining is very hard to detect; how do most platforms deal with this type of behavior?', 'This is indeed the textbook intersection between Deep Reinforcement learning based Data science and Blockchain via analysis of Blockchain incentive mechanisms!I would imagine being able to give constructive feedback to blockchain protocol providers be a priceless insight to further develop end user adoption to their Blockchain protocol. To imagine analysis of Block chain mechanisms vulnerability is already underway while the adoption of the Blockchain towards Web 3.0 is yet to be widespread!Indeed, I think the biggest hurdle for Blockchain companies is a well defined incentive mechanism to have end users go on chain. Competing against well established cloud services and its centralized ease of use, cross platform convenience and conventional paradigm of Web 2.0 is a difficult task to overcome. To this day, most industries are still trying to adapt to the Web 2.0 landscape!Other than traditional Paas platform A/B testing, Design of experiments, or adversarial modeling of contributing agents to improve platform usage, analysis of the Blockchain network architecture itself would definitely bring much to new domains of datascience.Out of the blue, I wonder how feasible is it to undergo such DRL analysis of block chain incentive mechanism? I would imagine setting up a Blockchain Paas to market others to go on chain be a difficult task  in and of itself.  How would the AIops data pipeline Blockchain architecture look like before even running such analysis?', 'Thank you for the question! Selfish mining actually is detectable in the real world by looking at the rates at which miners collect rewards, proportional to their hash power. So assuming we have a reasonable estimate for miners’ hash power, we should be able to get a pretty decent estimate as to whether a particular miner is engaging in selfish mining or not. Despite this, we have not seen substantial evidence of selfish mining in practice, to the best of my knowledge. (If you or anyone else knows of studies that have observed selfish mining in the real world, please share a link!) So in practice, many platforms don’t need to deal with this problem. One question that we wanted to understand in this work is why existing platforms don’t observe selfish mining; our results suggest that it may be because in the real world, there are multiple, competing parties, for whom it’s actually not profitable to deviate from the honest strategy (unlike in the two-party setting that is typically analyzed in academic literature).', 'Hi Tony, in general, DRL can be sensitive to hyperparameters and unstable, as other commenters have pointed out, which makes it potentially tricky to use in practice. It can also be computationally intensive. Our experiments were run on a departmental cluster, and each experiment could take as long as a few days to complete. We believe this is ok from a latency perspective (you only need to analyze your incentive mechanism once), but the associated computational costs could be prohibitive. I expect these costs will come down in the coming years, but these factors could certainly be a barrier to deployment.', 'Thank you so much for taking the time to share your experiment results, Dr. Fanti.If you don’t mind me asking, what do you think would happen if there are 3 players, but the hash powers are extremely tailed?I was thinking that suppose it was (almost 0, almost 50%, almost 50%), then that would be a setting close to 2 player game.Do you think selfish mining could happen in this setting? When we gradually move the distribution this way (+, -, -), do you think we can assume continuity on player behavior change, and observe a “tipping point” in which selfish mining behavior vanishes?I’m also curious about what impact do you anticipate different rewarding policies (e.g. policy gradient) would have on the outcome? How do the RLs play differently when they can think “long term”?', 'I know because of time limitations you had to concentrate on case studies 3 and 4 in this video. Case study 4, which combined a selfish mining attack with a voting attack, piqued my curiosity. The results slide showed that SquirRL (DRL framework you created to analyze incentive mechanisms in an automated way), started to show increased voting reward fraction vs. honest behavior somewhere between 0.198 and 0.231 attacker’s voting/mining power. And it looks like the voting reward fraction increased significantly from there as the attacker’s voting/mining power approached 0.330.Expanding on the idea of combining attack scenarios, to include Exploring the Attack Surface of Blockchain: A Systematic Overview Figure 25 2, and possible combinations; when n = 22 and r = 2, C(n,r) = 231. And of course that’s the simplest version, where r = 2; and n = known attack types, but doesn’t account for yet unknown attack types. Additionally, since I’m citing a single source for context purposes, it seems almost certain that n > 22. Bottom line, combining attacks leads to a lot of possibilities.Assuming a potential adversary would concentrate on trying to use a combination of attacks that would result in the highest rewards ($, voting power, etc.), is there an existing predictive model that estimates which combinations of these attacks are the most likely to occur? In other words, a way to prioritize the order in which SquirRL should simulate attack combinations in order to inform the community what to defend against? Or, are adversaries moving so quickly, that the community is left to simply catch-up and defend against already known attacks, much less possible combinations of attacks?I’m not sure how many attack combinations have been simulated with SquirRL to date, but it would be interesting to see if somewhere between 0.198 and 0.231, other attack combinations saw a significant rise in increased voting reward fraction or not. And coming at it from that perspective, that > 0.198 attacker’s voting/mining power, could be an indicator/threshold of a potential/impending attack. Because I’m not versed on the research in this space, is there an already existing indicator/threshold used and/or known < 0.330?Thank you for having the ingenious insight to use DRL instead of MDP to analyze this space and for taking the time to answer questions here!', 'Dr. Fanti, thanks very much for a fascinating presentation. I have a larger “overarching” question about the choice of consensus mechanisms.In your video exchange with Jason Anastasopoulos you note that permissionless blockchains need an incentive mechanism to make the system work, but if the mechanism is poorly designed it can bring the whole system crashing down.Many people have expressed concern that Proof of Work (PoW) greatly contributes to climate apocalypse, while Proof of Stake (PoS) contributes to the equally serious “inequality apocalypse” by insuring that the rich (those with the highest number of tokens) continue to get richer as the poor get poorer.If there were ever “poorly designed consensus mechanisms” that will guarantee that “the whole system comes crashing down,” we seem to have hit them on our first two tries. Why do you think a more fair incentive scheme is NOT being implemented in the “more fair” world of DeFi?Finally, what other options are open to humanity, technically speaking? Delegated Proof of Stake (DPoS), for example, seems to have intrinsic “fairness” advantages over PoS. Do you agree with that statement? Are there any other incentive mechanisms that researchers believe could result in a more stable economy over the long term?', ' Twan:If you don’t mind me asking, what do you think would happen if there are 3 players, but the hash powers are extremely tailed?I was thinking that suppose it was (almost 0, almost 50%, almost 50%), then that would be a setting close to 2 player game.Do you think selfish mining could happen in this setting? When we gradually move the distribution this way (+, -, -), do you think we can assume continuity on player behavior change, and observe a “tipping point” in which selfish mining behavior vanishes?We haven’t run that exact experiment, but in the 2-party setting (1 strategic, 1 honest), there is a known threshold of hash power below which selfish mining is no longer profitable, which can be computed (roughly 0.25, but depends on some parameter settings) (Sapirshtein, Sompolinsky, and Zohar, https://arxiv.org/pdf/1507.06183.pdf 1). For this reason, my guess is that in the setting you are describing, it would be similar to 2 strategic parties and 1 honest party with just a little bit of hash power. And that actually is an experiment we ran (https://arxiv.org/pdf/1912.01798.pdf 1, Fig. 7). Here we see that as the two strategic parties get closer and closer to 50-50 hash power, their advantage from selfish mining vanishes. And actually, they seem to not be stealing from each other, but from the 3rd honest party, while converging to an honest strategy only when the hash power is actually 50-50. So I believe SM continues to be profitable for 2 strategic players, but our experiments suggest this may not be the case for 3 players.', ' luc:Assuming a potential adversary would concentrate on trying to use a combination of attacks that would result in the highest rewards ($, voting power, etc.), is there an existing predictive model that estimates which combinations of these attacks are the most likely to occur? In other words, a way to prioritize the order in which SquirRL should simulate attack combinations in order to inform the community what to defend against?Thanks for the question–the short answer is no, I don’t know of any systematic way of ordering or choosing what combinations of attacks to try first. One possibility is to give the RL agent the option of using any known attack, and it will choose which ones to exploit. However, this can be difficult to encode in a compact action space if there are many different action spaces for different attacks, which may affect learning stability. It’s the kind of thing that should be possible, but may be tricky/sensitive to hyperparameters in practice.One broader point to consider is that many of the attacks in the figure you mentioned cannot be easily translated into a direct numeric reward that is comparable to the reward from other attacks (e.g., selfish mining and DDoS attacks have very different reward structures). In those cases, it doesn’t really make sense to use RL (or MDPs for that matter) to learn strategies combining the attacks. So there’s some domain knowledge needed to identify attacks with the same reward type and model the reward structure, so that an agent can meaningfully compare the reward(s) from different attacks. rlombreglia:If there were ever “poorly designed consensus mechanisms” that will guarantee that “the whole system comes crashing down,” we seem to have hit them on our first two tries. Why do you think a more fair incentive scheme is NOT being implemented in the “more fair” world of DeFi?Thanks for the question. DeFi applications are just that–applications. As you pointed out, they run on some underlying blockchain with its own (existing) consensus mechanism. This is mainly done for usability reasons, both for end users and for developers–if each DeFi app had its own blockchain with its own consensus and incentive mechanism, there would be much higher barriers to entry, and the security of each individual blockchain would be reduced due to less hash power/stake/resources backing it. So I think DeFi alone doesn’t really change anything with regards to this problem.Actually, the same techniques we used in SquirRL could probably be used to learn strategies for trading in DeFi apps to maximize profit. DeFi is of course not fair at all, and there has been a lot of work showing how strategic users can profit off other naive users (Chainlink researchers are doing a lot of work to try to address this problem). So in that sense, many DeFi apps may be unfair both at the application layer and at the consensus layer.', 'Thank you for contributing, Giulia!I noticed in the Q/A section of the webinar that you referenced how the flaws in current incentivize-based mechanisms could lead to collapse of the system attempting to be designed. I am curious as to whether or not there are any current examples of permissionless blockchains on a mass scale that you feel are showing signs of crashing because they lack the capability to fairly incentivize all miners.', 'Dr. Fanti, thanks for your response. My impression (as a non-specialist reader) is that the comparative “fairness” of decentralized finance was one of its main selling points to the public at large. Therefore your statement that “DeFi is of course not fair at all” makes the general public’s perception of it as “just another scam” seem credible.Speaking as a naive babe-in-the-woods, I thought that DeFi’s mathematicians were designing a transparent, close-to-incorruptible alternative to the world that existed circa ten years ago. What happened to that beautiful dream? Do the “researchers” you mention have any hope of resurrecting it, or have we all been out-finessed by the rich?', 'Thanks for your response.Aha! DRL could theoretically be used here, aka no need for actual human ordering of attack combinations, DRL should eventually lead to the best combo of attacks after several iterations. Understanding that this could theoretically be applied to your work, but may be tricky to apply in practice, is it something that you’ve considered and/or do you know of other researcher/s who might be attempting something like this?Another aha! I was trying to combine apples and oranges, thanks for setting me straight. Since you mention above, that using a DRL agent to choose which known attacks to combine may be too tricky at this point, do you have plans to model other attack combinations that are apple + apple or orange + orange with SquirRL, by choosing attack combinations outright, as you did with the selfish mining + voting case study? Are there specific attack combinations you’d like to simulate in the future?', 'Thank you so much for this fantastic presentation!  In response to the notion that it was not a Nash Equilibrium that was observed, conversely could it be inferred that too many cooperating selfish miners creates a Braess’ Paradox situation?  In this case, a group of selfish miners colluding against each other and against the group effectively seem to balance the system by effectively negating the nefarious activity?  I understand this may be extremely difficult to attempt to “prove”, but the collusion of 3 agents getting fewer benefits than 1-2 agents seems to have “some” effect.  While it may seem counter-intuitive, is it possible that nefarious attackers forcing a system into honesty could be a way to overshoot the Nash Equilibrium into Braess’ Paradox only for that state to look like a Nash Equilibrium when the nefarious actors cancel each other out?That is a hard question to ask in text format without getting it convoluted, so please let me know if I need to explain myself better!', '@Larry_Bates What might a Nash equilibrium look like emerging out of the current environment? What conditions might lead to Nash equilibrium based on where we are now?', 'Thank you for this question!  Considering that the Nash Equilibrium necessitates participating parties understanding the strategies of other participants; I would imagine in the current landscape that a Nash Equilibrium could only emerge at the organizational level, as the user base is not informed enough to understand the strategies of all participants within the ecosystem.One of the biggest hurdles to achieving a Nash Equilibrium currently is the notion of extractable value, in that extractable value does not always constitute a player incurring a loss but in many cases it does.  Any time participants in a system incur a loss while someone else extracts value without the loss being a calculated step in a larger play; this becomes a hurdle to achieving a Nash Equilibrium and is still a zero-sum game or potentially even a negative-sum game in some cases.All that to say, a Nash Equilibrium wouldn’t be absent of extractable value, but EV would not come at the loss of another player and value could only be extracted when the ecosystem produced excess or was in a situation where a product or service was valuable enough to swing the sum into a positive-sum game.In short, the service or widget being produced within a system has to have supply and demand both effectively outpacing natural entropy in order for the system to be efficient enough to not become a negative-sum game.  In other words, if the cost of running the ecosystem outpaces the profitability of the product or service being produced within the ecosystem, it becomes impossible to have a positive-sum game and in that context a Nash-Equilibrium could never actually be achieved anyway.So to bring this full circle, a Nash Equilibrium would not be necessarily free of malicious actors; conversely, the participants in the game would be so aware of the malicious actors’ intentions that they would be able to anticipate them and thus prevent the malicious actors from unexpectedly extracting value in a situation where another player incurs a loss.  One of the aspects of reaching that equilibrium is the players being able to anticipate the actions of the other players so much that the system ultimately finds balance because there are no surprises due to the players being informed.']"
                Discussion Post: What are the current major topics of discussion and research in MEV?              ,https://www.smartcontractresearch.org/t/discussion-post-what-are-the-current-major-topics-of-discussion-and-research-in-mev/560,Mechanism Design and Game Theory,89,[],"['front-running', 'defi', 'discussion', 'mev']","['CTA: “In these threads, we attempt to further the discussion of a key problem in this category and evolve our understanding of the domain space where research work has not yet answered the specific problem or question being considered. These posts are living documents, and it is our hope that the community will continue to contribute to their structure and content.”Related SCRF PostsPost Idea: MEV-SGX: A sealed bid MEV auction design 6Discussion Post: Flashboys to Flashbots 6Quantifying Blockchain Extractible Value: How Dark is the Forest 5Miner-Extractable Value, Oracle Frontrunning, and the Rise of Arbitrage Bots 6Core Research QuestionWhat are the current major topics of discussion and research in MEV?', 'MEV has been one of the most fascinating subjects to me as a relative newcomer to the cryptocurrency ecosystem. To me, as a ‘researcher’ the dark forest seems like a gigantic ecosystem with survival strategies pushing the system toward price equilibrium but as a retail investor I’ve been completely hosed by these wretched bots and it’s certainly given me pause before using DEXes and DeFi pools. What’s going on?', 'I was having a side conversation with @Eric regarding this. I consider myself to be pretty scientific process-oriented and it’s one of the things I enjoy about SCRF as well. As MEV gets brought up often, I’m interested in us as a community sorting through what are “settled” questions versus open questions regarding MEV.From my understanding so far, it appears that this is ultimately about gas price. Claim 1 is that higher gas prices are a generally negative thing. This seems to be the consensus of the space.Claim 2 is that MEV strongly contributes to higher gas prices. Maybe not its existence, but as a practice in in situ. There doesn’t seem to be as much consensus about this, so maybe this is still an open question. For example, the post Discussion Post: Flashboys to Flashbots 2 seems to give some voice to both sides of this issue. I think the open question here has to do with the manner of how MEV (or maybe if) contributes to gas price increases.Claim 3 is a little more generic. I think of it more of a “X solves the MEV problem.” type of claim structure. For example, “TEX solves the MEV problem” (Research Post:  TEX - A Securely Scalable Trustless Exchange 1. I think these are the types of general claims that we can be discussing and evaluating that help answer questions like what @jmcgirk asked above.Ultimately, I’m interested in this discussion because it seems like MEV occupies the mind of and drives the decision-making of several projects and initiatives. I would even make the cautious argument that changes like EIP 1559 could be analyzed as being a response to MEV concerns.', 'In terms of other works that can help provide context on MEV, Charlie Noyes’ work for Paradigm MEV and Me 3 is an excellent primer, and Philip Daian’s MEV… wat do? 1 is also a real thinker.A current topic of conversation is providing Chain Reorganization / Time Bandit Attacks as a Service.Charlie Noyes’ article I posted above describes Time Bandit Attacks as such:MEV Can Harm EthereumMEV inherently encourages consensus instability.Imagine there are two miners, Sam and Dan, who are paid a $100 reward for each block they find. Sam has found 3 blocks, the first of which contained our $10,000 Uniswap arbitrage.Now, Dan has a choice: he can either mine on top of Sam’s 3 blocks, or he can attempt to re-mine the first block in order to take the Uniswap arbitrage for himself. The $10,000 is much more lucrative than the $100 block reward, and Dan is more rational than honest, so he decides to re-mine the first block.While Dan’s at it, since the current longest chain is height 3, he also re-mines the second and third blocks (and captures any MEV that was in those, too). After the re-org, Dan owns the longest chain and he and Sam can progress from the third block.This is known as a “time-bandit” attack: if block rewards are small enough compared to MEV, it can be rational for miners to destabilize consensus.Our example was a two-party system. In the real multiplayer world, it is possible that every rational miner would attempt to re-org the third block and essentially halt progress. However, this could destroy the value of the miners’ hashrate investments. If we see this behavior at all, it will more likely be in the form of shorter, more frequent re-orgs that do not halt progress entirely.A conversation has been developing on twitter, stemming from discussions around searchers (MEV researchers/developers). A quick selection:https://twitter.com/mevintern/status/1413132589993447424?s=19 3@edgararout indicating they’ve begun the work, and links their repohttps://twitter.com/EdgarArout/status/1413322294345097218 2Philip Daian shares his thoughts:https://twitter.com/phildaian/status/1413480058677731330 1Its an enormously complicated issue.', '@jasonanastas - I’d be intrigued to read whatever you have to say about MEVs, and @Albert if you can think of some parallels with high frequency trading, and how that’s affected CEXes, I’d love to see what you have to say', 'Here is MEV extraction may through block re-org tool, someone name “0xbunnygirl 1” release this GitHub. I think the Slashing mechanism will not happen in the current Ethereum, but has the potential for slashing on Ethereum 2.0. It was discussed in MEV and Me “PoS-based blockchains can slash validators who attempt to re-org and thus make time-bandits significantly more costly, especially when combined with strong finality. However, with enough MEV the incentive to re-org could still be greater than the slashing penalty.”Also, it was discussed in Cosmos Slashing 1 when the slashing happens validator (usually the stakeholder) would cause penalizing to disincentivize any attributable action.If the re-org revenue is still greater than the penalty, I believe it would be a big problem for the future EIP 1559 upgrade. It needs to have a dynamic slashing penality solution.', 'Great observations about the dynamic slashing penalty solution. Is that approach being used by any project currently, or have you seen anyone describe what that might look like?', 'A similar approach of a dynamic slashing penalty is Polkadot’s NPoS GRANDPA and BABE equivocation slashing penalty that means the number of offenders(MEV) compared to the total number of validators in the network increases, the penalty charged for the incident also increases.', 'I have a theory that it’s only a matter of time until block templates are sold as NFTs.Time-bandits are not going to be incentivized if there exists established markets for block space, where the highest bidders can get their transactions prioritized – a tokenized mempool, if you will.There’s an argument to be made that the only reason that time bandit attacks are being considered now is that MEV relies on too much off-chain coordination between mining pools and traders. It’s all closed doors. Small to mid/scale miners and traders don’t have these relationships, so they need to rely on high fees and flashbots. Making MEV into a market of block templates will democratize order flow – whether embracing it is good or bad is to be determined.', 'Fun thought experiment: what if more than 33% of block producers want to engage in MEV?As a coalition, they can attempt to reorg any slashing penalty that is enacted at the protocol level. And depending on where the network sits in the CAP theorem, they would have the power to halt the network to make a political point ', 'Polkadot combines two consensuses as hybrid consensus: GRANDPA and BABE. The new transaction generates in the BABE consensus layer. If more than 33% of block producers want to engage in MEV with Polkadot. First, it will depend on the validators are block producer candidates or not. Second, if the block producers node is more than 2/3 of all the nodes can reach GRANDPA agreements. Following the longest fork chain and they continue MEV successful. I think it is possible to reorg any slashing penalty.', 'Generally, miners can extract additional profits from users by taking advantage of their ability to order transactions included in the new block, creating what is commonly known as miner-extractable value (MEV). Although it comes in different types, most forms of MEV seen today are taken not from miners themselves, but from third-party bots which are really far from the concept that miners actively take advantage of their ability mentioned above. Therefore, MEV can be extracted even when miners order transactions according to the highest gas prices, which is the most common way.Among various types of high-frequency trading tactics, arbitrage is one of the simplest and also most used strategies. Therefore, the most common form of MEV seen today is third-party bots performing arbitrage between two or more decentralized exchanges (DEXs). Because of the increasing popularity of decentralized finance, the arbitrage opportunities increased accordingly, which results in fiercer competition between arbitrage bots. What’s more, arbitrage usually happens between some very liquid decentralized exchanges, the time window is only a few microseconds. Therefore, a lot of bots by increasing transaction fees (gas) make it more possible for the miner to pick their transactions first.The result is raising the transaction fees for everyone else on the network, and transaction fees paid by the arbitrage bots, which is often a large percentage of the final profits generated from the arbitrage, go directly to the miners.', 'SCRF researcher @lnrdpss has started a fascinating discussion weighing the pros and cons of different methods of mitigating flash loan attacks 1. Flash loan attacks are common form of MEV. It’s an interesting read after this thread, and suggests that exchanges and other Dapps should combine a variety of strategies to prevent them.', 'I highly recommend anyone interest in MEV to read Analyzing and Preventing SandwichAttacks in Ethereum 4 by Patrick Züst.It provides a great overview of the evolution of MEV and the profitability of sandwich attacks:image791×584 85.9 KBThis attack alone has accumulated a profit of close to 190M USD. This makes MEV one of the fastest-growing “sectors” within crypto.', 'For those who prefer podcasts, there is a good episode from Uncommon core:  Interview with a Searcher - with MEV Senpai and Hasu 1.There is a fixation of the impact of MEV, specially on consensus. But for me whatever the content of the blocks, the miners/validators can always try to reorg. For instance last week we saw a lot of new NFT released and the race to be the one minting them. For any miner it was definitely tempting to reorg and try to mint most of the NFT by themself to sell them for a quick profit.', '+1 on that episode, really great convo on the impact of MEV.On your point on NFTs: has there been research/evidence of miners censoring NFT-generating transactions and minting their own instead? That would be a fascinating research field in the context of extraction attacks.', 'I am not aware of anyone suspecting miners to misbehave for NFT, yet. Which reminds me that there were a lot of suspicion around ICOs in 2017. I remember for instance one of them, the Status ICO, where a lot of people who got in were close to a mining pool. I just found this paper that look into that question and posit that a pool participated in the ICO for themselves.I would guess we should be able to see some weird behaviour when the NFT summer gets quieter. Trying to reorg the chain might not be worth the risks when you can make easier money just flipping some NFT.', 'A short update - FBI sets sights on crypto economy with arrest of former OpenSea staffer | Non-fungible tokens (NFTs) | The Guardian - an arrest of a former product manager at OpenSea for what was essentially frontrunning, he was buying up NFTs before he posted them to the front page of OpenSea. He’s charged with wire fraud and money laundering. @Larry_Bates any thoughts?', 'I had been an OpenSea skeptic for a while for a few reasons.  One of my biggest concerns with OpenSea was initially it seemed as if they were not doing any due diligence to prevent stolen art from being minted.  Second, they were effectively creating a market that encouraged theft and reselling of stolen items without much concern for the grieved party.  Now, we find out this individual was insider trading.  I sincerely doubt he was the only one engaging in such activities, and I think this type of individual will only serve to make laypeople less likely to get involved with NFTs at all, whether on OpenSea or another platform.', 'These talk/discussion from MEV day in Amsterdam could be of interest:']"
                Research Summary: DeFiRANGERS: Detecting Price Manipulation Attacks On DeFi Applications              ,https://www.smartcontractresearch.org/t/research-summary-defirangers-detecting-price-manipulation-attacks-on-defi-applications/1657,Mechanism Design and Game Theory,24,['http://arxiv.org/'],"['summary', 'defi', 'governance', 'discussion']","['TLDRThe rapid growth of the Decentralized Finance (DeFi) ecosystem has led to the emergence of new security issues.Existing smart contract vulnerability detection tools cannot detect certain DeFi attacks due to their lack of capacity to recover and understand high-level DeFi semantics.The researchers propose a platform-independent mechanism to recover high-level DeFi semantics and detect price manipulation attacks using patterns expressed in the recovered DeFi semantics.Core Research QuestionHow can price manipulation vulnerabilities and attacks on DeFi applications be detected?CitationWu, Siwei, et al. “DeFiRanger: Detecting Price Manipulation Attacks on DeFi Applications.” ArXiv:2104.15068 [Cs], Apr. 2021. arXiv.org, [2104.15068] DeFiRanger: Detecting Price Manipulation Attacks on DeFi Applications 7.BackgroundFront-running: A practice in which an attacker profits off privileged information about an upcoming transaction.Pump and Dump (P&D) Scams: Manipulating a stock’s value through misleading statements and selling once the price has risen following artificial inflation.Flash Loan: A form of non-collateral loan where an entire transaction is encapsulated in a single transaction.Price Manipulation Attack: An attack that exploits the vulnerabilities in the interface of an app to make a profit on loans and trades.Direct Price Manipulation Attack: An attack where an attacker directly manipulates a token’s price in the liquidity of an AMM by performing an unwanted trade in the DEX.Indirect Price Manipulation Attack: An attack where the token price of a vulnerable DeFi app whose price mechanism depends on real-time status is indirectly manipulated through making a trade on an AMM. If the price mechanism of a lending app is manipulable, a borrower may borrow more tokens than they are eligible to borrow.Decentralized Exchange (DEX): An exchange where users can trade different tokens in a decentralized way by interacting with smart contracts.Lending App: A DeFi app that allows users to lend their cryptocurrencies to borrowers. It relies on an AMM’s real-time reserve to price clients’ collaterals.Automated Market Maker (AMM): A decentralized exchange that provides cryptocurrency exchange service without an intermediary.Portfolio Management Application: A DeFi app that helps users invest their cryptocurrencies. It leverages AMM’s real-time quotation to price clients’ deposits.SummaryThe boom in decentralized finance has led to the emergence of DeFi related security issues such as front-running, P&D scams, and flash loan attacks.Code and logic vulnerabilities in DeFi apps contribute to the occurrence of many security incidents.Existing detection tools focus on code vulnerability, such as re-entrancy and integer overflow. However, they cannot be used to detect attacks caused by logic vulnerabilities due to the lack of capability to recover and understand high-level DeFi semantics.Two new types of attack, direct and indirect price manipulation, are emerging alongside the increasing popularity of two Defi apps, DEX and lending apps.Price manipulation emanates from the logic vulnerabilities of DeFi apps, thereby requiring an analysis of multiple smart contracts and an understanding of the high-level semantics of DeFi apps to be detected.The researchers propose a new approach in a tool called DeFiRANGER to detect price manipulation attacks.MethodUsing a full Ethereum node with a modified Geth client, the researchers collect internal transaction metadata, EVM depth, and execution order.After collecting Ethereum raw transactions, a cash flow tree (CFT) was built to convert raw transactions to token transfers, laying a foundation to lift DeFi semantics from.Based on the CFT, the researchers lifted DeFi semantics and used predefined patterns expressed in DeFi actions to detect price manipulation attacks.ResultsDeFiRANGER detected 524 attacks from 350,823,625 transactions. Of the 524 attacks detected, 432 were true positives, while the remainder consisted of 51 false-positive arbitrage transactions and 31 transactions with misidentified DeFi actions.Insufficient access control of a smart contract’s API makes vulnerable DeFi apps susceptible to direct price manipulation attacks.Four DeFi apps were found susceptible to direct price manipulation attacks. They are Loopring, Dracula, Seal Finance, and Metronome.Four vulnerable functions due to non-enforcement of access control, namely, sellTokenForLRC, drain, breed and closeAuction, were detected in the apps. An attacker could exploit the first three to sell cryptocurrencies in the liquidity pool of an AMM while the last one (closeAuction) directly deposits ether into Metronome’s AMM pool.The root cause of indirect price manipulation attacks in DeFi apps was insecure price dependency. Portfolio management and lending apps are susceptible to indirect price manipulation.Attackers now use a clean attack strategy to attack vulnerable apps and launder profits making attack traceability more difficult.Security incidents directly impact the market value of vulnerable DeFi apps.Discussion and Key TakeawaysDeFiRANGER cannot identify some DeFi actions, such as loan-related actions.DeFiRANGER can only identify two types of price manipulation attacks. However, with the recovered SeFi semantics, the tool has the potential to detect more attacks.Implications and Follow-upsThis paper is the first structured work to systematically detect price manipulation attacks in DeFi apps.The biggest challenges to detecting price manipulation attacks are complicated interactions between multiple smart contracts in a transaction and the semantic gap between raw transactions in Ethereum and high-level DeFi semantics in DeFi apps.DeFiRANGER misidentifies actions and gives false positives in some cases, especially in exchanges with more than two types of token.Improving the security of the DeFi ecosystem by enhancing DeFiRANGER to detect more DeFi actions and attack types could be the focus of future works.Can DeFiRANGER be developed to proactively detect vulnerabilities in smart contracts instead of detecting attacks only?ApplicabilityWhen designing DeFi apps, developers should enforce sufficient access control on the functions that may change the AMM’s reserves.DeFiRANGER can be used by DeFi apps to periodically monitor their vulnerabilities and detect price manipulation attacks.', 'Hi, Tolu beautiful job there!Let me quickly ask some questions. Did the researchers particularly mention what sets DeFiRANGERS apart from pre-existing tools for detecting attacks on DeFi platforms?Secondly, are there hints on method(s) which could be deployed for the proactive detection of vulnerabilities in DeFi smart contracts? Because realistically, prevention should be the ultimate target.', 'Thank you @UlyssesYes, the researchers explained how DeFiRangers is different from every other tool for detecting attacks on DeFi platforms. Other tools are only able to detect attacks caused as a result of code vulnerabilities due to their inability to understand high-level DeFi semantics. On the other hand, DeFiRangers is created specifically with the ability to recover and understand high-level DeFi semantics.Also, I agree with you that prevention should be the ultimate target. Numerous research has been conducted on the proactive detection of vulnerabilities in DeFi smart contracts. In Making Smart Contracts Smarter, the researchers examined Ethereum smart contract vulnerabilities. and proposed some improvements to the operational semantics of Ethereum. +However, some of the tools that have been proposed have several issues that make it difficult to achieve optimal detection. This paper and it’s supplementary material examine several tools that have been developed and highlight their problems. It is also important to note that most of the tools created to detect vulnerabilities focus on code vulnerabilities and not logic vulnerabilities. One of the future works for DeFiRangers could be improving the tool to proactively detect vulnerabilities.', '@Tolulope, thanks for the extra sources.As expected, every new technology comes with it challenges that patience and further research solves.', 'The rise in DeFi has made rise to all this Tolulope:Front-running: A practice in which an attacker profits off privileged information about an upcoming transaction.Pump and Dump (P&D) Scams: Manipulating a stock’s value through misleading statements and selling once the price has risen following artificial inflation.Flash Loan: A form of non-collateral loan where an entire transaction is encapsulated in a single transaction.Price Manipulation Attack: An attack that exploits the vulnerabilities in the interface of an app to make a profit on loans and trades.Direct Price Manipulation Attack: An attack where an attacker directly manipulates a token’s price in the liquidity of an AMM by performing an unwanted trade in the DEX.Indirect Price Manipulation Attack: An attack where the token price of a vulnerable DeFi app whose price mechanism depends on real-time status is indirectly manipulated through making a trade on an AMM. If the price mechanism of a lending app is manipulable, a borrower may borrow more tokens than they are eligible to borrow.By exploiting the transparency of the Ethereum blockchain, a large group of DeFi-related checking devices should be made accessible to general society to communicate with more noteworthy trust in monetary applications.Checking network wellbeing: For individual clientsOn loaning stages, client stores are in danger of liquidation once the guarantee proportions dip under specific limits because of cost vacillations. When the insurance falls, the rate drops, and the vault becomes open for liquidation. Clients can follow the insurance and choose to take care of borrowers or add stores to keep the vault protected and out of liquidation. Observing apparatuses will assume a significant part in sure client association with loaning conventions in the event that these devices give dependable ongoing and solid cost takes care of for various resources to make clients aware of make a move ahead of time.One more proportion of observing on loaning stages is the use proportion of resource liquidity. The usage proportion is determined by separating the aggregate sum of obligations in view of the size of the proposal in the liquidity pool. On the off chance that all the cash in the complex is acquired and not paid, the use rate is practically 100 percent.Smart contract reviews/auditsAuditing can recognize potential contract weaknesses through severe testing and white hat penetration before protocol or feature release. As the DeFi conventions fill in number, intricacy, and interconnectedness, more vulnerabilities and security dangers are probably going to happen.', 'Thank you @kingdamiethAuditing is definitely helpful to detect vulnerabilities and attacks.', 'Good use and understanding of TWAP can prevent most price attacks. This paper does not mention it. Using something external to the EVM for attack detection typically ends up being a centralization/attack vector. (Fully transparent and deterministic mechanisms, be them on chain or off, are gamble.)I think all of this falls under MEV. (Miner/Maximal Extractable Value) Detecting price attacks is easy. Preventing them given a faulty smart contract is impossible / indeterminable since the agent doing the prevention can be included in the attacker’s plan.TWAP Oracles vs. Chainlink Price Feeds: A Comparative Analysis 2 Tolulope:Can DeFiRANGER be developed to proactively detect vulnerabilities in smart contracts instead of detecting attacks only?There are a range of tools currently used to this end by most smart contract auditors. They fall into different categories. Slyther, a static analysis framework is a popular and free one that is usually a default in smart contract developers’ environment. Other, more powerful, commercial tools: Mythx, Certora 1.For me, having not read the paper, there’s alot of distance between detecting unusual price changes (attack) and smart contract vulnerabilities. As such, I think it’s unlikely and definatelly innoportune given the existing advances and research already present in the space.P.S.I was just looking for a place to write my first comment on this forum.', 'I’m glad you found a place to write your first comment!I’m hoping you can go into more depth about your observation that TWAP can prevent most price attacks. The article you linked to also indicates there are many weaknesses to TWAP as a solution as well. Even with those weaknesses, would you see TWAP being a better solution to the same problem that DeFiRANGERS is trying to solve in a less centralized way?', ' parseb:Good use and understanding of TWAP can prevent most price attacks. This paper does not mention it. Using something external to the EVM for attack detection typically ends up being a centralization/attack vector. (Fully transparent and deterministic mechanisms, be them on chain or off, are gamble.)I agree with you that TWAP can prevent price attacks, I think it is possible that the authors did not mention this due to the ‘easy’ manipulation of TWAP. Also, I think another thing to point out is that the researchers are focused on attacks that arise from the smart contract logic and not the code. Logic vulnerabilities are seemingly not as commonly researched as the code vulnerabilities and have higher chances of passing DeFi audits based on existing tools that tackle code-based attacks like re-entrancy and integer overflow. It seems TWAP may be unable to prevent attacks arising from the smart contract logic.To contextualise this, take the Harvest 1 case, for instance, the arbitrage opportunity used by the hacker was not based on the code or detected in the security audit, rather, it was based on the protocol infrastructure. So, it seems there is still more research to be done on tools that can detect and prevent attacks that are based on the smart contract logic as opposed to the code.Also, Promutator 1 is a great tool that examines price oracle susceptibility (but based on the code vulnerabilities in the smart contract).Here are some papers that I found helpful when writing this  summary:A Survey of DeFi Security: Challenges and OpportunitiesDeFi Security Audit: How to Prevent your Defi Project from Hacking? - this one examines the code and logic vulnerabilities in the smart contracts of DeFi apps.I just read this paper 1 but it explains how TWAP can be manipulated and the cost.P.S: Super excited to host your first comment haha', ' Tolulope:tools that can detect and prevent attacks that are based on the smart contract logic as opposed to the code Tolulope:the researchers are focused on attacks that arise from the smart contract logic and not the codeI fail to understand the distinction between code and logic. The code is a materialized sequence of logical instructions. If the code (smart contract) is vulnerable to a price attack it is so as a result of a failure in its logic. Its authors for example failed to account for a particular sequence of events. That is, they used unsound assumption.But here I am getting ahead of myself because the reality is that a price attack is just an unwanted or wanted change in price at an inconvenient or perfect time because ‘the attacker’ is nothing more than a user that executes that which was made possible by the price attacked protocol.The job of the smart contract developer is to ensure that the desirable is possible and, ideally, the undesirable impossible. Preventing price attacks is a matter of risk management in the specific use case. The only way to prevent price attacks is to define them as such: if my source says to me that the price is 1% different that what I did on average with the past 5 transactions then I refuse to execute this function. And that might just work for the top 10 most traded pairs but will brick your protocol relative to low volume or infrequent/niche tokens. So you can either limit what your protocol can do (fail to serve on volatility) or who it can serve (whitelisting collateral tokens etc.) or you accept that price is a number that is produced as a result of “this authority says so” or “this immutable logic living at this EVM memmory address determines it so”. Either way, it’s a matter of risk management and trust assumptions.Formal verification (certora) is to the best of my knowledge the only way to prove that all undesirable states are unreachable. But that in turn require humans to logically map the totality of the undesirable. zube.paul:more depth about your observation that TWAP can prevent most price attacks.TWAP is perfectly suitable for more meaningful applications where you can always ensure the attack is irrationally expensive. It’s not fine if you’re trying to prevent an intra-second arbitrage involving a low volume token. But this is a limited, niche activity in the grand scheme of things because in truth there’s nothing but surplus capital at stake in such DeFi. If this were the price of potatoes, the consumer as well as crop insurance companies would be totally fine using a 1 week fully traceable TWAP price. zube.paul:would you see TWAP being a better solution to the same problem that DeFiRANGERS is trying to solve in a less centralized way?No. parseb:having not read the paperI just fail at seeing how anything mempool or external to the EVM, that’s also rational can detect and mitigate price attacks consistently in the presence of faulty logic. That’s implying there’s a superpower that can be used only by the good guys to cost efficiently cancel the bad guys with no hope for the bad guys to work their way around it. I think it’s possible on chain with risk management, but not reactive and from outside looking in. In my world, risk management is the only way to prevent price attacks in decentralized, deterministic, permissions protocols whereby a successful price attack is an unfortunate event in a calamitous state where it’s unreasonable to expect for the protocol to perform.', 'Hello Tolu,Thanks for the wonderful summary of the subject. I understand that price manipulation attacks are a serious concern in DeFi because they impact on trust. I believe there should be a further distinction made between the direct beneficiaries of price manipulation and the threat agents. This is because, given the DeFiRANGER detection analysis in the research, there were some false positives. Also, the need to enforce access control on API calls as a way to reduce the vulnerability of DeFi AppsIn the research, it was stated that DeFiRANGER can only identify two types of price manipulation. Is there a way to categorize the potential of other detection mechanisms for DeFiRANGER?', 'Thank you for your comment. I agree with you that a distinction should be drawn between direct beneficiaries of price manipulation and threat agents but I would like to know your thought process. Why do you think this distinction is necessary?Also, yes! access control is absolutely necessary as a mitigating measure to reduce vulnerabilities of DAPPs. I think the categorisation of other detection mechanisms for DeFiRanger could form a part of future work for the tool.', 'I appreciate your feedback.  The reason for the distinction is to narrow down the actual threat actors in the chain.  For instance, a couple of malicious actors focus on an individual on DApp vs an organization focusing on the entire network.  In either case loss of funds and reputations.  I think this will further create an avenue to holistically classify the intents of the threat agents. Whether for undue benefits or disruption.', 'Thank you for the response. I think I greatly agree with you.']"
                Building Systems of Trustless Science              ,https://www.smartcontractresearch.org/t/building-systems-of-trustless-science/1531,Governance and Coordination,44,[],"['summary', 'discussion', 'governance', 'dao']","['lbry://TrustlessScience#ddef381812a891d625d9d28b80b33bf43eddd043Medium – 9 May 22Building Systems of Trustless Science 7lbry://TrustlessScience#ddef381812a891d625d9d28b80b33bf43eddd043Reading time: 11 min readThe ProblemThe system of science has reached a bottleneck. The structures that emerged hundreds of years ago are failing. Their mechanisms, manifested in a world of third party arbiters, are robbing humanity of critical advancements in medicine, energy production, materials, exploration, mathematics, and countless other areas of scientific development.Gates to knowledge, resources, and tools prevent some of humanity’s brightest and most motivated minds from utilizing the process of science to solve our most urgent needs or to discover the next groundbreaking truth of the universe.Without the ability to participate in and understand the scientific process, the general public is taught to view it with distrust and apathy. At best we ignore the outcomes of science. At worst we actively reject its results.As humanity stares down increasing existential threats, some of which are already coming to fruition, the only system we rely on for impartial discovery and production is losing its credibility and functionality.Now, more than ever, humanity needs science. And so too does science need us.EmergenceThe contemporary system of science emerged as a result of converging technological, political, and economic advancements of the 16th century. In short, those with money and privilege had time to access, consolidate, and profit from novel technology used to produce, share, and consume knowledge. Producers of knowledge profited by industrializing science through societies, institutions, and universities. Aggregators of knowledge profited by industrializing the publication, credit, and reputation mechanisms through journals and impact scores. Consumers of knowledge profited by owning and controlling the knowledge-to-technology translation pipeline.Closed and permissioned processes of knowledge production, consumption, and translation created a system that incentivizes consolidated control of its mechanisms. To this day, and despite several significant advancements in knowledge-sharing technology, these arbiters of trust, capital, power, and culture choose who has access to the resources, tools, and outcomes of science.On top of it all, the general public has never had a choice but to live with the outcomes of science, over which they had no control. While still a nascent political truism during the emergence of the system of science, it is obvious today that excluding forced consumers from the process that creates the necessary product leads to untenable levels of distrust and unrest.Contemporary science is haunted by its legacy as an emergent system of the 16th century.IntentionThe technological innovations at hand today offer the opportunity to expand the scale of science in ways not seen since its emergence. During this process, many, if not all of the mechanisms that comprise the system of science can be renovated or entirely rebuilt. Instead of emerging as a product of the times, we can build the system of science to actively engage and empower all stakeholders, producers, and consumers within the system.We can root science in mechanisms that incentivize public engagement instead of public exclusion, that encourage knowledge sharing instead of knowledge hoarding, that support independent production and experimentation instead of conformity, that credit collaboration instead of siloing, that reward quality and replicability instead of quantity;Technological, economic, and political advancements are once again converging. This time however, we have the opportunity to control the outcomes. This time we can move with intention.The MechanismsThere are many mechanisms in the system of science. Most are interwoven with one another. Several have become cornerstones of entire structures of the system.This intersectionality means that almost every mechanism of the system will need to change simultaneously to truly crack the bottleneck threatening science.The changes will not come from one source. Many projects must design different iterations of alternative mechanisms. The projects must work together to design the edges by which the iterations interact. The ideal outcome is a permissionless system where new iterations are seamlessly injected into the larger network while failing iterations drop away without causing disruption.Below are a list of mechanisms and some questions that might help inform the intention of architects.Data and Knowledge CreationWho has the tools to produce data and knowledge?What assets of data and knowledge are made accessible?Who owns the data and the knowledge?How is the data and knowledge referenced in future research?At what point in the creation process is data and knowledge shared?ReplicabilityHow easily can data and knowledge be reproduced?Who has the tools to reproduce data and knowledge?What role does replication play in the legacy system?What role should replication play in the digital system?ImpactWhat defines the impact of a discovery?Who decides the impact of a discovery?What role does impact play in the discovery’s standing?How is impact related to the other mechanisms?CreditWho gets credit for the work required to produce data and knowledge?What right or privilege does credit grant the owner?Who disseminates credit?How is credit disseminated?How can the credit mechanism encourage collaboration?How does credit relate to income or funding?Can credit for foundational discoveries produce income if that foundational discovery is used to produce a translated product?How is credit related to the other mechanisms?AccreditationWhat does expertise mean?Who defines expertise?Does the legacy model exclude populations?Does the legacy model limit the number of positions of employment in pure research?Can expertise be granted on a network level?What rights or privileges does accreditation grant a participant?Can network accreditation translate to steady reliable funding or income?How does accreditation relate to the other mechanisms?ReputationWhat defines the reputation of a participant?How can reputation be defined to encourage collaboration?How does reputation relate to network rights and privileges?How does reputation relate to funding or income?How is reputation related to the other mechanisms?FundingWhere do the funds for research come from?How are funds received?Who decides what gets funded?Who are the benefactors?Who covers the “loss” for non-translatable outcomes?Can everyone be made a benefactor of research?When are funds distributed to a researcher?How is continued funding achieved?How is funding related to the other mechanisms?PublicationWhat is the purpose of formal publication?In a system with open ledgers, economics, and incentives why is formal publication necessary?What can replace the modern publication model to ensure everyone can share their research and receive the proper credit and accolade?Can a publication mechanism incentivize interdisciplinary discussion and collaboration?What does a “publication” look like in a trustless digital world?ReviewWhat is the purpose of peer review?In a system where anyone can freely produce, publish, access, and replicate data and knowledge, is formal review necessary?What can replace the modern review model to ensure data and knowledge production can be trusted and is quality?IPWhat is the purpose of IP?In a system where credit is guaranteed and verifiable at publication, and where remuneration can occur automatically and upon use of data and knowledge, is IP necessary?What can replace the modern IP model to ensure research is monetarily rewarded?Access to Physical ResourcesWhat are the physical resources required for research?Who has access to them?How can access to these tools be granted to more people?A simplified example incorporating a few mechanismsImagine a scenario in which a system participant creates and publishes foundational data, methodology, and other research assets to an immutable ledger that forever recognizes the research “object” as belonging to that participant. Anyone can access the object, reproduce the data, and build on the knowledge. Let’s say four separate participants access the research object and contribute toward translating it into a product on the market. Each contributor posts their outcomes to the network creating a “stack” of research objects. When brought to market, credit and profit is shared among all four contributors without the need for legal intermediation. Profits continue to trickle to all contributors of the stack so long as the product is “syndicated”. If necessary, all participants can point to their contribution and timestamp on the ledger to prove their role in the creation of the translated product.Now imagine that each of the four contributors receives verifiable and non-transferrable credit based on their contribution to the discovery and production of the translated product. This credit might give the holder specific privileges, responsibilities, and opportunities within and without the network in which they participated. Perhaps they can elevate the voices of new network participants, perhaps they have a louder voice in the funding mechanism, perhaps they are given resource priority, perhaps they are even given network and legally recognized accreditation which itself comes with specific benefits.Continuing the example, each contribution to the knowledge stack that resulted in a translated product continues to exist as its own object. This means that the foundational object can be referenced again to produce a second knowledge stack that might produce a second translated product. The impact of that original research object could be determined by the number of times it is used in a knowledge stack. Each time it is used or results in profit, it generates income for the contributors that produced it. Imagine the impact the theory of general relativity would have today, and the royalties it would produce for the contributors to its stack and original research objects.This is a brief example of how some aspects of an open, trustless, permissionless, and digital system of science might operate. It is up to the architects to design as many networks as possible, and for those networks to interact, collaborate, and compete to form a marketplace of system iterations in which participants can choose to engage.The Effects of an Open System of ScienceInclusivityThe world was a certain way when the legacy system of science emerged. Now we have the opportunity to rebuild the system with that legacy in mind. Architects can choose to proactively seek diverse perspectives when designing their mechanisms. A permissionless system guarantees that any new iteration from any perspective is given a chance in the marketplace of mechanisms.An Economy of ScienceCryptocurrency generating distributed ledgers can be viewed as open economic networks (OEN). The network mints a currency based on variables defined in software. The currency is distributed to network participants based on rules defined in the software. This is similar to how a central bank might mint and distribute currency. The difference is in the open and permissionless nature of the predictable software which governs cryptocurrency generating distributed ledgers. Anyone can make an OEN at any time and the marketplace of currencies will decide which networks are more valuable at a given point in time.A cryptocurrency generating distributed ledger that is rooted in a system of science would essentially create an OEN rooted in scientific outcomes. The ledger could consist of pointers, research objects, credits, accreditation, and other traceable objects. The currency would be generated and distributed based on instruments of science production. Contrast this with a currency based on instruments of debt creation.For example, the software might mint and distribute currency directly to participants that produce an impactful research object, contribute computational resources to discovery, become accredited on approved networks, or contribute quality improvements as defined by a weighted web of trusted peers and network participants.Engagement, Education, and ParticipationMany of the mechanisms of science control the potential engagement, education, and participation of the general public. They can be designed to incentivize proactive and synergistic relationships between knowledge producers, aggregators, and consumers.Ultimately, an actively engaged, educated, and participating general population is more likely to appreciate and accept the outcomes of science. The technology at hand offers us the opportunity to build economies and societies based on science-literacy and science-participacy.For example, imagine there is an OEN that funds science production and consits of both scientists and a general population. The OEN distributes currency to researchers based on the evenly weighted decision of all network participants. Let’s say a scientist wants to receive funding from the OEN. To receive funding, the scientist must convince the general population of the network that their research is worthwhile and network-value-aligned. To ensure network-value-alignment, the general population must learn about the research and understand its goals and methodology. Both parties are thereby indirectly incentivized to interact, educate, and engage. Contrast this with the current tax/corporate model of funding where a researcher convinces only other researchers or administrators that their science is valuable.Now imagine that the research itself requires resources that are more efficiently obtained through distributed processes. Distributed computing, crowd-sourcing data, and monetary funding are three examples. The general population of the network can be incentivized to directly participate in the creation of a research object. The OEN can distribute currency directly to resource contributors. Resource contributors can also receive credit and accolades for their contributions. In this scenario as well, the general public will seek education from the researcher while the researcher seeks the general public’s engagement and participation; ignoring some interesting potential distribution models, the better a researcher can convince contributors of the value of their research, the more resources will be allocated to that research.Speed and AnonymityMore avenues of income, publication, respect, and participation means more minds will enter the field of science. More jobs besides the limited tenured position will emerge and enable stable income. More edge-case experiments and discoveries will move forward. More young researchers will use their energy for exploration of high-risk questions. Anonymous contributors will build without fear of persecution from overreaching authority or arbiters of culture.Furthermore, if science production is the root of currency creation and distribution, individuals seeking solely profit will focus on producing and translating knowledge. Greed can be utilized to move science forward.Some networks might tune their mechanisms in ways that incentivizes collaboration, collaboration being one of the practices that greatly increases productivity and efficiency.An appropriately tuned network might also encourage single experiment publishing, incentivizing researchers to open up their research after each step. The network might encourage, acknowledge, and value negative outcomes. It might create replication that can occur on the fly.Foundational Research is ValuableIn a network iteration of research objects, knowledge stacks, and syndicated funding, the original research object of multiple knowledge stacks becomes a very valuable outcome of the system. If I create the foundational research that is used to create 50 translated products, I receive passive income and credit from each of those products.A Marketplace of NetworksA significant problem of the legacy system of science stems from its single iteration. It emerged, it progressed, it is reaching a logical conclusion, and now there is no competing system to save it from itself.The open, trustless, permissionless, and digital nature of the technology on which an open system of science is forming demands multiple iterations of scientific processes. If one OEN fails, participants can move to another. Participants can choose to engage with multiple OENs simultaneously. OENs can interact, learn from, and compete with one another to build the most efficient mechanisms. Different OENs can develop different value-sets, and each value-set can compete and shift in societal-value over time based on social, political, and economic context. All of this without disrupting the continued flow of knowledge creation and translation.In a marketplace of interoperable currencies defined by different mechanistic iterations, stability reigns.Why Choose to Work Toward Trustless ScienceAn open system of science is not a new idea or movement. Over the past several decades, open access, knowledge, data, education, and publication movements have attempted to build new ways to produce and translate knowledge. Many have succeeded in several key arenas. Similar to the precursors of Bitcoin, however, their ongoing struggles stem largely due to the limitations of the technology they have at hand. Distributed ledger technology offers new tools with which the same ideas can be successfully implemented along-side new visions of a more open, inclusive, and participation-based system of science.Whether it was through securing a ledger by finding prime numbers (Primecoin), building economic networks rooted in scientific outcomes (Gridcoin), incentivizing distributed computing networks (Gridcoin, Curecoin, Foldingcoin), raising funds for research (Pinkcoin, Einsteinium), or building distributed and cloud computing marketplaces (Golem, SPARC, iExec), people working with distributed ledger technology have been experimenting with these tools since the beginning of crypto, some with measured success.While building trustless science you will find yourself next to contributors who have dedicated their careers to open science. You will find contributors who have dedicated their time to advancing trustless technology. You will find some of the most engaging discussions centered around solving some of the most fulfilling challenges, challenges that offer the best opportunities to build a world where the production and translation of knowledge belongs to and is accessible by everyone.Continue the conversation: jringo@protonmail.com', '@jringo thanks for bringing this into forum! What do you make of the argument that the current scientific funding model seems to work pretty well, despite its flaws? It seems like for engineering, there’s a lot of good research going on in industry and academia, and the same is probably true for other fields. Does the current system really block much useful research? I’m definitely persuaded by the human factor, I think STEM PhDs get a raw deal, but I’m not convinced that society is missing out on a bunch of breakthroughs… there’s a pretty compelling argument for there being too much scientific publication (c.f. Too much academic research is being published 2)', 'It’s a very complex problem, but neither argument stands up to scrutiny, as outlined in the writing. Yes the current system blocks an unimaginable amount of research. Underappreciated negative outcomes alone should force skepticism. If not that, then the literal restriction on pure research positions in the system.As mentioned in the write-up, quality and replicability are far more important than quantity. A system that rewards quantity is not a good system. That’s the point = )I have a fun metaphor, and I apologize in advance for its language.I think the outhouse works pretty well, despite its flaws. Plenty of shit gets deposited in it, maybe too much shit. Still, solid technology. Works fine. Keep it.', 'This is pretty sound in your discussion. As we look at our science today its commercialized and  it is mostly used for profit. There have been some innovations that have been for the betterment of humanity. But time and time again we see it get bogged down as we compete with one another to not let the other one advance. Ranging from pharmaceutical companies to advances in biology.The systems your mentioning is quite fascinating and which I can see this creating more freedom for not only scientist but innovators as well that can share there information without an institution trying to stop them. That it could very well possible speed up the rate in which we discover things and solve complex problems.', 'I appreciate the metaphor, and for what it’s worth the modern research university emerged in the 19th Century. Even then there were criticisms about the deleterious effects of industry on ‘pure learning.’  I appreciate your commitment to DeSci and am largely on your side. In this forum, we rely on evidence and prior scholarship. So I think it would really make your post more compelling if you could cite a few studies. I’m sure they exist. Could you point to evidence suggesting that universities and scientific publications are the bottleneck for other research?', 'Happy to be wrong (and probably am), but my understanding is that rich and powerful people and societies started universities and “science” through land grants and whatnot in the early 1000s. Knowledge production and translation got decentralized in the 1500/1600s with the convergence of societal advancements. Then decentralized and consolidated again a couple more times to where we are now, at another inflection point. The 1800s could be a good target of discussion around inflection, but industrialization in the 1800s was not the same as the 1500s – physical production vs knowledge production. That cycle of science was still informed by its past which was based on wealthy/powerful arbiters.Separating into “contemporary” or “modern” seems to me like ignoring the past, which is similar to accepting a non-replicated result from the early 1900s only to find out 100 years later that the methodology was flawed and everything built on top of it is wrong.Universities and publications are not the bottlenecks of research. They are parts of a complex systems failure. This is similar to when a financial or political system collapses. The premise is that every mechanism of knowledge production and translation is reaching a logical conclusion, weakening the structures, and collapsing the system. As an example: if we recall the 2008 financial collapse, MBS was a mechanism that was abused and reached a logical conclusion, collapsing a structure (or two).Regardless, the point of the post is not to argue, prove, or convince of a largely accepted problem: I would challenge someone to find a researcher that loves spending more than half their time searching for funding, for example; or find a doctor that loves explaining to a patient that no, mRNA vaccines do not change your DNA; or find a meteorologist that loves explaining that they cannot say it will be “rain” or “sun” with confidence because their work is based on probabilities; or find a smoker that believed the research funded by the tobacco industry, or funded by the leaded gasoline industry… etc.I understand the goal of including academic-based evidence, and such evidence exists I’m sure… people have been opening science for decades, but what’s the point of finding a study to prove the earth is round?The point of this writing is to help guide people designing the new systems of a digital, trustless reality, and given their choices, to encourage them to help with redesigning science. Secondary, the point is to highlight the mechanisms of science and the questions surrounding them. Tertiary, to discuss answers to the questions and possible implementations.Regarding citations on this forum, I was encouraged to post a random comment about 99% of DAOs being likely to fail. This is a much more cogent post about something much more important than the adoption cycles of DLT. So color me confused, but happy to oblige any requests to remove the post = )', 'Oh, please don’t remove your post! One of the things we’re doing here at SCRF is using the forum format to test open peer review, so think of the comments here – and requests for citations – as a communal attempt to help you develop your project. jringo:The point of this writing is to help guide people designing the new systems of a digital, trustless reality, and given their choices, to encourage them to help with redesigning science. Secondary, the point is to highlight the mechanisms of science and the questions surrounding them. Tertiary, to discuss answers to the questions and possible implementations.Aside from scoping out the problem, which is why I was asking you for citations (you need to show your work, even when something does seem obvious), how are you imagining tackling this project? Is it a guide, is it a wiki, or something closer to a DAO?(And speaking of, we’d love to hear why you think 99% of DAOs will probably fail, which I also agree with)Happy to describe the roots of scientific university research if you like. Humboldt University in Berlin (1810) was the first to combine teaching and research into a single institution, which became the model for incorporating scientific research into universities in the 20th century. You’re absolutely right that there were ecclesiastical and liberal arts colleges for a millennium before (such as Oxford, which I think was founded in the 11th century), but the way that universities and government labs do research today came from Wilhelm von Humboldt’s ideas.', 'There is no project, as described in the original post.Besides the three goals I already mentioned, I don’t know how else to describe the intention.I think we might disagree on the definition of “roots”.', 'Inherently, if the original post makes claims that are uncited; time will have to be spent clarifying those claims.  I don’t think @jmcgirk was suggesting that you remove the original post, but the original post makes many unfounded claims (that may in fact be true) and moves forward assuming those claims to be true.We would not have to spend time in the comments going back and forth about the claims if there was a peer-reviewed article which could be referenced to prevent this type of back and forth about individual aspects of the post.  The original post has many useful questions that could be examined to help further the space.  On the other hand, the intermixed opinions that do not have citations make it more difficult to get to the questions that really need to be addressed.I think it would really increase the chances of a project happening if you were able to identify your claims and substantiate them so the forum comments could focus on the subsequent questions that are not rooted in potential logical fallacies.  On the one hand, I think many people would agree with some of the claims you have made; but on the other hand we would not have to spend any time debating those aspects if there were references.In this context, I think it may be useful to examine your original post and look at what is clearly an opinion and try to find a peer-reviewed article to substantiate it.  Where you can’t prove a claim, it might warrant deletion of that claim, but not the entire post.  I am not certain if you had your original medium post edited or reviewed before posting it here, but the process of editing and review is the step which usually points out those needs for citations so that by the time the piece gets published the discussion is about the intended subject rather than trying to clarify uncited claims.All that said, I think there are many useful questions in the original post that could guide project development.  The main problem is that they are coming from unsubstantiated claims which ultimately leads to an ever-fragmented discussion.  I believe it would go a long way for the potential for any subsequent project to occur if you could just take a few days and cite or delete the unsubstantiated claims in the OP.', 'The point being, if we’re trying to get to a certain point, we cannot start from incorrect assumptions and presume to get to the desired goal.  I’m definitely not trying to be pedantic about picking apart your posts.  The issue is that a reference goes a long way in preventing derailing the discourse.  Some references for the original post would be a great start to directing the questions that have been posed.  As I stated earlier, I am not sure if there was any review between when you posted originally on Medium and when you reposted it here; but if you consider this feedback as “review,” you could edit the original post into a much more compelling framework from which projects could occur if there were citations.', 'I think there is confusion around what this is. I am not looking for review. I was told to post a random thought to the forum, so I did not expect the forum to be a place for peer-review.It’s difficult to not sound rude saying this, so please appreciate that I’m not trying to be rude but: I don’t really care if people think these things are unsubstantiated. That’s not the conversation nor audience I’m looking for and will no longer engage with it.If anyone wants to discuss alternative mechanism designs, on the other hand, I’m game!', 'I do want to jump in here because I think there is some strong alignment with what everyone here is posting about and I’m concerned we might lose that agreement and ability to move forward in the conversation toward actionable outcomes based on @jringo’s ideas.  Also, I think this is a valuable discussion to have about the nature of knowledge creation, distribution, and engagement.At the core, I think the main premise here is that there are a variety of flaws in the current system in that information is often behind paywalls, access to tools or legitimacy are often secluded within institutions, and contributions are not always appropriately rewarded. Additionally, this is proposing that a trustless and open system would correct these failings. Or maybe even that a new system is wholly needed and that a trustless system of science is what replaces the current status quo altogether.It’s at this point that I think both @Larry_Bates and @jmcgirk are making some valuable contributions. It seems there is agreement that these flaws exist, but if we are in the pursuit of solutions to problems, then we need to precisely define and understand both the problems and the solutions. For example, the claim that jringo:Closed and permissioned processes of knowledge production, consumption, and translation created a system that incentivizes consolidated control of its mechanisms. To this day, and despite several significant advancements in knowledge-sharing technology, these arbiters of trust, capital, power, and culture choose who has access to the resources, tools, and outcomes of science.would greatly benefit from some additional development and support. My reading of this is that science as an endeavor is monolithic, and I’m not sure I agree. From my own experiences, I suspect that there are both between and within variations of centralism and consolidation in academic/knowledge specific disciplines. To my mind, that would have a significant impact on mechanism design choices and considerations.This post might get us started on addressing some of the questions posed by @jringo in the Mechanisms section of the post. Many of the questions posed have some robust discussions already occurring within the legacy system and are scattered across a variety of disciplines. This section in particular strikes me as having a lot of potential to become a Key Questions in DeSci and Notable Works in DeSci if SCRF creates a DeSci category on the forum. From what I could tell in the chat, this seems likely and something that I would hope @jringo would contribute to and potentially lead the initial populating of these future posts.I do agree that a discussion about alternative mechanism design might also be a fruitful direction for this conversation, so I’m looking forward to that unfolding as well. I’m going to come back to this post soon as I would particularly like to explore mechanisms of reputation related to the key questions in Publication and Review.', 'Very much appreciated. Looking forward to it. Happy to contribute where I can.', 'I’m going to pen down my thoughts and contribute to the discussion here as I’m fascinated by trustless systems.I do concur with @jringo on the emergence ‘roots’ of science. Modern universities and current systems may be pioneered in the 1800s as @jmcgirk points out but the roots for scientific knowledge began in the 16th century.  I would like to reference here a short paragraph from the chapter of Marriage of Science and Empire from Harari’s book  - SapiensEuropean imperialism was entirely unlike all other imperial projects in history.Previous seekers of empire tended to assume that they already understood theworld. Conquest merely utilised and spread their view of the world. The Arabs, toname one example, did not conquer Egypt, Spain or India in order to discoversomething they did not know. The Romans, Mongols and Aztecs voraciouslyconquered new lands in search of power and wealth – not of knowledge. Incontrast, European imperialists set out to distant shores in the hope of obtainingnew knowledge along with new territories.This gave birth to Europe leading the technological race which was built on the knowledge foundation which was then centered in Europe. The Europeans funded a lot of research through which amazing results were produced including the Theory of evolution as we know it today from Charles Darwin. We should keep in mind that these fundings were not altruistic but had a political, religious or economic goal behind it. Without those funds, we might not have had advancement in knowledge and hence in societies and systemsSimilarly, even today, funding is the bottle neck of research. I think @jringo is referring to this complex intertwined systems of research funding which revolves around external influences which are result oriented.I am very much interested in the further development of such proposed systems. @jringo Would like to hear your thoughts on these,In your system, from which date would one start incorporating the research ‘object’ ? Most of the research now is built on preexisting knowledge, how can we ensure that the system recognizes previous work? Or do we just start afresh.How do you define a research ‘object’ ? Are they peer reviewed papers?Ref. your publication questions, I think SCRF can be a platform for initial experiment on discussion and collaboration over publications. What is your take on this?I’m however concerned about,To receive funding, the scientist must convince the general population of the network that their research is worthwhile and network-value-aligned. To ensure network-value-alignment, the general population must learn about the research and understand its goals and methodology. Both parties are thereby indirectly incentivized to interact, educate, and engage. Contrast this with the current tax/corporate model of funding where a researcher convinces only other researchers or administrators that their science is valuable.Since the learning curve for generic population will be pretty steep and might lead to false, wrong or misinformed decisions of the groups. I think this is pretty evident in today’s democracy. How many people actually read or completely understand the Manifesto’s or similarly in DAOs? I’m afraid this needs a different solution.', 'I think that it is almost impossible for us to understand or predict what will make an effective trustless system. We live in gated systems and suddenly have the technology to build something never really seen before.It’s like creating the 3d printer and celebrating that you’ll always be able to make a screw whenever you need one. The shift of the technology, though, is that it makes objects that don’t require fasteners.The way forward is to enable experimentation on mass scale, which is what Bitcoin did. “Here is an economic model based on technology that enables permissionless and trustless systems, here is how it works, try to do something else with it.” We have had tens of thousands of experiments over the years since and every single one, even the intentional rug-pulls and scams, play with the potential of the technology. That play has produced some very exciting projects.This is what needs to be done by those interested in using the technology to improve knowledge creation and translation. Thousands of iterations.All this to say:I don’t have a preferred system. I think the questions of how an object begins and how to maximize recognition are good ones. I imagine some iterations will use centralized bootstrapping mechanics, others will start completely fresh; some will incorporate legacy research and attempt to retroactively reward contributors from the past, others will not; and others might use a combination of extremes. For the time being I think building future systems first and porting in legacy knowledge trees makes the most sense. Maybe in the end there will be multiple open-source repositories of knowledge and recognition along-side centralized repositories, each with different views and value-sets. Different iterations can then choose to use one or more of these repositories or develop their own. The team at Foresight is doing some fascinating work on the subject.The constitution of a research object is another great question. The incorporation of peer-review will undoubtedly be critical to some iterations. At the moment, however, my view is that peer-review structures are not required in trustless and permissionless systems. The goals of legacy peer-review structures can probably be achieved in novel ways that simultaneously increase access to and participation within the system; Peer-review structures might be like the screws that hold the current system together – maybe we don’t need screws anymore. What I can say for certain is that the contemporary “research object” is a .pdf and there is so much more to research than a .pdf. Research objects can be interactive, include software, include multiple translations, include multiple specialized language versions, be verified, validated, and elevated in a thousand different ways, and much more. DeSci Labs is doing some awesome work with research objects.Regarding your concern:First off, and probably most importantly, I imagine there will be thousands of iterations of decision making structures. Some might completely exclude the general public from decision making. Others might only have the wealthy make decisions, or maybe only tik-tok influencers make decisions. Others might let only specialists in a field make decisions on matters related to their own field. Other iterations might weight specialists differently than the general public based on contributions or reputation. The possibilities are unknown until people start playing around.This is the point where I have a very strong opinion = ).I think concerns around public inclusion in decision making are valid, but are based more in taught fear and exclusivity than any valid foundation. We are all human. We were all at some point the general public.One of the critical failures of the contemporary system of science (along with most contemporary institutions and systems) is its active exclusion of the public. The difference between a specialist and the general public is the level of exposure and access to a system. So if a system perpetuates a gyre of exclusion, it reduces exposure, access, and ultimately interest to the point that the general public actively rejects the outcomes of the system. Eventually, the center can no longer hold the system together and things fall apart. That is what we see playing out all around us, not a learning curve that leads to misinformed decisions. It is active rejection of the systems that control our lives and to which the vast majority of society has absolutely no connection. We might be pleasantly surprised how quickly active inclusion can change someone’s level of acceptance of misinformation; If I am actively included in the creation of a vaccine (a stakeholder… at any level), I might more readily accept the technology behind it – If I am actively included in democracy, I might more readily accept democracy’s outcomes.Regarding how many people understand the specialized language or read-the-whole-thing: It only needs to be one for the information to spread accurately. That one person needs to teach the information to a communicator who then translates it to unspecialized language. Communication is critical to the adoption of a system. With Bitcoin, for example, people like Andreas Antonopolous have been just as important as Jakobsson and Juels.Another thing to keep in mind with regard to the fear of the general public making bad decisions. The specialized public is part of the general public. In an open iteration, specialized individuals would be making decisions along-side everyone else. How that looks will depend on the choices of the network and its iteration.In the original post I imagine indirect incentives reinforcing education and communication between the specialized and the general. This might lead to more informed decisions from the public. It needs to be played around with. We do a lot with this concept at Gridcoin where the head of a computing project must convince the entire network that their project is worthy of resources, in our case computational power. The mechanism leads to very intense and educational discussions around science. Even the general public that might only be interested in making money through Gridcoin must be sure that the science done does not hurt the value-proposition of the network at large (does something illegal, or has junk methodology, for example).Regarding SCRF:This is a think-piece. It doesn’t belong on a forum that is dedicated to strict experiment and research discussion and collaboration. At the same time, think-pieces can be very engaging and inspire research and collaboration. The two contribution-types come with their own pros and cons. There are a couple ways I can think of to get the benefits of both worlds – engage researchers and engage the public in their research which in turn communicates the research and might engage industry and inspire more research. Whatever SCRF decides, and it’s not an easy decision to be sure, it needs to commit.', 'I totally agree with you @YeshThose questions need to be answered.Plus, the excerpt you added gave me more insight into how Europe was able to have such economic, technological and political impact during those times.']"
                SCRF Terms Glossary and Content Tags              ,https://www.smartcontractresearch.org/t/scrf-terms-glossary-and-content-tags/364,Meta,91,[],['reference-material'],"['This document details the process and resources for managing the terms and tags that will appear in the forum and their associated definitions.What is this?This is a list of terms that represent SCRF’s best and most up-to-date framework for organizing content. Some of these also represent content area tags that are not specific to any particular category of the forum. For all of these terms, the associated definitions and sources are also provided.How will terms and associated tags be used?The average SCRF reader will be able to use these terms and tags to search for specific content, filter categories by subtopics and genres, and ‘crawl’ from post to post in the forum by finding related content to what is in a given post by clicking on its tags. This should also give the reader an immediate signal on what a given post is primarily about, improving the reading experience. These terms will also be linked in research summaries that have been posted to the forum.Note: Researchers generating content for the forum may only select from the previously approved list of tags when submitting work to the forum.How should I propose a new term? Or edit an existing definition, tag, source, etc?The source material for the master terms glossary list lives in GitHub 42.Once consensus has been achieved on this forum thread about new terms or changes to definitions/tags/sources then a pull request can be submitted to GitHub and a moderator will review the change with the content team and the request will either be merged or rejected with feedback.What kinds of terms should be included in this list?The terms in this list are meant to be representative of significant concepts that are referenced in multiple summaries and posts on the forum. If these terms are not already associated with a content tag in the forum, then they should at least be good candidates for tags. This means that they are specific but not excessively niche; they are broad but not too general; they should be able to be used to meaningfully and usefully filter and sort content on the forum.What should I do? How can I engage with this list?Researchers may add to the forum thread below to propose and discuss additions or edits to the list. This is meant to be a living document. Terms and associated information are not set in stone and by no means do we expect the first iteration of this list to be complete or even mostly correct. We ask and encourage all forum participants to contribute to our taxonomy and help make this list better. We would love to see active discussion and disagreement even about what the appropriate terms are to describe forum content and what the most accurate definitions are for those terms.', 'After a discussion with @cipherix, I believe a tag for “Sybil-protection” or “Sybil attack-resistant” or “Sybil attack” may be useful for the forum.  It is important to articulate the difference between something that is Sybil-attack resistant vs. BFT in that they perform similar functions but the means of preserving node integrity are slightly different.', 'Great suggestion. I think there are some past posts that reference Sybil attacks that could be tagged and I can easily envision future content using this tag. Can you go ahead and submit a pull request for this?I have added “sybil-protection” to our tags and tagged this post accordingly: Research Summary - Chainlink: A Decentralized Oracle Network 3', 'We have updated this document to now include definitions for terms (and related sources): docs/reference_terms_glossary.md at main · smartcontractresearchforum/docs · GitHub 14Researchers can use the same process to interact with this reference material. Would love to see engagement and community contribution to this document. Please post here if you have suggestions for additions or edits to these terms, tags, definitions, or associated sources.', 'I will need to add some tag for scalability articles.', 'We have the scalability 3 tag right now', 'But I am wondering how far we should go in dividing scalability question.First we Systematization of Knowledge (SOK) papers that encompass a big part of the state of the art. I don’t think I can find more than 10 that are relevant for blockchain or cryptography so I am not sure it needs its own tag.Then you have tweaks to the consensus rule that are not POW/POS/BFT. For instance introducing a DAG instead of a Merkle tree. That could fit in “consensus”.There is also the characterization with L0/L1/L2. L0 would be more the network, L1 the usual blockchain and L2 everything from lightining to rollup. And we could also subdivide L2.', 'What would you propose for the more specific list of tags for scalability?', 'I would say Layer-2 would be a nice addition.', 'Agreed. Here are some suggestions:Layer-2,Payment Channel,Rollups', 'So we have:OptimisticLayer 2RollupsScalability… as terms right now, but only Scalability has an associated tag. We can convert these other terms from the glossary into tags and add Payment Channel. Can one of you submit a PR for this?', 'I can take care of the PR.@Vishesh Pull Request 3', 'After a brief discussion with @Rich and upon the recommendation of @zube.paul, I would like to propose an “Ethics” content tag that can be applied to research that deals with blockchain technology in the context of data ethics in particular. This paper that I summarized provides a broad discussion of some of the ethics issues in the smart contract and oracle space in particular. Looking forward to hearing your thoughts.', 'Hi @jasonanastas, sounds like a good idea.Have you had a look at the linked glossary 6 page to get a sense of the format? There are some instructions in the OP that go over adding new tags.For the ethics, here are some questions to get us started:I wonder if there is a more specific term that encompasses blockchains specifically?Is it generic and applicable enough to be a standalone tag?Are there fundamental papers in the space that we can pull a definition from?Who should we cite in the glossary?If you are unsure about how to submit a PR to github we can come up with the formatting here and then I can add it to the repo.', 'Thanks Rich, these are great questions. I’ll get back to you shortly with some responses.', 'Hey @Rich some good news. The Stanford Encyclopedia of Philosophy was willing to entertain a proposal in their encyclopedia for “Blockchain Ethics” so I’ll let you know how that turns out. Also here is a revised proposal below. I included a link to a syllabus I found which seems like the most comprehensive treatment of the issue I’ve discovered thus far.Proposed TagTermDefinitionSource Linkblockchain-ethicsBlockchain EthicsAn area of research which studies the ethical aspects of the design,  implementation, and governance of blockchain based monetary exchange systems and institutions.  Source 2Let me know what you think!', 'Cheers, thanks for the update.Some notes:feels a bit circular to be referring to our own forum for a definition, there are no official sources?the Term is the expanded version of the shortened Tag in the glossary so they should directly reflect each other I thinkyou have a tag for ‘ethics’ but in the term you get more granular with ‘Blockchain’ and ‘Oracle’ ethics, then in the definition there is ‘data’ and ‘contractual’ ethics mentioned. This is what I was getting at in my comment above about whether we need to create some clarity about what type of ethics we are talking about. It’s a fantastically encompassing term… Do we need to have one or more tags for oracle-ethics, data-ethics, etc. Or perhaps blockchain-ethics is enough to imply that it covers all the rest?', 'Thanks  @Rich, all fair points. Here’s my response to each below:It is indeed circular, but that’s because there is no “official” source yet to the best of my knowledge. The paper that I summarized in the post that I linked to is the most official source I could find thus far but I can dig a bit more. If I were to refer to another source it would probably be something like “business ethics” which deals with data ethics and contract ethics.No problem regarding the TermI think blockchain-ethics is be best tag since it would encompass both data ethics and contractual ethics that are relevant to blockchain.I will revise accordingly and reply.']"
                SCRF Interviews | The Decentralized Art Object Framework with Eric Barry Drasin (Ep. 8)              ,https://www.smartcontractresearch.org/t/scrf-interviews-the-decentralized-art-object-framework-with-eric-barry-drasin-ep-8/1419,Governance and Coordination,15,[],"['discussion', 'summary', 'dao', 'governance']","['OverviewThis episode of SCRF Interviews features an interview with CU Boulder Media Studies Ph.D. student artist Eric Barry Drasin, a media artist who describes the Decentralized Art Object Framework he created, some of his work, and his thoughts about serious art on the blockchain. Hosted by James Brandon McGirk, an editor at the Smart Contract Research Forum.Links:Video 9Audio (Apple 2, Spreaker)At issue:One of the characteristics of contemporary art–particularly conceptual art–is the dematerialization of the art object itself, which refers to emphasizing the thought process even when it renders the art object (such as a painting on a canvas) obsolete.The Decentralized Art Object Framework (DAOF) was a legal and technological framework using a blockchain to store coded sets of instructions that could become art objects in themselves, this allowing both transfer and “rematerializations” of generative work, i.e. the creation of authorized replicas per an artist’s specifications.Since the DAOF framework mostly replicated traditional art contracts, Drasin used another maneuver to create his own work; he used the framework as a conceptual scaffold to present several speculative works that reimagining and interrogating reality.Opening Normcorp 2’s whitepaper takes you to a blank page. Scroll down further and the page will increase in size, and continue increasing and increasing in length unspooling nothing but a blank white page until the browser crashes. Beneath the hood, the “whitepaper” is a set of instructions endlessly creating content-less div tags.“It’s a monochrome,” explains artist Eric Barry Drasin. “These are no-content, single color pieces with a history stretching back to the Middle Ages with radically different meanings from what a Medieval Gnostic mystic might say versus a Russian Constructivist, but the basic concept is that a monochrome reflects the society that exists around them.”Normcorp, part of Drasin’s 2020 MFA Thesis show “Exit Strategy,” took aim at startup culture, or more specifically Eric Ries’ The Lean Startup, which advocates for rapid reinvention and reorganization that Drasin sees as eerily akin to performance art. “[The attitude is] we are starting a startup. We value value, it’s totally self-reflective except for the legality and the performativity of value itself.”The work was built on the conceptual framework of another project that Drasin had been working on with his friend and mentor Benton C. Bainbridge 3, the Distributed Art Object Framework (DAOF) 2, which was a technological and legal framework for storing and transferring art that gave rise to certain properties.DAOF was based on a standard contract that had been developed for selling and transferring conceptual art in 1970, the Artists Reserved Rights and Transfer Agreement, developed by curator Seth Siegelaub, and lawyer Robert Projansky. The idea was to take performance art created using a generative process, meaning each time it was enacted, and capture the instances on the blockchain as coded instructions, so that they could be editioned, meaning replicated from a master copy, and collected.“At the time we had envisioned it as a kind of open source commons platform, and while we might make some money, we didn’t want to do it at the expense of the community,” Drasin says. “We were working VCs and startup guys who were pushing us to become a business and talking about how they’d sell us out in a few years. It made us uncomfortable, and so we shelved the project.”Meanwhile, as Drasin and his team worked on DAOF, he realized that he was also expected to produce artwork for his MFA that would refer to the historicity and conceptual frameworks he was learning about in his program. Drasin tapped into conceptual art’s long tradition of contracts and provenance.Dadaist Marcel Duchamp had sold “Monte Carlo Bonds” in 1925, decorated with his shaving foam covered face and signature, promising ‘investors’ a stake in his ‘casino winnings.’ Yves Klein had sold receipts for “Zones of Immaterial Pictorial Sensibility” (1959), which verified the existence of an invisible piece of artwork, and were given in response the successful performance of a ritual, in which the collector threw gold into the Seine River, witnessed by a lawyer and gallerist.More recently, Mitchell F. Chan had recreated Klein’s bonds on the Ethereum blockchain (in “Digital Zones of Immaterial Pictorial Sensibility,” 2017).“I became very interested in the idea of contracts mediating social reality,” Drasin says. “I wondered what else we could do. Benton was talking about how blockchains could create an economy from scratch, and me being a student of political economy, I was always like… ‘wow, what does it mean to be able to do this? Can we encode better behaviors, better ideologies?’”Using the DAOF framework as a conceptual scaffold, Drasin imagined a number of hypothetical organizations: there was the “Decentralized Autonomous Katamari” (2017), a “relational sculpture and digital poem existing as an object across digital and virtual space.” Like Klein’s rituals, DAK relied on a set of rules constraining the use of the tokens and guaranteeing recursive value increase.He imagined the results as a Cookie-Clicker like stack beginning with an ICO sale and a server farm, and ending with the retrospective on a terraformed Mars after consuming all of the earth’s resources.Next came “Post Capitalist Karaoke,” (2018/2019) where a speculative piece where a whirling hypercube danced behind the words to an altered karaoke version of Cher’s (1998) Believe, where the word “love” had been replaced by “capitalism.” But there was a problem.“It’s very difficult to imagine life beyond capitalism,” Drasin says. He considers himself part of the tradition of artists using new communications technology to reimagine reality. When asked about blockchain related work specifically, he replies that he’s sad there hasn’t yet been a great blossoming of art that makes use of the blockchain as a medium.“There’s been a bit of a gold rush,” Drasin says, referring to the explosion of NFT collecting. “Artists have been paid peanuts for years and suddenly think it’s time to get theirs.”The problem with the combination of web3 and art is that anything that touches the blockchain seems to become a hard speculative asset.“You have to deal with the nature of the system itself,” Drasin says. “What is the material structure system? What are the fundamental ideological encodings within, can we start twisting, can we glitch them, can we break them? That slippage allows new things to emerge.”Check out his next piece, DungeonMasterLLC, which debuts on April 15, 2022.dungeonmasterllc | Twitter, Instagram, YouTube | Linktree 3', 'To get the discussion kicked off here, how do you think artists and engineers could be working together to improve the way that artists are compensated and their work archived and safeguarded? @tebogonong I’m curious what you make of this conversation given your background in archiving and blockchains. Is anyone else at SCRF working with any art projects?', '@jmcgirk I enjoyed this trip down the rabbit hole of artwork detached from everday reality. To your question, I have high hopes and deep-running concerns. Web3 and the NFT movement were supposed to establish clear ownership of digital goods in general (not just artwork), and in so doing create entirely new capabilities for the internet.Not surprisingly, the first wave of NFTs was largely a stunt, easily co-opted by insiders at the expense of FOMO-driven public latecomers.What will it take to create an equitable DeFi world immune to all the oldest scams, cons and machinations of self-dealing monopolists? Will it come down once again to regulation by centralized authorities which essentially strangles decentralization in the cradle? Or can DeFi marshal not only its technologies and protocols but also its constituents to create a new extra-planetary path forward, independent of the battles between sovereign states?It could still go either way. Unfortunately, unlike the arc of the moral universe, human financial affairs don’t naturally bend toward justice.', ' rlombreglia:To your question, I have high hopes and deep-running concerns. Web3 and the NFT movement were supposed to establish clear ownership of digital goods in general (not just artwork), and in so doing create entirely new capabilities for the internet.One of the strange things about buying art NFTs is that they don’t actually fit the most stringent guidelines for buying art in the United States (I think this is the law - CA Civ Code § 1744.7 (2020) 1. So many ‘serious’ artists have hesitated to get involved (and Eric’s Moving Pictures gallery project was intended, among other things, to address this issue).Not sure if it was entirely a stunt. The first real NFTs were created by Monograph and Anil Dash, around 2014ish, and that was entirely on-chain, then there were similar projects like Mitchell Chan’s Digital Zones of Immaterial Pictorial Sensibility in 2017. Cryptokitties was the first big collectible project and I’d probably draw a bright line between collectibles with varying levels of rarity, which are more like packs of baseball cards than pieces of art, I do agree deeply with what Drasin was saying about there being a lack of art that addresses the intrinsic qualities of the blockchain, it’s even more apparent with narrative art – you see a few poems and ebooks sold as ERC-1155 (editions) but they don’t really make use of blockchain technologies. Not that you need to, but someone should… .I think we’re stuck with scams, cons, and machinations. One of my favorite books is Jay Robert Nash’s (1974) Hustlers and Conmen which details just about everything you see in web3 today, with a few tech variations. Eventually, I think DeFi will be tamed by the institutions and the legal systems and onerous requirements like KYC and anti-money laundering and it’ll be functionally indistinguishable from other banks.I have more faith in DAOs personally, I think new, flexible, organizations will be an important bulwark against what @ntnsndr described as the economization of everything (which Drasin is also criticizing with his work).', ' jmcgirk:Not sure if it was entirely a stunt.I was searching for the right word, but “stunt” wasn’t exactly what I wanted, and of course I painted the whole phenomenon with one broad brush to make my point. jmcgirk:I do agree deeply with what Drasin was saying about there being a lack of art that addresses the intrinsic qualities of the blockchain, it’s even more apparent with narrative art – you see a few poems and ebooks sold as ERC-1155 (editions) but they don’t really make use of blockchain technologies. Not that you need to, but someone should…I agree for organic reasons, but what exactly are the intrinsic qualities of the blockchain? Decentralized records of activity verified by consensus of some sort? jmcgirk:Jay Robert Nash’s (1974) Hustlers and Conmen which details just about everything you see in web3 today, with a few tech variations. Eventually, I think DeFi will be tamed by the institutionsNash added to my reading list. And I agree about DeFi, but then we’ll have to stop calling it DeFi and just call it Fi, or CenFi. Or BigBrotherFi. jmcgirk:I have more faith in DAOs personally, I think new, flexible, organizations will be an important bulwark against what @ntnsndr described as the economization of everythingAgree here as well.', ' rlombreglia:I agree for organic reasons, but what exactly are the intrinsic qualities of the blockchain? Decentralized records of activity verified by consensus of some sort?I’m a big fan of Robert Coover who wrote about how various technological leaps radically changed the novel. There were stone tablets, then scrolls of fragile parchment, then wax codicies, then delicate handwritten codexes, then hand pressed printing, typesetting and mass distribution and finally what we have to do – an entirely digital process except for the very last step (unless you’re using a Kindle). There’s a similar story to be told with art… Drasin refers to electricity becoming seen as a sculptural medium, and the dematerialization of the art object; now we can re-materialize conceptual art to some extent with the blockchain… so that opens up new possibilities, it’s just hard to say what… Damien Hirst made auctions and fractional ownership a kind of medium, while making millions in the process. Blockchains are great for fixing events in history, I think there are interesting things to do with memory and of course in the microsocieties we build on top of the blockchain (as Drasin has done).', '@jmcgirk Thanks for reminding me of the whole dematerialization/rematerialization theme, which is clearly an important path into the future of the metaverse (which will NOT be owned or controlled by Mark Zuckerberg, by the way). I can see its applicability to art and I hope it means control of work and fair compensation for artists. (See my comment about Punk6529.)But somehow it isn’t the art world that really brings home dematerialization for me, and it certainly isn’t securities (which don’t have much physical reality in the first place). It’s the much larger story of the dematerialization of immovable properties, specifically real estate, that gets me to the “holodeck” dimension of all this. When you wrap your mind around that, the depth and scope of the paradigm shift we’re talking about really hits you.', 'I think I pointed this marvelous project out to you elsewhere, and I know that @jyezie is writing a review of one of their presentations; but [https://movingcastles.world/](https://Moving Castles) - which was funded by Gitcoin - have created a model for an independent metaverse that would rely on binding together the little secret chatrooms, guilds, and servers that communities use, they imagine themselves as a kind of ‘moving castle’ a kind of shambling bolus of stuff held together by a friendly demon and the community’s will that moves a community of people through a hostile universe… perhaps that could serve as model for art and the metaverse to come', 'Thanks for this podcast, learnt so much about decentralized art but the guest speaker said something that intrigued me and made me a bit confused,When discussing the History of Media Art, he said something about a contract created in the early 70’s "" artist reverse rights and transfers agreement, the contract is so easily replicable that can meditate sale and re-sale of works protecting the rights of artists that made dematerialized art work,Here’s my question : artist have the right to revoke the rights without transfer of ownership,is there any law that supports this and how is it possible that an Artist can revoke their own work without transfer of ownership and if that’s the case who does it belong to then ?', 'One area that frequently battles with attribution is provenance, where the question is a little hazier because the conflict usually tends to focus on ‘verifying’ whether paintings are real or not. This DW article goes into the legal questions a bit: How provenance research came into the spotlight | Culture | Arts, music and lifestyle reporting from Germany | DW | 12.04.2022 1One thing that came up recently in the US, and is more pertinent, was an issue over Cady Noland revoking some her work, which focuses on a 1990 law called the Visual Artists Rights Act.One key provision states that an artist “shall have the right to prevent the use of his or her name as the author of the work of visual art in the event of a distortion, mutilation, or other modification of the work which would be prejudicial to his or her honor or reputation.”Apparently, it’s a very rare exception to the US’s general stance on property rights, where ownership tends to be the final word.', 'I found this podcast interview very informative. it encapsulated many of my interests: art, the mechanisms of video art and emerging technological communications.research is an excellent start for artists and engineers to come together, mainly because there are certain overlapping jargon and definitions that seem similar across industries but actually differs, essentially getting engineers and artists on the same page regarding specific problems, in this way better-informed solutions and opportunities can be presented for the improvement of artist compensation and the archiving of their work.By creating multidisciplinary projects and initiatives specific problems can be solved considering the country in which the artists and engineers find themselves, some challenges that may be considered are socioeconomic conditions, technological infrastructures, political and legal frameworks.DAOs and industry-specific platforms can be created to compensate, fund artists and archive their work.']"
                Research Summary: The decentralized financial crisis              ,https://www.smartcontractresearch.org/t/research-summary-the-decentralized-financial-crisis/265,Auditing and Security,33,['https://arxiv.org/abs/2002.08099'],"['simulation', 'defi', 'summary', 'governance', 'network-security', 'scalability', 'flash-loans', 'testing']","['TLDRThis research paper presents evidence showing how a financial crisis could occur in the DeFi ecosystem.(1) The researchers demonstrate a flaw in Maker’s governance, which if exploited could have caused a general meltdown (this flaw has since been patched);(2) The authors use a simulation based on historical ETH prices to show how a stylized protocol with USD$400 million in loans, backed by ETH, could become undercollateralized in just 19 days.CitationL. Gudgeon, D. Perez, D. Harz, B. Livshits and A. Gervais, “The Decentralized Financial Crisis,” 2020 Crypto Valley Conference on Blockchain Technology (CVCBT), Rotkreuz, Switzerland, 2020, pp. 1-15, doi: 10.1109/CVCBT50464.2020.00005.arXiv.orgThe Decentralized Financial Crisis 7The Global Financial Crisis of 2008, caused by the accumulation of excessivefinancial risk, inspired Satoshi Nakamoto to create Bitcoin. Now, more than tenyears later, Decentralized Finance (DeFi), a peer-to-peer financial paradigmwhich leverages...Core Research QuestionHow can design weaknesses and price fluctuations in DeFi lending protocols lead to a decentralized financial crisis?BackgroundDecentralized finance (DeFi) comprises a set of (some-what) decentralized financial protocols running on a given blockchain network. Examples include lending platforms (e.g., Maker, Aave, Compound), decentralized exchanges (e.g., Uniswap, Kyber, dYdX), and derivative market protocols (e.g., Synthetix).DeFi protocols are essential “Money legos”; returns from one protocol can be fed to another protocol, and so on and so forth, creating an intricate money flow network. Failure in any of these components potentially causes a cascading effect, a.k.a financial contagion.At the heart of DeFi are liquidity providers, holders that provide their own crypto as liquidity to lending, exchanges, and other platforms. With time, liquidity providers accrue earnings from fees paid out by users.In DeFi, lenders cannot be mapped to the actual actors borrowing money; hence, lending protocols require lenders to provide a security risk deposit known as collateral. Borrowing against one’s own crypto, given as collateral, allows lenders to quickly gain liquidity in another coin or have spending power while potentially deferring government taxes.Flash loans are one particular loan type that does not require collateral. The catch, however, is that borrowers must pay the full loan amount in the same transaction where the loan is granted; otherwise, the transaction is reverted. Lending platforms make a profit by charging a lending fee on each flash loan. The lending fee must also be paid in the same transaction as the loan grant. Among others, flash loans allow borrowers to take advantage of arbitrage opportunities with little upfront investment.To fine tune protocol as a means to adjust to community demands and/or values, DeFi protocols often rely on some form of governance. Governance itself could be fully decentralized, as in Maker, or rely on a centralized figure—e.g., as in Compound.To be able to cast a vote, one must hold governance tokens. In the case of Maker, the token is MKR; one’s voting right is directly proportional to the amount of MKR locked in Maker’s voting system. In Maker, an executive contract is granted control over the entire system as long as it has more staked MKR than the current executive contract. If a malicious actor holds such an amount, he can elect a malicious executive contract to steal all the ETH backing Maker’s algorithmic stable coin: DAI. The malicious executive contract could proceed to mint as much DAI as wanted; the attacker could then use the minted DAI as collateral in lending platforms supporting DAI, effectively getting loans that would never be honoured. The latter would then trigger a cascading effect on many other DeFi components; a generalized meltdown would likely follow.To prevent the latter scenario from ever occurring, Maker implemented a delay period preventing any action from a chosen executive contract, giving time for anyone with enough MKR to trigger a global settlement of the system if needed be.SummaryThe authors present two perspectives on how a decentralized financial crisis could occur.First, they present a real-world case where a design flaw in the Maker governance could have allowed a malicious actor with enough MKR tokens to elect an executive contract to take control over the entire Maker system, while stealing funds from systems relying on DAI-collateralized loans. Essentially, Marker’s governance allowed an elected executive contract to immediately perform any action, i.e., the safety delay period was set to zero. If elected, the malicious executive contract could immediately transfer all the ETH collateral in Maker to the attacker’s account, as well as minting as much DAI as wanted. With the latter, the attacker could take DAI-collateralized loans that would never be honored, depleting the funds in any DAI-supported lending platforms and decentralized exchanges. If executed when the paper was published, just the ETH collateral in Maker would give the attacker an estimated net profit of $190 million dollars.Second, the authors present a stylized DeFi lending protocol that abstracts over the main lending market protocols at the time, namely Maker, Compound, Aave, and dYdX. The stylized lending protocol takes ETH as collateral and provides loans in a USD pegged stable coin, with a collateralization ratio of 1.5, i.e., for every 1 dollar given in the loan, borrowers provide 1.5 dollars worth of ETH as collateral. Furthermore, the protocol starts with an extra reserve of 1 million tokens of a generic governance token, initially pegged to the same price of ETH for simulation purposes. System debt (amount of loans given) was initially taken to be between $100 million and $400 million dollars, seeking to reflect the same debt levels as found in major DeFi lending protocols at the time the paper was written. Taking parameters from the historical ETH/USD price data between Jan. 1st/2018 and Feb. 7th/2020, the authors use Monte Carlo simulation to estimate the ETH price at each point in time in the next 100-day period; additionally, the authors simulate the decline in liquidity over time. Overall, they show that if the stylized protocol starts with $400 million given in loans, undercollaterization could happen in just 19 days. Since the given loans would be worth more than the locked collateral, borrowers would have a financial incentive to pay back their loans.MethodEmpirical analysis and simulation.ResultsThe attack on Maker’s governance would have been possible, but it would have required a large sum of money (over USD$20 million at the time of the paper was written); hence, the attack would either need a supporting whale (one holding large sums of valuable crypto), a crowdfunded pool, or a flash loan. Following a flash loan approach, the attacker should operate as follows:In one transaction (t1), deploy the malicious governance contract and the flash loan contract.In a second transaction (t2):Acquire a flash loan whose ETH amount is enough to buy the MKR amount to elect an executive contract. At the time the paper was written, this was estimated to be 50,000 MKR (about 379,000 ETH).Upon granting the flash loan, the lending platform invokes the flash loan contract, whose logic executes the following steps:Swap the given ETH to MKR. Then, use the 50,000 MKR to vote to replace the executive contract with the malicious one. Through the latter, mint as much DAI as wanted, setting the attacker as the beneficiary. Invokes the executive contract so as to get a hold of Maker’s entire ETH collateral. Last, pay back the ETH loan.In a third transaction (t3), provide the minted DAI as collateral to any DAI-X market, where X is any coin for which a lending platform pairs with DAI-backed loans. For instance, if X = USDC, the market is DAI-USDC; the attacker provides DAI and gets a loan in USDC.For each loan of coin type X (or a batch of coin types), have a transaction depleting all X tokens from the target lending platform. Continue this step for as many lending platforms as wished, or until the value of DAI drops to zero.The outlined attack was not possible at the time the paper was written, as lending platforms did not have enough ETH liquidity. However, it would only take 66 days for Aave to accumulate enough funds to cover the required flash loan. As of today (March 6th, 2021), both Aave and Compound hold enough ETH liquidity: over 513,000 and 1.2 million, respectively.As for the second perspective brought by the authors, the simulation of the stylized protocols shows that:If the initial amount of loans is USD$400 million, the protocol can be undercollateralized in just 19 days. This occurs when the governance asset strongly correlates with the collateral coin; i.e, their prices follow similar directions.A governance asset that weakly correlates with the collateral asset delays or prevents undercollaterization; if strongly negatively correlated (e.g., collateral coin value is low, while reserve coin value is high, or vice-versa), the protocol can back the given loans.Discussion & Key TakeawaysThe Maker governance system had defined a delay period before any elected executive contract could make any action, but the delay was initially set to zero. This shows that the governance contract lacked basic input sanitization. The latter could have been coded to reject any delay period equal to zero.A governance token whose value weakly correlates or negatively correlates to the collateral coin increases the chance of lending protocols to back its loans upon price drops in the collateral price.Under certain assumptions, lending protocols could be undercollateralized in less than a month time.Implications & Follow-upsThe flaw in Maker’s governance contract highlights the importance of auditing smart contracts. The flaw was disclosed to the team and developers eventually fixed it.The paper shows evidence that a financial crisis in the DeFi space is indeed possible; since protocols are intertwined, undercollaterization in one protocol could cause financial contagion across the entire ecosystem. Faulty governance and security flaws exposing funds could also lead to such contagion.ApplicabilityGovernance token economic designers should consider the correlation implications with collateral coin price as a means to prevent undercollateralization.Simulations such as the one performed by the authors could be made standard across the DeFi ecosystem; protocol designers could use them to assess the risk of undercollaterization and whether the economic security triggers in place would work as expected; investors could rely on the same simulations to assess investment risk.', 'Great point about this demonstrating the need and value of audits! It looks like the vulnerability identified by the paper was fixed by Maker and that left me thinking two things that maybe @lnrdpss and others could help me with.First, do we know how Maker fixed/addressed this vulnerability and how that prevents this type of decentralized financial crisis from happening this way?And second, it seems like this paper demonstrated a process through which under collateralization can occur, but it looks like it takes a significant amount of cash to pull it off. Do we have any estimates of how likely this is to happen?Actually, that brings up maybe a third question but I’ll just call it 2.1. Does the emergence of flash loans increase the risk of a decentralized financial crisis? Perhaps not something that they really worked to simulate, but perhaps that would make a change to their model?', '@zube.paul Thanks for your comment To fix the issue, according to the paper,  the safety delay control was set to one day. Hence, an executive contract must wait 24h before it is given full control over Maker. One day gives time for others to trigger a global settlement if the elected governance contract is found to be malicious.Since Maker’s code is publicly available, one can verify that Maker does indeed implement the change: (1) first, one needs to compile Maker’s source code - this has to be the same version supposed to match Maker’s deployed code;  (2) assert that the resulting bytecode matches what is deployed at Maker’s governance address (the actual deployed governance contract); (3) if the two bytecodes match, then the change is in. If not, then the change is not deployed in the network.As for flash loans, I totally agree with you. They do increase the risk of a decentralized financial crisis. But, the other perspective is that they empower users with money that would not otherwise be ever available to them. Among others, poor actors are now in a position to use flash loans to take advantage of arbitrage opportunities, largely increasing  returns that would not have payed transaction costs otherwise.While it is absolutely true that flash loans can be used for a bad purpose, an unintentional positive side effect is that flash loans forces the entire industry to deeply think and consider best security practices, and learn from its own mistakes. Sadly, this occurs at the expense of liquidity providers, who might loose their funds when locking them in lending protocols.  I guess for every investment, there is a risk to consider. Would you agree?', 'Is anyone aware of any projects attempting to increase the loan terms of a flash loan beyond one block?Theoretically, it seems possible if lending protocols implement stronger principal clawbacks. In a world where that is no longer a constraint, seems like governance delays become a cat-and-mice game and that additional security layers are required.', 'I agree with you twice! First, of course, there is risk in every investment opportunity. It would be nice to know the future, of course, but risk is inherent. Second, great insight regarding the empowerment dynamic that flash loans can offer. Seems like this not only helps potential investors but also can help networks become more secure if I’m understanding the research in Why Stake When You Can Borrow 4 correctly.Back to this paper though! I guess what I was getting at originally in my series of questions is how well does this paper help us understand the risk of a decentralized financial crisis? It looks like they ran a simulation on a known problem that has been fixed (and please correct me if that is an oversimplification!). Demonstrating this risk is valuable but isn’t it a little too limited in scope? I might not be understanding the theoretical position they’re developing here well enough to see it but I’m left wondering if this paper can help us look beyond Maker specifically and help us better identify and evaluate potential decentralized financial crisis red flags in the future? Maybe that’s not what this paper is set up to do and it’s just something I want it to be doing. Am I being too unfair here? Anyone else left wondering the same question?', 'I’m not aware of any projects attempting to do this at the moment. But since we’re in the land of the theoretical, could you maybe go into a little more detail about what you see that additional security layer having to be and/or accomplish?', '@zube.paul I believe the paper helps in the following directions:Real-world evidence of a bug that existed in one of the major lending platforms; if exploited, the consequences would have been catastrophic.A simulation of whether a hypothetical lending protocol borrowing the same characteristics of major ones (but simplified in nature for the sake of the experiment) could become undercollaterized; if so, a melt down could occur, as the debt in the lending protocol would surpass its current locked value (collateral). In the latter case, borrowers have no incentive to pay back their loans. Following their experiments,  they show that undercollaterization is indeed possible.While I personally find their contributions enough for what the paper aims to answer, my biggest concern is its Reproducibility 1 and largely (undocumented) methodology. At the very least, I would expect the authors to have published their simulation scripts somewhere so anyone could run them and verify the results. Also, it was unclear to me  the criteria of the chosen simulation parameters, which in my humble opinion is a methodology flaw. Having the latter two in place would help other researches build on their results, validate or refute their experiments, etc. That would have helped the entire community towards having reliable models we could all use. @zube.paul What do you think?', 'I think those are some great observations. I wish I had the answers/information. Perhaps in time we’ll hear from the authors themselves.Since you brought up criteria, I wonder if you might be willing to go through what you think good criteria would be? Not just for this paper, but overall. Or maybe it would just be easier to point out a paper that has done it really well.That might be valuable from other forum members also, what are some great examples of related methodologies?  Links to papers might be helpful if you’re commenting.', '@zube.paul At the core of any paper, reproducibility is a must. For reference, I really like the discussion in here: Reproducibility: A tragedy of errors | Nature 1As a general set of guidelines, the paper "" Scientific Integrity Principles and Best Practices: Recommendations from a Scientific Integrity Consortium 1""  provides a list of principles that apply “broadly across scientific disciplines as a mechanism for consensus on scientific integrity standards and to better equip scientists to operate in a rapidly changing research environment”. Note that this is not entirely tied to paper publishing, but rather takes a more holistic view as how we can have a mature environment as a whole.This is just a starter and probably deserves its own discussion thread  Let me know if you have other pointers that are worth sharing.', 'I wanted to revive this thread with the recent collapse of Luna, UST & the Terra ecosystem. I think that event was the decentralized financial crisis…or the closest thing to it that we’ll see. At the time of writing this, BTC has hovered at $30k for the last 9 weeks. Altcoins are showing some gains in Polkadot and Cardano but overall the market is, in my opinion, at its bottom. This is not financial advice but wanting to set the tone for the point in time crypto is at. The collapse of UST as both totally undercollateralized and algorithmic by design caused a weakness that lead to an overall fall in crypto. All major leaders were affected, and retail investors lost significant savings that had been held in the stablecoin. It was a disaster for individuals, projects, and the Terra ecosystem - which is now forking into Terra Classic and a new chain that will remove UST from design.There are a few significant risks that were prevalent before Terra’s collapse but are now systemic risks for crypto: lnrdpss:supporting whale (one holding large sums of valuable crypto),Whales are unhealthy for the ecosystem. It is likely a whale caused the death spiral in Terra. Regardless of if the deals were made because of larger macroeconomic conditions (looming recession; war in Ukraine; supply chain constraints across the world), Whales have too much power as unchecked market movers. This is well known in crypto. This does not happen the same way in TradFi and DeFi protocols need to ask why. lnrdpss:The Maker governance system had defined a delay period before any elected executive contract could take any action, but the delay was initially set to zero. This shows that the government contract lacked basic input sanitization. The latter could have been coded to reject any delay period equal to zero.Automation of these ‘deals’ causes unchecked implications that traditionally are handled by people because we know there are complexities here that no software system models for. Rather than algorithms, we use analysts and managers to ensure we are not collapsing the financial system because of arbitrary rules. We have instances of this to learn from. lnrdpss:undercollaterization in one protocol could cause financial contagion across the entire ecosystemThis is the crux of Terra’s impact. Long story short, IT DID. The whole crypto market is still reeling from the shock of that collapse. Sentiments control everything (in my opinion). It’s debatable how long it will take the markets to return. Crypto folks are quite resilient investors but I’ve seen some rumours that we could be in a bear market for two years (This is speculation. I am not a financial analyst or economist). lnrdpss:Simulations such as the one performed by the authors could be made standard across the DeFi ecosystem; protocol designers could use them to assess the risk of undercollaterization and whether the economic security triggers in place would work as expected; investors could rely on the same simulations to assess investment risk.They should be standard. Crypto needs standards. This is about efficiency, efficacy, and protecting consumers in the market. Crypto is risky (we want it that way), but we are lacking basic levers and valves to stop the bleeding when it starts. As more of the economy becomes crypto users, and as these ecosystems grow to invite and encourage the average user, it is going to have to model some basic standards of risk management.If this simulation could be set up as an open-source audit, with the LOE defined, it would be a valuable asset to the DeFi ecosystem.', '@valeriespina thank you for reviving this thread, we’ve been chatting about TerraUSD’s depegging in the following thread - Discussion Post: Taming Wildcat Stablecoins - #24 by jmcgirk 2 - which of course you’re welcome to join the conversation in. valeriespina:Crypto needs standards. This is about efficiency, efficacy, and protecting consumers in the market. Crypto is risky (we want it that way), but we are lacking basic levers and valves to stop the bleeding when it starts.How do you imagine crypto standards evolving? Should they be similar to SCRF Interviews | DAO Standards - DAOStar One (Ep.10) 1 or should they come from legislation?', 'Re the Terra/Luna collapse, Luna 2.0 has fallen 77% in the 14 days since it was issued, and Do Kwan has taken his twitter account private. If the resuscitation of Luna was a scam, it certainly didn’t take long for the rug-pull to occur.But perhaps more foundational to the future of DeFi than the collapse of Terra/Luna is a Bank for International Settlements bulletin issued yesterday 2 which states that decentralized blockchains suffer from “inherent limitations” that prevent them from harnessing network effects, and which thus prevent crypto from “fulfilling the social role of money.”What is your view of this report’s conclusions?']"
                Research Summary: Using Distributed Ledger Technologies to Improve and Maximise the Collection of Property Taxes              ,https://www.smartcontractresearch.org/t/research-summary-using-distributed-ledger-technologies-to-improve-and-maximise-the-collection-of-property-taxes/1176,Governance and Coordination,57,['https://ssrn.com/abstract=3899060'],['summary'],"['TLDR:Developed countries collect six times the percentage of property taxes (PT) by GDP (2.2%) that developing countries do (0.38%).These figures could be higher, but fraud, corruption and incapacity, among other challenges, cause leakages.Distributed Ledger Technologies (DLT) could help maximize the collection of PT due to features like transparency, traceability, verification and automation.Core Research QuestionCan DLT be used to maximize the collection of property taxes? In what ways can they be used or have there been any case studies in which the technology was applied?CitationNicolaou-Manias, Kathy and Wu, Yuchen, Using Distributed Ledger Technologies to Improve and Maximise the Collection of Property Taxes (August 4, 2021). Available at SSRN: SSRN Electronic Library 5 or SSRN Electronic Library 3BackgroundDistributed Ledger Technologies (DLT): A database network where participants can read and write the state of a database from any location, at any time.Tax: A major source of revenue for governments. Governments levy taxes, which are mandatory payments on individuals, businesses and goods and services.Property Tax (PT): A tax levied by governments on properties, often landed properties, on their use or transfer.SummaryDeveloping countries only collect a fraction of PT (0.38% of GDP) when compared with developed countries (2.2% of GDP).Property taxes are generally seen as good taxes.A good tax is equitable, fair, easily enforceable and involves low administrative costs. Bad taxes cause inequities, are largely unenforceable and involve high administrative costs.Tax evasion resulting from inequity turns property taxes from a good tax to a bad tax. The use of real estate for money laundering contributes to this.Price manipulation, undeclared transactions, and using corporate vehicles to conceal identities are ways governments can lose out on PT revenue.At the centre of PT collection is the deeds registry.The deeds registry is usually a government agency that serves as a single source of truth for recording a real estate transaction.In many developing countries the deeds registry is an opaque system, which promotes tax evasion.DLTs like blockchains can assist governments in achieving transparency.Blockchain’s features, such as peer-to-peer networks, traceability, decentralization and automation make this transparency possible.As property tax collections increase, “corporate income tax, personal income tax, capital gains and inheritance tax” collections will also increase.PT collection uses sensitive personal and national data, making the selection of an appropriate type of blockchain crucial.The authors emphasize a blockchain’s ability to securely record transactions, transfer value and validate ownership in a way that is traceable and equitable.Blockchains come in different varieties: permissioned and permissionless; what distinguishes the two is whether access is restricted (the former) or open (the latter).Due to the sensitive data involved in PT collection, permissioned blockchains are recommended.The authors further describe the use of permissioned DLT for identification, citing benefits such as privacy, security, convenience and accurate tracing.On the other hand, while permissionless DLTs could be cost-effective, low patronage makes adoption difficult.The advantages of using a blockchain for PT collection includes privacy for Politically Exposed Persons (PEPs), real-time information exchange, traceability of transactions, and cost reduction.The disadvantages of using a blockchain for PT collection include difficulty in scalability and interoperability, the potential for crypto forensics to pierce the privacy veil; the more permissioned the blockchain, the less secure it is; governance challenges; and high transition costs.Deeds registries are characterized by heavy dependence on paper, they are time-consuming to use, potentially corrupt, and thrive in informal sectors. Verification and traceability can be difficult and cause massive disputes.The following intermediaries are involved in a traditional transfer of landed property.The seller, who wants to sell the property.The buyer, who wants to buy the property.The agent who is a neutral third party who helps facilitate the transaction.The conveyancer who is usually a lawyer who helps draft the relevant legal documents.The buyer’s bank, which sends the consideration for the sale to the seller’s bank.The seller’s bank, which receives the consideration for the sale from the buyer’s bank.The deeds registry, which is usually a government agency that serves as the single source of truth for recording the transaction.The authors describe the following process for using DLT for conveying property digitally:The buyer and seller reach an agreement relating to a property transaction.Transactions are sent to a peer-to-peer network.The network validates all related information.Upon validation, value transfers are made. Property ownership is transferred to the buyer, and the balance is transferred from the buyer’s wallet to the seller’s wallet.The value transfers result in a transaction record on both wallets.The transaction record serves as a digital certificate with cryptographic proof (hash).The following needs to be housed in the DLT property wallet:Ownership information.A unique identification number.Address and parcel or erf number.Geo-spatial information.Cadastral zoning classification.Physical characteristics of the property (size linked with geo-spatial coordinates).When all this information linked to the property, estimation and annual adjustment of PT becomes easy and transparent.MethodThe authors employ a qualitative approach using a literature review to identify the challenges associated with increasing the equitable collection of property taxes.Literature reviews were also used to highlight the embedded features of blockchain and discuss how its application could increase PT collection.ResultsBlockchains offer security, privacy and transparency, which can be useful when there is a need to verify and trace beneficial ownership of assets.2016: Delaware’s Blockchain Initiative allows corporations to register and transfer shares on a blockchain.2016: Georgia transitioned to a blockchain-based registry, becoming the first country to register property on the blockchain.2016: BitLand, BenBen, IBM and Seso Global begin working with the Ghanian government in different capacities to register land on the blockchain.2016: After an initial test, the Swedish Land Registry, Lantmäteriet, begins using a private blockchain for land and property registration.2017: With a pilot in Pelotas and Morro Redondo, Brazil collaborates with Ubiquity for its blockchain property deeds.2017: Pilot trials of blockchain use in land registration are announced by the Ministry of Economic Development of Russia.2017: The Dubai Blockchain Strategy is released. The Dubai Land Department (DLD) conducts land-related transactions from start to finish using the blockchain.2017: The first pilot program to transition Ukraine’s State Land Cadastre onto a blockchain begins with a partnership with BitFury.2017: Chicago sees the successful pilot of a blockchain property record by velox.RE and Chicago’s Cook County Recorder of Deeds (CCRD).2018: The state of Andhra Pradesh partners with Zebi to digitize land records on a private blockchain. Earlier in 2017, the Telangana government sponsored a blockchain registration platform with a pilot in Hyderabad.2018: A MoU signed by Rwanda Land Management and Use Authority (RLMUA) and the Rwanda Information Society Authority (RISA) with MLG leads to Ubataka, a blockchain land transaction platform. Ubataka is paperless and uses Public Key Infrastructure (PKI) and biometrics for authentication and verification.2018: A successful pilot of blockchain property records in Zambia begins through an MoU with MLG and the Ministry of Land and Natural Resources.2019: After free pilot programs based on an MoU between the Liberia’s Ministry of Finance and Development Planning and Medici Land Governance (MLG), the Liberia Land Authority (LLA) launches.2019: Malta moves all rental contracts onto a blockchain.2019: Working with the Nigeria Mortgage Refinance Company and the Nigerian Ministry of Finance, Seso Global creates a blockchain-powered marketplace for real estate transactions in Nigeria.2019: The Khayelitsha pilot in Cape Town for a blockchain property registry launches through a partnership amongst the Centre for Affordable Housing Finance in Africa (CAHF), 71point4 and Seso Global.2019: MLG and the government of St. Kitts and Nevis sign an MoU to develop a blockchain-based “cadaster system”.2019: Her Majesty’s Land Registry (HMLR) supported by R3 Corda and Instant Property Network (IPN) uses a blockchain prototype in a house sale.2020: In Afghanistan, the United Nations Human Settlements Program (UN-Habitat) and the UN Office of Information and Communications Technology (OICT) launch goLandRegistr that integrates with the Afghan Land Authority via an API.Although not blockchain-based, Estonia has a fully functional and efficient e-Land registry. However, other sectors like health, justice and business are secured with blockchains.Discussion and Key TakeawaysThe main hurdles for African countries in the use of blockchains for PT are data collection and quality. The cost of data collection is quite high, but fortunately, it is a one-time expense with a high long-term return on investment.Additional immediate benefits include:Lower costs.Fraud and corruption detection.Seamless information sharing.Auditability.Security.Sped up processes.Minimized disputes.Backup.Transparency.Fairness and accountability.E-government readiness.Generally, governments use blockchains for notarization, shared databases and workflow automation.To better capture the benefits of blockchains beyond PT, the authors suggest African countries should do the following:Create an environment with the right political will, awareness, organization, policy, regulatory oversight and procedural frameworks.Facilitate technical capacity building, including digital infrastructure.Reconstruct governance systems that promote stakeholder inclusion.They consider two types of blockchains, public and private, for registries. A safer strategy is to first incorporate before considering replacement.The diverse implications of public and private blockchain use for African countries makes a consortium blockchain a better choice.Governments using blockchains can be categorized under the following standards:Basic Standard: Contracts are housed electronically. Afghanistan, Liberia and South Africa follow a basic standard.Silver Standard: Contracts are housed electronically and use automation and verification. Brazil, Estonia, India, and the Netherlands use a silver standard.Gold Standard: Contracts are housed electronically in an automated and verifiable way and use tokenization. Nigeria, Rwanda, Ukraine and the United States are at a gold standard.Platinum Standard: Contracts are housed electronically in an automated and verifiable way and use tokenization, in addition, there is further tokenization of property-related utilities and services like water and electricity. St. Kitts and Nevis, the United Arab Emirates, the United Kingdom and Zambia are at a platinum standard.Implications and Follow-upsConsidering the four phases of technology adoption listed above, the authors suggest African countries begin their blockchain adoption from the basic standard and gradually scale up to a platinum standard.The main challenges with leveraging blockchain for PT include a steep learning curve, technical know-how, maintenance and interoperability.User education can be a challenge.ApplicabilityTaxation plays a huge role in nation-building.Good taxes empower citizens to keep their government accountable and transparency allows them to keep their government’s spending in check.Blockchains are ideal for minimizing tax evasion.The authors recommend that African governments start from a basic standard and scale-up.', '@Fizzymidas have there been any comparisons between collection rates in developing countries that have adopted blockchain registries vs. those that haven’t? It would be interesting to see. @Astrid_CH what type of registry does Taiwan use for tracking property? What do you think of this idea?', 'That’s an interesting question.At the moment, I do not think there is sufficient data available from which to draw such insights. Reasons include, while some pilots were successful, for some countries, adoption was never fully implemented. For instance, in Nigeria and Ghana. Also, most countries lacked the digital infrastructure essential for successful full adoption.Let’s have a few more years, and we could do comparisons.', 'Thanks @Fizzymidas for a very interesting research summary.I have no doubt that blockchains for recording property ownership could offer many benefits to developed as well as developing societies–assuming that the people involved are honest. But human beings have always brought great ingenuity to tax evasion in particular, and even the most watertight blockchains won’t be a magic bullet for collecting taxes if the (often very wealthy and powerful) property owners are determined to avoid it.Your summary says that “blockchain’s features, such as peer-to-peer networks, traceability, decentralization and automation” can “assist governments in achieving transparency.” Among the “immediate benefits” that you cite for Distributed Ledger Technologies are “fraud and corruption detection.”But you also state that “price manipulation, undeclared transactions, and using corporate vehicles to conceal identities are ways governments can lose out on PT revenue.” How will those sorts of deceptions be cured by DLT systems that in the end contain only data that people choose to reveal?', 'Thank you for this question @rlombreglia, it highlights one of the major challenges with using technology, including blockchain. Because garbage in garbage out (GIGO).One measure for mitigating this can be putting in place policies and measures for data quality management. For instance, requiring a certain percentage of network participants to verify data. In addition, people could also be incentivized to comply.', '@Fizzymidas Thank you for the wonderful summary, the list of use cases is very useful for doing related research.@jmcgirk Thanks for your question. In Taiwan, we usually take papers such as contracts and land certificates to the deeds registry to process land transactions. Then the deeds registry records the information on the electronic system, so we can search for the latest information about the property state. I like the idea to leverage DLT in the process, but I also have concerns about the privacy issues.  @Fizzymidas Are there any details about how it deals with privacy issues? Would it become a problem to fit GDPR requirements? On the other hand, I think NFT could be used as one type of certificate in the process. Does the paper mention anything about it?', 'I have been trying to think about how to approach this post due to my experience with this issue and want to raise a few questions that you might be able to help answer:If the problem with the current systems is a lack of transparency in general due to malicious government actors, how does the technology get implemented if those malicious actors are still present?In other words; if this technology is meant to aid in fraud prevention by proxy of transparency, then what is the logical course of implementation if the malicious actors are still present and thus have the capacity to block implementation?', '@Fizzymidas did you see @Larry_Bates’s questions for you?', '@Fizzymidas I really appreciate this summary! FYI - I couldn’t access the paper through the citation link. It says it was removed.From a political science perspective:One of the main questions I had was, what are the other factors decreasing property tax collection in developing countries? I’m wondering what percentage of the outcome (low property tax collection) is tax evasion vs something else like economic conditions (do folks have cash on hand to pay taxes?), enforcement from the government (if no one comes to collect your taxes or there are no consequences, why pay your taxes at all?), or even private land ownership rates (private owners pay taxes, the government does not). There are some assumptions about governing in this study that I’m not sure were addressed.From a technical perspective: Fizzymidas:PT collection uses sensitive personal and national data, making the selection of an appropriate type of blockchain crucial.My concerns are with privacy and security of the system. I honestly wonder if paper becomes more secure in digitization. Can anyone address security at scale for blockchain systems that seek to become nation-level infrastructure? What really has to be considered here?', '@Fizzymidas, I found this summary highly helpful as it gives an overview of the solutions being provided in creating land registries with DLT. The challenge is that, as @rlombreglia and @Larry_Bates point out, a blockchain solution doesn’t solve the core administrative issues, and there is little incentive for the adoption of a more transparent system by the current administrators.However, administrative challenges are only part of the problem. Another challenge is accessibility. Difficulties in accessing property records (high cost of due diligence) contribute to the prevalence of fraud and property-related litigation. An easily accessible property records database could solve a large part of the issue. Luckily, private companies/projects (like VerifyPro 1 – a personal project) are pushing into this area and trying to build businesses around property validation. The goal, I suppose, is to make transparency profitable and incentivising.A blockchain isn’t strictly required in implementing this. The company I know implementing a blockchain does it as an added feature to boost customer confidence in title documents they help validate.', 'Brilliant Summary @Fizzymidas.Reading the summary, I would love the answer to this Larry_Bates:If the problem with the current systems is a lack of transparency in general due to malicious government actors, how does the technology get implemented if those malicious actors are still present?I am not sure if blockchain technology is really needed to improve and maximize the collection of property taxes at this time.  Check @Tony_Siu’s Research Summary: Do you need a Blockchain? to confirm. My primary concerns with integrating blockchain for the collection of property taxes in Africa include the cost of integrating, the high barrier to educating actors, and blockchain’s scalability challenge.I agree that your suggested takeaways are needed to improve the collection of property taxes in Africa. To add to it, Incentive mechanisms should be used to reward good actors and punish bad actors Fizzymidas:Create a conducive environment with the right political will, awareness, organization, policy, regulatory oversight, and procedural frameworks.Facilitate technical capacity building, including digital infrastructure.Reconstruct governance systems that promote stakeholder inclusion.', '@Austin_jul what did you think about the idea of registering property on a blockchain specifically? (So it would replace a land registry) I couldn’t think of a reason why it wouldn’t work, other than there being some complexity in inputting and updating the system, and the third party risk of a government didn’t do it themselves.', 'Exactly @jmcgirk. The idea of registering property on a blockchain specifically could open up the economies of developing countries due to the transparent nature of blockchain and the reduction of the use of TTPs. I scanned through countries that are already using blockchain to register properties and I saw the immense potential of blockchain technology in the real estate sector.However, the barrier of entry is rather high for developing economies - a massive education campaign is needed to kick this off the ground because writers are not educated to write on the blockchain platform. For now, the benefit of using TTPs in the real estate sector of developing economies is greater than the benefit of removing TTPs from the Mix. Nevertheless, This Benefit ratio could change in a few years.', 'Having worked on this specific problem, it is WAY more complicated than it appears.  There are many stakeholders, and even beyond that the notion of “Trustless” systems do not resonate well with people who have never had a functioning government that they can trust.  I am starting to realize that the capacity to “trust” things will work is a luxury that many have taken for granted.Even though Bitland was named one of Time Magazine’s “Genius Companies of 2018” the actual “cryptocurrency community’s” response was “why wasn’t it Ethereum?”Many in the space either did not understand the potential for land registries years ago, or pretended not to understand to try and position maximalist frameworks as the only viable solutions.  Many of these approaches were entirely disconnected from local cultures and treated all developing regions as “the same” for the barrier of entry.  Effectively “why don’t we just slap a blockchain on a map and replace their registry?” has been a lot of the framing for approaching developing countries for almost a decade now.I can tell you, it’s WAY more complex than even having the technical side solved.  Assuming the “technical” aspects are perfect, actual implementation becomes another issue on its own that there haven’t really been any viable solutions to be presented from 100% private sector solutions (As far as I know).  As @Austin_jul pointed out, the education on the ground might legitimately be the biggest barrier to entry into any market.  Further, that education may need to take a dedicated form depending on the national or regional culture and whether that population is primed to accept a general digital framework or whether it needs a more region-specific framework.As Nigeria is one of the countries in which I worked 1, I can say there were definitely Ethereum maximalists framing the conversations in ways that made the government hesitant and wary to adopt any blockchain technology at all.  The education problem is most certainly one of the biggest hurdles to adoption, and a lot of that comes from maximalists presenting frameworks that put the government or local population on guard.', 'That’s a vital question @Astrid_CH! And this applies not just to using blockchain for PT, but to the general blockchain ecosystem.The only reference the authors make to privacy issues was recommending the use of a permissioned blockchain as opposed to a permissionless blockchain. This is because permissioned blockchains are a better fit for the GDPR in terms of compliance.The question of GDPR-compliance for blockchains will depend on blockchain type and use case 1. GDPR compliance is technology-neutral, the focus is not on the technology but on its application.With the use of permissioned blockchain for PT and with the aim of achieving GDPR-compliance, personal data can be stored off-chain. Data could also be programmed to delete in the off-chain storage after the blockchain achieves a certain block height 1.More information in this IBM+IAPP research: https://iapp.org/media/pdf/resource_center/blockchain_and_gdpr.pdfNo, the paper did not mention anything about NFTs. That’s an interesting use case, which is supported by my next research summary still in the pipeline, should publish once editing is completed. The authors of that paper propose NFTs as patents. As NFTs are a good fit for uniqueness and authenticity, they could be considered for certificate issuance on the blockchain.At the moment though, most people consider NFT as an add-on in property related transactions. More like getting an NFT after a property transaction for societal displays.', 'The million-dollar question! I believe this is where a strong political will comes into place, and also the reason the authors mention a weak political will as a challenge to implementing these types of changes.For instance, in the whole country, only the state of Lagos (and Abuja) in Nigeria has a partly digitized land registry. Even that is not open to the public and requires showing up physically at their office to access digital records. Lagos state is known for being innovative and past and current governors generally have a strong political will to implement change.Another instance of a strong political will is the implementation of the Treasury Single Account 1 (TSA) in Nigeria as recommended by the International Monetary Fund (IMF). TSA received stiff opposition from its proposal until implementation because it meant drying up revenue streams for corrupt civil servants. Implementing TSA helped Nigeria recover over 2 trillion naira.If more states (and countries) develop a strong political will, we will see more progress in implementing some of these corruption-curbing solutions, including blockchain. It might be slow, but we will eventually get there.Also, citizens have a role to play here. And this is where sensitization and enlightenment are essential. When citizens are properly informed, they become empowered and can demand good governance from their leaders. A determined government with a strong political will can achieve anything.I hope I haven’t turned this into a political discourse. ', 'yes, finally got around to replying', 'Thank you for the comment @valeriespina and apologies for only just responding.I am not sure why the authors removed the paper from SSRN, This has motivated me to try reaching out to them again via email as I couldn’t find them on any social media platform while writing the summary.You are right that the authors did not go in depth on the socio-economic issues you raised. In most developing countries, at least for Nigeria, often, property taxes are levied only in urban areas and on commercial properties. For instance, a landlord not living with the tenants pays lesser than a landlord living with tenants. Also, most landlords simply transfer the costs to tenants.On your privacy question, I reference it in my answer to @Astrid_CH. As much as there is a need for transparency, personal data also needs to be protected. The authors suggest the use of a permissioned blockchain and anonymization of data to help preserve privacy.Digitization is often aimed at portability, durability. ease of access and speedy retrieval. Whether it is secure will depend on the underlying infrastructure. As such, some considerations would include confirming that the infrastructure used meets the CIA triad standards: confidentiality, integrity and availability. Blockchain (permissioned) is able to satisfy the CIA triad.', 'Thank you @tomideadeoye for engaging!If we follow this rubric as explained in @Tony_Siu’s summary “Do you need a blokchain?”, the key point to determine is, “can you use an always online trusted third party?”. Using Nigeria as a case study, the answer to this is NO. The trusted party here (Land Registry) is mostly offline, and in states (Lagos and Abuja) where they are partly digitized, access is still restricted.As further explained in the summary, the Land Registry can function as a certificate granting authority that also verifies blockchain writers, in this case, local government authorities. This is important because, as you rightly identified, one huge challenge is high cost of due diligence. This high cost could greatly reduce by involving local government authorities and in turn making them blockchain writers.Often companies implement blockchain when involving tokenization.', 'Thank you for enaging @Austin_julI have responded to Chris here.As mentioned in my response to @tomideadeoye above, I am of the opinion that blockchain is needed. This does not mean that blockchain must be used.']"
"                Project: Building culture in decentralized organizations, intentionally and systematically              ",https://www.smartcontractresearch.org/t/project-building-culture-in-decentralized-organizations-intentionally-and-systematically/1145,Governance and Coordination,30,[],"['discussion', 'governance', 'summary', 'dao']","['tl;dr: “Culture” is mysterious and goopy, but valuable and important in decentralized organizations. By taxonomizing the systems of people who build it for a living, we can help communities be more intentional about building shared meaning together.Culture is the secret sauce of organizations: the half that you can’t just fork. This is because how an organization performs depends just as much on the behaviors that are incentivized as those that people internalize. You’d think DAOs would have changed that. Mechanized trust could have eliminated the need for all the goopy stuff that makes traditional organizations so hard to build: norms, values, human trust, tacit knowledge. But are you really going to be surprised if it turns out that culture matters, that organizations, centralized or not, are more effective when their members feel aligned with others, connected to them, recognized and trusted by them, heard by them, when they are constantly in touch with how they fit into a bigger system that does something they care about? As DAOs get ever better at pinning down the formal dimension of accountability, this informal side will become an ever greater fraction of the mystery of decentralized organization design.We’re Seth Frey and Taylor Ferrari. Seth is a Prof at UC Davis, trained in cognitive science and data science, and specializing in the study of online governance systems. He does that work mostly with large data sets and fun computational methods, but recently with goopier approaches like surveys and interviews. Taylor is a consulting researcher and strategist trained in anthropology and design. She specializes in qualitative data collection and strategy development across public and private organizations. We’re working with SCRF support to aggregate the concrete practices and processes that traditional organizations use to build strong cultures. In this project, we’re beginning from a faith claim that internalized values and behaviors matter for DAOs. It could be wrong. We may be attaching too much importance to culture. If so, we’ll all see. In the meantime, let’s take the challenge at face value.Culture, for our purposes, is the set of values, mental models, and behaviors that are widely internalized by members of a group. Culture building is an organization getting itself to a point where many members have done that internalization. It matters because following rules you believe in gives you another consensual route to complex collective initiative.Organization and mechanism designers currently have great tools for all the formal structure side of building an organization: review systems, accountability processes, supply chains, and so on. The tooling for culture building is, well: is there any tooling exclusively for culture building? With a better understanding of how groups get there, we can start to say what those tools might look like, and what they’d need to succeed.It can be tough to root up the internalized dimensions of an organization, but the “secret sauce” metaphor might be holding us back, giving culture a little too much mystique for our own good. There’s a fair chance that it’s mysterious not because it’s hard, but just because it’s a lot of work.What’s the work of culture building? If we could talk to dozens of experts, compare and group their insights, and translate them for decentralized organizations, could we get a concrete set of practices that anyone can follow? It’s not so far-fetched.So that’s what we’re up to. We’re interviewing and studying on-the-ground culture building professionals who have taken a systematic approach across organizations, to distill the specific practices they use, and the purpose of those practices. We’re doing it by reviewing and taxonomizing dozens of books by professional organizational culture consultants, and with interviews of some of those same professionals. These include consultants for traditional hierarchical organizations, large and small, but also culture building professionals who specialize in the more value-heavy non-profit sector, who specialize in supporting collaborations and networks, and those who specialize specifically in non-hierarchical organizations. We’ll then aggregate, organize, and pass it around for use by people who run decentralized and non-hierarchical organizations: DAOs, volunteer-run organizations, collaborative networks, cooperatives, and collectives.We’ll be sharing what we find as we go. If you have questions or comments, we’re all ears. Thanks for your interest!Seth Frey (Home 4, Twitter 6)Taylor Ferrari (Home 4, Linkedin 3)', 'Can you share some details about the cultural consultants you’re planning to contact / engage with? What type of backgrounds do these culture builders come from? Are they generally tech types who’ve come from other projects or are they new to web3?', 'This is a really exciting project!  I am looking forward to seeing this and all the resulting data.', 'Hi!  Thanks for engaging! We’ll be asking permission to name them and their firms as we work down our list—this gets into IRB issues—but in the meantime, I’m happy to share our current reading list (bottom). We’ve been fairly deliberate in not selecting tech/web3 folks. For the most part we’re seeking people who work in the consulting industry for traditional organizations, not-for-profits, and industry networks and collaborations, with a focus on culture building.  We’re targeting them because it pays, meaning that it’s possible to find people who have served dozens of organizations with a relatively systematic, if not uniform approach. Part of our work will be adapting those insights from traditional orgs for decentralized/web3 communities.  Our presumption for now is that these skills will translate, particularly because culture building, even in a hierarchical organization, tends to be pretty bottom-up.  That presumption, of course, will be tested as we proceed, but it is one of the things we’ll be running by our participants as we talk to them.', 'That sounds interesting! The idea to question consultants is quite interesting. What do you think you’ll find there that we might not find in the academic literature on the question? What do you think consultants might see that anthropologists and economists will miss?My other point is more nitpicky, but it’s the object of your work, so it might help thinking about it: your definition of culture doesn’t seem to restrain itself to the goopy stuff. Indeed, smart contracts are culture (of your kind) at work—they rely on and underlie shared values, they document and disseminate shared mental representations and they describe and impose behavioural norms. As a result, I’m not really sure what the goopy stuff is. Is it things that rely on non-economic values (things like honor and care and loved ones, whose value are independent of markets)?', 'Thanks so much! Us too!', 'That sounds interesting! The idea to question consultants is quite interesting. What do you think you’ll find there that we might not find in the academic literature on the question? What do you think consultants might see that anthropologists and economists will miss?We’re targeting the experiences of consultants as “data” because our question is a “How do you make culture?” question. I’ve found that the academic literature is more descriptive: “What are the types of culture? What are the dimensions of cultural variation?” There’s a bit of “What are the properties of cultures that stick around a long time?” that I’ve found in anthro but where it really gets into practice is in the business/management literature, particular the subset by people who do consulting.  The rationale is ultimately the same as any other qualitative deep dive.  If you want to understand a cultural phenomenon (or in our case “meta-cultural”: creation of culture) then you should talk to people who have experienced it.', ' locha:My other point is more nitpicky, but it’s the object of your work, so it might help thinking about it: your definition of culture doesn’t seem to restrain itself to the goopy stuff. Indeed, smart contracts are culture (of your kind) at work—they rely on and underlie shared values, they document and disseminate shared mental representations and they describe and impose behavioural norms. As a result, I’m not really sure what the goopy stuff is. Is it things that rely on non-economic values (things like honor and care and loved ones, whose value are independent of markets)?Not nitpicky at all, certainly meaty. I also don’t know what the goopy stuff is.  It certainly is where I’d guess that a lot of the non-economic component is being kept, so if we’re on board that non-economic dimensions matter in an org, then we’re on board with culture (which I didn’t highlight: thanks for pointing it out).I don’t know if I fully agree with your point about smart contracts already being culture. Literal smart contract artifacts are in code not in heads. If you make it about “effective smart contracts” then I agree almost completely: our premise is that formal organizational structures, to be effective, need a symbiosis with informal (i.e. internalized) structures. But that depends on a sort of s/goopy/effective/ switcheroo.they rely on and underlie shared values, they document and disseminate shared mental representations and they describe and impose behavioural normsI don’t think externalizing the internal is the same thing as internalizing. With your picture, smart contracts can give a community a representation of what we’re calling culture, but that’s not the same as being culture or a substitute for it. A little bit of what our definition is is behavioral, and it allows that behavior either came from external constraints or internal drives. Relying on, underlying, documenting, and disseminating are effective with internal drive in place, but are not a cause of it, and imposing is external.Let me know if I’m getting your point.  I do a lot of realizing what I think in exchanges like this so more is welcome.', 'This is super interesting! Welcome to SCRF.I worked as an internal consultant for several years doing exactly this: creating large-scale/transformational culture change. If it helps, there is a growing field of change management practitioners that focus on the ‘people side of change’. Many are now certified with a certification through Prosci Change Management. This might be a good pool to find some participants for your study. I would think that active, full-time consultants may not be able to share significant information with you due to NDAs and how tight consulting firms hoard information (that’s literally their business model).I’m also very interested in how online communities in general (and now the DAO) create culture. I don’t believe that the DAO is unique in its need for culture, and digital social groups will teach DAO organizers a lot…i.e., they don’t need to reinvent the wheel (of online culture).', 'Hi Seth,Interesting angle in a tech heavy world I assisted Commons Stack community in a “cultural build” research exercise for the token engineering commons launch. Happy to speak more about that experience with you or point you to the ongoing leads of the initiative within CS/TECKindlyKelsie', ' enfascination:We’ll be asking permission to name them and their firms as we work down our list—this gets into IRB issues—but in the meantime, I’m happy to share our current reading list (bottom).I don’t see the reading list anywhere. Could you give an explicit pointer? enfascination:We’re interviewing and studying on-the-ground culture building professionals who have taken a systematic approach across organizations, to distill the specific practices they use, and the purpose of those practices. We’re doing it by reviewing and taxonomizing dozens of books by professional organizational culture consultants, and with interviews of some of those same professionals.My instinct is that your hands-on ethnographic work will be more interesting than the “reviewing and taxonomizing dozens of books by professional organizational culture consultants,” which sounds like a vastly dull job, though I suppose someone has to do it.You hypothesize that “the ‘secret sauce’ metaphor might be holding us back, giving culture a little too much mystique for our own good,” and yet your project description starts with the very convincing assertion (unless I’m missing an irony somewhere) that “Culture is the secret sauce of organizations: the half that you can’t just fork.”I’m not so sure about secret sauce being an expendable part of the equation. I think it’s a mysterious and real thing. In fact, when I read your project summary I couldn’t help but think of Steve Jobs, whom I realize is a cliché today, as well as a reputedly Bad Man who should be cancelled. But I think he understood the subject you’re researching very well. Whatever people may say to dismiss him, Jobs built a great (or at least once-great, while he was alive) tech company precisely by not turning his back on fine art, fine design, fine manufacturing standards, etc., but rather by embracing and insisting on those things.That still sounds like an oxymoron, doesn’t it? I think so, particularly in the eyes of all those professional culture-builders who consult to the Microsofts, Facebooks, Amazons, and other dull companies of the world. Maybe those well-educated, not-stupid consultants you’re reading actually know interesting things. But their clients won’t change their behavior one whit based on any of it.I think Apple’s culture in its heyday was a once-in-a-hundred-years phenomenon, driven by the weird, vital, willed, personality-based magic that lies at the heart of the “secret sauce” metaphor. The fact that such companies can’t continue unchanged after the departures or deaths of their founders seems to testify to that.', '@rlombreglia @enfascinationYou hypothesize that “the ‘secret sauce’ metaphor might be holding us back, giving culture a little too much mystique for our own good,” and yet your project description starts with the very convincing assertion (unless I’m missing an irony somewhere) that “Culture is the secret sauce of organizations: the half that you can’t just fork.”I personally have never been a fun of the concept of “organizational culture”. Research on organizational culture dates back at least as far as the late 1970s. A search in ABI/INFORM on the term organizational (or ‘corporate’) culture in the abstract yields more than 5,300 results of peer- reviewed papers published in scholarly journals, spanning a period of five decades. Although references to organizational culture are found in both popular management books and the academic literature, there does not seem to be a sharp, accepted definition of the concept. In fact, despite the large number of academic publications, there is no clear consensus of what ‘organizational culture’ entails.One of the most widely used definitions is provided by Edgar Schein (2004), who describes organizational culture as“A pattern of shared basic assumptions learned by an organization as it solved its problems of external adaptation and internal integration, which has worked well enough to be considered valid and, therefore, to be taught to new members as the correct way to perceive, think, and feel in relation to those problems.”The fact that Steve Jobs is quoted in this conversation is spot on, albeit I would take off the mysterious and the magical to talk about the real.@rlombregliaI’m not so sure about secret sauce being an expendable part of the equation. I think it’s a mysterious and real thing. In fact, when I read your project summary I couldn’t help but think of Steve Jobs, whom I realize is a cliché today, as well as a reputedly Bad Man who should be cancelled. But I think he understood the subject you’re researching very well. Whatever people may say to dismiss him, Jobs built a great (or at least once-great, while he was alive) tech company precisely by not turning his back on fine art, fine design, fine manufacturing standards, etc., but rather by embracing and insisting on those things.Jobs understood early on in his career most of the principles of scientific management from the father of the quality movement Joseph Juran. He talks about it extensively in the interview Steve Jobs on Joseph Juran and Quality. Apple’s success perhaps is nothing more than the application of the science of management dressed with the taste of beauty (this one borrowed from Adriano Olivetti).Olivetti728×400 81.9 KB@rlombregliaI think Apple’s culture in its heyday was a once-in-a-hundred-years phenomenon, driven by the weird, vital, willed, personality-based magic that lies at the heart of the “secret sauce” metaphor. The fact that such companies can’t continue unchanged after the departures or deaths of their founders seems to testify to that.So I won’t go into the mystical, great man theory when explaining the incredible Apple’s culture. This conclusion in fact reminds me of fallacies extensively discussed by Phil Rosenzweig in The Halo Effect: And the Eight Other Business Delusions That Deceive Managers. Instead, Apple’s pattern of shared basic assumptions perhaps simply involved the understanding that repetitive processes + objective observation + decomposition and optimization and its application to the business at 360 degree is the secret sauce of success. Which then is not secret anymore.After all, what Jo Juran proposed is nothing else than a mechanism mimicking nature’s Principle of Least Action for which everything moves saving the maximum amount of energy (= efficiency). So in my opinion yes, culture’s secret sauce metaphor is definitely holding us back. We should look back to nature instead and mimic its real, tangible genius.']"
"                Research Summary: Arbitrum: Fast, Scalable, Private Smart Contracts              ",https://www.smartcontractresearch.org/t/research-summary-arbitrum-fast-scalable-private-smart-contracts/885,Scaling,46,['#links-4'],"['scalability', 'summary', 'layer-2', 'rollups', 'iot', 'oracles', 'zero-knowledge', 'dex']","['TLDRThis paper (and whitepaper) describes an Optimistic rollup, an L2 scaling solution for smart contracts that recently launched on the Ethereum blockchain. The protocol uses mechanism design to incentivize an agreement off-chain on the VM execution. Malicious behavior is penalized by a loss of deposit after a challenge is resolved in a multi-round game. Any honest party can raise a dispute to correct potentially fraudulent activity in an efficient manner and advance the VM state on-chain.Core Research QuestionHow do we scale blockchains with smart contracts using an off-chain solution?Citation[PAPER] Arbitrum: Scalable, private smart contracts. Harry Kalodner, Steven Goldfeder, Xiaoqi Chen, S. Matthew Weinberg, and Edward W. Felten, Princeton University[WHITEPAPER] Arbitrum: Fast, Scalable, Private Smart Contracts. OFFCHAIN LABSLinksusenix.orgsec18-kalodner.pdf 15479.99 KBhttps://www.usenix.org/conference/usenixsecurity18/presentation/kalodner 3web.archive.orgarbitrum.pdf 23770.57 KBArbitrum’s Team Ask Me Anything on Reddit 2BackgroundLayer-2: collective term for solutions designed to help scale the transactional throughput of applications by handling transactions off the underlying blockchain (layer 1).consensus: protocol for users (nodes) to come to an agreement on the shared history of transactionsVerifier: It describes the underlying consensus use. Arbitrum is consensus agnostic and uses the term to express that they can work with permissionless or premissionned chains.verifiers: The term generically refers to the underlying consensus mechanism participants. For example, in the Bitcoin protocol, Bitcoin miners are the verifiers.validators/managers: A manager [PAPER] of a VM is a party that monitors the progress of a particular VM and ensures the VM’s correct behavior. When a VM is created, the transaction that creates the VM specifies a set of managers for the VM. A manager is identified by its public key. [WHITEPAPER] uses the term validators.VM: Taking the blockchain as an operating system, Arbitrum uses VM to refer to smart contracts/Dapps.The Arbitrum compiler [WHITEPAPER] takes a group of contracts written in Solidity, compiles and links them together into a single executable file that can run on the Arbitrum Virtual Machine (AVM) architecture.Ethbridge: Dapp running on Ethereum. Its job is to serve as a bridge between Ethereum and Arbitrum. Anyone in Ethereum-land can call the EthBridge to interact with Arbitrum-land, for example to launch an Arbitrum VM, to make a call to a contract running on Arbitrum, or to send ether or any token to an Arbitrum VM. The EthBridge’s other important job is to referee disputes between validators.AnyTrust guarantee: Arbitrum guarantees correct execution as long as any one validator of a Dapp acts honestly. Most other approaches require majority-honest or two-thirds-honest assumptions, or else require moving the entire state of a contract to the main chain in case of a dispute.bisection protocol: the dispute resolution protocol of Arbitrum between two validators, a challenger and an asserter, with the L1 as referee.SummaryWhy scaling is hardVerifier dilemma: The problem is the high cost of verifying VM execution. Either verifiers end up accepting the transactions without verifying them because they expect everyone will do the same or others will do the hard work in their stead. Or they honestly verify the incoming transactions which make them vulnerable to time-consuming computation that can slow them in the block producing race.participation game: A mechanism design approach that aims to induce a limited but sufficient number of parties to verify each VM’s execution. These systems face the Participation Dilemma, of how to prevent Sybil attacks in which a single verifier claims to be multiple verifiers, and in doing so can drive other verifiers out of the system. Prior works to Arbitrum fail to resolve this problem since smart contract verification is a repeated game and in those, there are numerous other equilibria that do not project onto Nash equilibria of their one-shot variants. For example, the case where only one player participates is a Nash-equilibrium.Arbitrum: An L2 protocol in the Optimistic Rollup family to improve scaling on an L1 blockchain like Ethereum.In Arbitrum, parties can implement a smart contract by instantiating a Virtual Machine (VM) that encodes the rules of a contract. The creator of a VM designates a set of validators for the VM.The Arbitrum protocol provides an AnyTrust guarantee: Any honest validator can force the VM to behave according to the VM’s code. Parties that are interested in the VM’s outcome can serve as validators themselves or appoint someone they trust to manage the VM on their behalf. Parties can send messages and currency to a VM, and a VM can send messages and currency to other VMs or other parties. VMs may take actions based on the messages they receive. The Verifier also tracks the hash of the VM’s inbox. Arbitrum also allows contracts to execute privately, publishing only contract states.Relying on validators, rather than requiring every verifier (here, miners) to emulate every VM’s execution, allows a VM’s validators to advance the VM’s state at a much lower cost to the verifiers. Verifiers track only the hash of the VM’s state, rather than the full state. Arbitrum creates incentives for the validators to agree on what the VM will do off-chain. Any state change that is endorsed by all of the validators will be accepted by the verifiers.If, contrary to incentives, two validators disagree about what the VM will do, the verifiers employ a bisection protocol to narrow the disagreement down to the execution of a single instruction, and then one manager submits a simple proof of that one-instruction execution which the verifiers can check very efficiently. The liar must then pay a substantial financial penalty to the verifiers, which serves to deter disagreements.The Arbitrum VM has been designed to make checking one-step proofs fast and simple. In particular, the VM design guarantees that the space to represent a one-step proof and the time to generate and verify such a proof are bounded by small constants, independent of the size and contents of the program’s code and data.Assumptions: It is assumed that users will only pay attention to a VM if they agree that the VM was initialized correctly and have some stake in its correct execution. By Arbitrum’s Anyrust assumption, parties should only rely on the correct behavior of a VM if they trust at least one of the VM’s managers. One way to have a validator you trust is to serve as a manager yourself.One key assumption that Arbitrum makes is that a validator will be able to send a challenge or response to the Verifier within the specified time window. In a blockchain setting, this means the ability to get a transaction included in the blockchain within that time. While critical, this assumption is standard in cryptocurrencies, and risk can be mitigated by extending the challenge interval (which is a configurable parameter of each VM).Two factors help alleviate denial of service (DoS) attacks against honest validators. First, if a DoS attacker cannot be certain of preventing an honest validator from submitting a challenge, the risk of incurring a penalty may still be enough to deter a false assertion. Second, because each manager is only identified by a public key, a validator can use replication to improve its availability, including the use of “undercover” replicas whose existence or location is not known to the attacker in advance. Lastly, a motivated malicious manager can indefinitely stall a VM by continuously challenging all assertions about its behavior. The attacker will lose at least half of every deposit, and each such loss will delay the progress of the VM only for the time required to run the bisection. We assume that the creators of a VM will set the deposit amount for the VM to be large enough to deter this attack.MethodVM lifecycle:552×736 17.3 KBThe Arbitrum protocol recognizes two kinds of actors: (public) keys and VMs. An actor is deemed to have taken an action if it is signed by the corresponding private key.An Arbitrum VM is created using a special transaction, which specifies the initial state hash of the VM, a list of validators for the VM, and parameters such as the length of the challenge period. The state hash represents a cryptographic commitment to the VM’s state.Once a VM has been created, validators can take action to cause that VM’s state to change. The Arbitrum protocol provides an AnyTrust guarantee: any one honest validator can force a VM’s state change to be consistent with the VM’s code and state, that is, to be a valid execution according to the AVM Specification.An assertion states that if certain preconditions hold, the VM’s state will change in a certain way. An assertion about a VM is said to be eligible if the assertion’s preconditions hold, the VM is not in a halted state, and the assertion does not spend more funds than the VM owns. The assertion contains the hash of the VM’s new state and a set of actions taken by the VM, such as sending messages or currency. Note that for each VM, the Verifier tracks the hashed state of that VM, along with the amount of currency held by the VM, and a hash of its inbox.Unanimous assertions are signed by all validators of that VM. If a unanimous assertion is eligible, it is immediately accepted by the Verifier as the new state of the VM.Disputable assertions are signed by only a single validator, and that validator attaches a currency deposit to the assertion. If a disputable assertion is eligible, the assertion is published by the Verifier as pending. If a timeout period passes without any other validator challenging the pending assertion, the assertion is accepted by the Verifier and the asserter gets their deposit back. If another validator challenges the pending assertion, the challenger puts down a deposit, and the two validators engage in the bisection protocol, which determines which of them is lying. The liar will lose their deposit.Bisection protocol: If a validator challenges an assertion, the challenger must put their deposit in escrow. The asserter and the challenger engage in a game, via a public protocol, to determine who is correct. The party who wins the game will recover their deposit, and will take half of the losing party’s. The other half of the loser’s deposit will go to the Verifier, as compensation for the work required to referee the game.The game is played in alternating steps. After a challenge is registered, the asserter is given a pre-specified time interval to bisect its previous assertion. If the previous assertion involves N steps of execution in the VM, then the two new assertions must involve ⌊N/2⌋\\lfloor N/2 \\rfloor⌊N/2⌋ and ⌈N/2⌉\\lceil N/2\\rceil⌈N/2⌉ steps, respectively, and the two assertions must combine to be equivalent to the previous assertion. If no valid bisection is offered within the time limit, the challenger wins the game. After a bisection is offered, the challenger must challenge one of the two new assertions, within a pre-specified time interval. The two players alternate moves. At each step, a player must move within a specified time interval, or lose the game.After a logarithmic number of bisections, the challenger will challenge an assertion that covers a single step of execution. At this point the asserter must offer a one-step proof, which establishes that in the asserted initial state, and assuming the preconditions, executing a single instruction in the VM will reach the asserted final state and take the asserted publicly visible actions, if any. This one-step proof is verified by the Verifier. See Figures.1098×1502 67.6 KBDescription of the Bisection Protocol from  [PAPER]The VM uses a stack-based architecture. Its state is organized hierarchically. This allows a hash of a VM’s state to be computed in Merkle Tree fashion, and to be updated incrementally. The VM architecture ensures that instructions can only modify items near the root of the state tree and that each node of the state tree has a maximum degree of eight.The state of a VM contains the following elements:an instruction stack, which encodes the current program counter and instructions;a data stack of values;a call stack, used to store the return information for procedure calls;a static constant, which is immutable; anda single mutable register which holds one value.1684×778 155 KBInstruction Stack: Arbitrum maintains an “instruction stack” which holds the instructions in the remainder of the program. To advance, the Arbitrum VM pops the instruction stack to get the next instruction to execute, halting if that stack is empty. Jump and procedure call instructions change the instruction stack, with procedure call storing the old instruction stack (pushing a copy of the instruction stack onto the call stack) so that it can be restored on procedure return. This approach allows a one-step proof to use constant space and allows verification of the current instruction and the next instruction stack value in constant time.A VM interacts with other parties by sending and receiving messages. A message consists of a value, an amount of currency, and the identity of the sender and receiver.Preconditions, Assertions, and One-Step Proofs: Each assertion is accompanied by a set of preconditions consisting of a hash of the VM’s state before the asserted execution, a hash of the VM’s inbox contents, an optional lower bound on the VM’s currency balance, and optional lower and upper bounds on the time (measured in block height). An assertion will be ignored as ineligible unless all of its preconditions hold. Still, parties may choose to store an ineligible assertion in the hope that it becomes eligible later.In addition to preconditions, an assertion contains the following components: the hash of the machine state after the execution, the number of instructions executed, and the sequence of messages emitted by the VM.A one-step proof, which is a proof of correctness, assuming a set of preconditions, for an assertion covering the execution of a single instruction. It must provide enough information, beyond the preconditions, to enable the Verifier to emulate the single instruction that will be executed.Because the state of the VM is organized as a Merkle Tree, whose root hash is given as a precondition, the proof only needs to expand out enough of the initial state Merkle tree to enable the Verifier to emulate the execution of the single instruction. It verifies that the result matches the claimed assertion.Messages are sent to a VM by users (with their keys) by putting a special message delivery transaction on the blockchain; and by other VMs using the send instruction. A message logically has four fields: data (an AVM value), a non-negative amount of currency and the identities of the sender and receiver of the message.A VM’s validators track the state of its inbox, but the Verifier need only track the inbox’s hash, because that is all that will be needed to verify a one-step proof of the VM receiving the inbox contents.ResultsScalability. This is the key feature of Arbitrum. Validators can execute a VM indefinitely, paying only negligible transaction fees that are small and independent of the complexity of the code they are running. If participants follow incentives, all assertions should be unanimous and disputes should never occur,but even if a dispute does occur, the Verifier can efficiently resolve it at little cost to honest parties (but substantial cost to a dishonest party).AnyTrust Guarantee. Arbitrum guarantees correct execution as long as any one validator of a dapp acts honestly—even if all of the other validators collude to (try to) cheat.Privacy. Arbitrum’s model is well-suited for private smart contracts. Absent a dispute, no internal state of a VM is revealed to the Verifier. Further, disputes should not occur if all parties execute the protocol according to their incentives. Even in the case of a dispute, the Verifier is only given information about a single step of the machine’s execution but the vast majority of the machine’s state remains opaque to the Verifier.Interoperability. Arbitrum is interoperable with Ethereum. A Dapp written in Solidity can be compiled using the open source Arbitrum compiler to generate Arbitrum-ready code. Users can also transfer Ether or any other Ethereum-based token back and forth between Ethereum and Arbitrum.Flexibility. Unanimous assertions provide a great deal of flexibility as validators can choose to reset a machine to any state that they wish and take any actions that they want – even if they are invalid by the machine’s code. This requires unanimous agreement by the managers, so if any one manager is honest, this will only be done when the result is one that an honest validator would accept–such as winding down a VM that has gotten into a bad state due to a software bug.Discussion and Key TakeawaysArbitrum is a L2 protocol that aims to fix many issues on L1, specifically on Ethereum. As it is today, Ethereum cannot scale because requiring miners to emulate every smart contract is expensive, and this work must be duplicated by every miner. It has no privacy which then has to come as an overlay. Solutions based on Zero Knowledge could help but they are expensive to run so the throughput would be limited to a few transactions per block. Finally there is an inflexibility of Ethereum-style smart contracts since deviation from the code is not possible. This is possible in Arbitrum as long as all of the VM’s honest managers agree to it.By making disputes relatively cheap to resolve, and imposing a substantial penalty on the loser, Arbitrum strongly disincentivizes attempts to cheat. Even if a dispute occurs this doesn’t impose a huge on-chain impact. In the common case, validators will agree and progress will occur off-chain, with only occasional touches to the main chain.To demonstrate Arbitrum’s efficiency, the authors measured the throughput of an Arbitrum VM which performs iterative SHA-256 hashing. They evaluated its performance on an early 2013 Apple MacBook Pro, 2.7GHz Intel Core i7. They were able to attain 970,000 hashes per second. Comparatively, using native code on the same machine, they made 1,700,000 hashes per second, while Ethereum is only capable of processing approximately 1600 hashes per second due to its gas limit.Arbitrum’s performance advantage extends further. The Verifier is capable of handling largenumbers of VMs simultaneously. Instantiating many copies of the Iterated Hashing VM, the authors measured that the Verifier node running on the test machine was capable of processing over 5000 disputable assertions per second. This brings the total possible network throughput up to over 4 billion hashes per second, compared to 1600 forEthereum.Implications and Follow-upsExtensionsZero Knowledge: Arbitrum provides privacy as long as none of the validators reveal the offchain data. If there is a challenge, they still have to reveal a small portion of the state (such as during the bisection protocol) which can contain sensitive information. The authors propose using a zero knowledge protocol to implement the one step proof. While zero-knowledge proofs could in theory be used to prove the correctness of the entire state transition (and not just a single step), doing this for complex computations was not feasible with the tools available at that time.Reading the base chain: As described, Arbitrum VMs do not have the ability to directly read the underlying blockchain. This could be easily solved by extending the VM instruction set to allow a VM to read the blockchain directly. To do so, the authors would create a canonical encoding of a block as an Arbitrum tuple, with one field of that tuple containing the tuple representing the previous block in the blockchain. This would allow a VM that had the tuple for the current block to read earlier blocks. The precondition of an assertion would thus specify a recent block height, and the VM would have a special instruction that pushes the associated block tuple to the stack. In order to be able to verify a one-step proof of this instruction, the Verifier just needs to keep track of the Arbitrum tuple hash of each block.Arbitrum VM enables the use of different Virtual Machines other than the Ethereum Virtual Machine. This will help support bigger smart contracts and also multi-language supports like C/C++, Python, Go and Rust. This in turn might help with the adoption of ZK-rollups where developers have to write applications in special purpose and new languages.ApplicabilityAfter 3 years of development, Arbitrum was finally released on Ethereum’s mainnet 3.', '@jyezie Thanks for posting a fascinating summary.I have a few general questions about Arbitrum’s design and the company’s experience since the launch. As the summary’s author, do you feel qualified to respond to any of these?In the limited time since Arbitrum’s launch, what have been the company’s major learnings?Have there been more or fewer fraud proofs than you expected to generate in that time?Have there been any issues/learnings with the Any-Trust guarantee that requires only one honest validator?Unlike Optimism, Arbitrum supports all EVM languages, but uses a transaction processing method that can mean longer wait-times for a fraud proof. How are those differentiators working out in practice? Has Arbitrum’s fundamental design approach held up well in the real-world experience so far?What are the implications for security guarantees, and also for users/dev adoption?', '@jyezie – in the article you mention that Arbitrum has been around for three months now and has been successfully implemented. (cc @Eric (if you’re around)). Could you tell us about the process of implementation and how it differed from expectations? Three months in, how is Arbitrum working with ETH?', 'Good questions @jmcgirk @rlombreglia. The main issue with Arbitrum is that the design is pretty to undertand and the papers are clear. But there are many questions left as to how that does look since it is implemented. It would be awesome to get those answers from Aribitrum’s team because in their own words, they released a mainnet beta 2.For instance let’s take a look at the security guarantees. For the VM validators, how is the onboarding process working? The stake needs to be high enough to deter malicious participation, like challenging correct states to halt the VM. But it should enable enough validators to join a bring more resilience through decentralization.For the end user, one question is how do they apply the “AnyTrust” guarantee. If I am a VM manager, that’s easy I only need to trust myself. But the chances that for example Uniswap select me for this task or that I have enough stake to take part are rather low. Thus how does a user raise the alarm to one (honest) validator?Arbitrum brings some nice short and long term advantages to the Ethereum ecosystem. In the short term, Arbitrum, like other OR enables developers to simply port their existing Solidity code to their L2. This is especially important to profit from DeFi composabilty and a usual point of comparison with Zero Knowledge Rollups that do not support, “yet”, solidity smart contracts.In the long term, and this is where OR & ZKR come together, we can expect developers to be able to use other  programming languages (C++, Rust, Go, …) to develop smart contracts via more powerful virtual machines.', 'Very detailed summary! The mechanics introduced Arbitrum reminds me of truebit(https://truebit.io/). Seems to me they are similar in multiple aspects including both of them being L2 scaling solutions, and the way they deal with malicious behaviors. A head to head comparison between this two should be interesting.', 'Hey @jyezieThis protocol seems like a non-trivial breakthrough in blockchain research. It’s a pleasure to get to learn about this meaningful information. Thank you for selecting this paper and publishing the summary.I have a few questions, and along the way, I would like to share a version of my very generalized understanding. This should put us on the same page. (And make sure I didn’t misunderstand the author. Cheers.)–The first reason why Ethereum smart contracts are hard to scale is that they are subject to Ethereum’s hashing power limit. Arbitrum is a protocol that moves complicated calculations off-chain, making versatile contracts possible.To verify the data and determine the state, Arbitrum relies on third parties referred to as managers. The protocol will make sure it rewards the manager that submits the right verification. So as long as one out of the handful of managers turns out to be trustworthy, the outcome would be reliable.This notion is referred to as the Any-Trust Guarantee: If at least one manager of a VM is honest, the VM will execute correctly according to its code. (VM is short for Virtual Machine)My question here -Smart contracts are for decentralized, trust-less purposes. What’s the point of having a protocol that only works when there is a trusted third party?I understand that players don’t need to place their trust in any of the managers, but just one of them. Yet how do you know which one of them to trust?I’d also like to ask about an unclear detail: can a “challenge” be raised by someone that has already participated in the assertion?Thanks', 'Before I dive directly into questions specifically about Arbitrum, I want to ensure that I have the context, much like @Twan did in her comment.Although there are other L1 options, the majority of users are still on Ethereum. Focusing solely on scaling on Ethereum, it can be done at L1 with options like moving from a Proof of Work (PoW) to a Proof of Stake (PoS) consensus mechanism or sharding. That said, these solutions take time to implement. Hence, several L2 solutions to assist with scaling have emerged.Among L2 solutions: sidechains, state channels, plasma, rollups (zk and optimistic), and validium (similar to zkrollup, but data availability is off-chain); Arbitrum falls into the optimistic rollup family. And, some of the criteria that could be used to evaluate L2 solutions include: security, performance, usability, and how data is moved, generated, stored between/within L1 and L2.Narrowing in on L2 rollup scaling solutions, how does Arbitrum compare to StarkNet and Optimism with respect to the evaluation criteria above?The summary mentions in the Implications and Follow-ups section, that a zk protocol be used to implement the one step proof. Is this perhaps to address what might be considered a weakness in optimistic roll-ups, namely that the state transition is assumed correct until an invalid transaction is challenged, unlike zk-rollups where every state transition generates a SNARK which is verified by the rollup contract on the mainchain? This question is asked with the understanding that all solutions must consider and try to balance trade-offs.', 'Thanks for your prompt and detailed reply, @jyezie! It’s true that it’s still early days for Arbitrum right now—possibly too early for answers to some of my questions. However, we still hope to have a member of the Arbitrum team participate in this discussion at some point.Meanwhile, your own thoughts on porting Solidity code, the eventual use of other languages, and the appearance of more powerful VMs all suggest a strong future for Rollups of both families.', 'It’s a very great summary of Arbitrum. In the layer 2 solutions of on-chain store the data and off-chain compute the transaction. I’m very curious about the flexibility, if one of the honest node is suddenly offline, this means the other honest node can choose to reset a machine to any state with other validators’ agreement?', 'They have a whole section on participation games like Truebit.In the context of this paper, think of participating as “verifying a computation.” It costs something to verify the computation, but once you’ve verified it, you can claim to have verified it from any number of additional Sybils for free, and these Sybils are indistinguishable from “real” verifiers. The goal would then be to design a participation game (i.e. a reward function f (·)) such that in equilibrium, no player has any incentive to Sybil, and a desired number of players participate, so that the apparent number of verifiers equals the actual number of  separate players who were verifiers.In summary, Arbitrum’s authors show that Truebit is designed with a One-shot Sybil Proof setting. While smart contract verification is more of a repeated game. This leads to an equilibrium where only one party, sybilling as many parties,  is doing the verification.But someone more familiar with Truebit could make a more thorough comparison.', 'The first reason why Ethereum smart contracts are hard to scale is that they are subject to Ethereum’s hashing power limit.Broadly speaking, I would say Ethereum (and other blockchains) have a limit on their scale because each node has to reproduce the work done by one. In a way, we have so many miners but they are competing instead of cooperating to include the transactions in the blockchain.Smart contracts are for decentralized, trust-less purposes. What’s the point of having a protocol that only works when there is a trusted third party?Good question. There seems to be a contradiction. Remember that what we call trustless or without intermediaries on blockchain actually means that have a network of intermediaries, miners/validators, that do the work. We hope that the majority are honest so that they won’t censor your transactions and keep it in the ledger.  I can make the case that both trust, in Ethereum or Arbitrum, are the same. The key difference is that you assume there will be more different and honest parties on Ethereum than on Arbitrum.We can clearly say we increase scalability at the expense of decentralization here.', 'Thanks for replying boldly and clearly.What are your thoughts on the privacy improvement? Arbitrum’s privacy comes when data is only revealed to the manager, but what happens when the manager leaks the data?I see it’s better compared to automatically revealing all information on the public ledgers. But is there any way we can punish misbehavior?', 'The paper is an explicit introduction to Arbitrium that is easy to comprehend for newcomers to the blockchain world. I learned about the distinctive attributes of arbitrium compared to Ethereum, while not limiting the usability of Abitrium for Ethereum developers.The four main advantages of scalability, privacy, AnyTrust guarantee, and interoperability with Ethereum touched on all areas of concern especially from a novice point of view where risk tolerance will be relatively lower.The most interesting feature of the Arbitrium to me is the Validators, I am however concerned about them being chosen by the dapp creator. Makes me skeptical about how honest they would be should any party decide to cheat but that fear was struck out by the dispute resolution protocol.It is reassuring to see that the creators of Arbitrium completely understood the gaps in other smart contracts scalability solutions and designed the Arbitrium to fill that gap.', '@jyezie Thanks for sharing this so much great summary! It’s a wonderful way to learn L2 solutions for non-technical background learners.Also, thanks @Twan  for your enlightening questions. jyezie:Smart contracts are for decentralized, trust-less purposes. What’s the point of having a protocol that only works when there is a trusted third party?Good question. There seems to be a contradiction. Remember that what we call trustless or without intermediaries on blockchain actually means that have a network of intermediaries, miners/validators, that do the work. We hope that the majority are honest so that they won’t censor your transactions and keep it in the ledger. I can make the case that both trust , in Ethereum or Arbitrum, are the same. The key difference is that you assume there will be more different and honest parties on Ethereum than on Arbitrum.We can clearly say we increase scalability at the expense of decentralization here.I am just thinking about some trust mechanisms that are designed to count on smaller and smaller amounts of honest nodes. Would it make sense that the essence of decentralization may not be necessarily linked to the amount or proportion of trusted nodes, while it lies in the implementation of machine rules? When the trust mechanism is designed to rely on anyone who works align with the rules and its results are verified by a set of rules, so it actually depends on the machine rules rather than that party. If we say this does not diminish the quality of decentralization, even if it only passes through by one party because it still works by the machine rules regardless of any party’s will, does it make sense to you?', 'A small comment on L1 scalability: by itself, moving from POW to POS doesn’t bring more scalability. You need to play on many part of the blockchain and consensus to push better performance. You can skim some of the different approaches in this list of Notable works in scaling 1.Now onto your questions. You are right, when we compare L2, we have to look at many more parameters than what you have with L1 solutions. Personally, I would have a hard time comparing those solutions in depth because on one hand I haven’t read that much about the other protocols, beyond some podcasts debate, and on the other I focus on the papers from Arbitrum, which lack the implementation details. So far the best comparison resources I found are:A  L2 overview by Patrick McCorry L2BEAT – The state of the layer two ecosystemBroadly speaking, I feel like we are trying to escape L1 consensus, yet we are realizing that we need some consensus on the data (Note: I am still wrapping my head around the Data Availability problem). For instance, Arbitrum requires all the validators to agree while on DYDX (that use Starkware ZK Rollup), there is one entity generating the proofs. OR have the advantage that they can onboard dev and by extension much more easily: they keep L1 Composability, despite their optimistic nature. ZKR seem much powerful in the long term but have issue onboarding projects.What does this mean for the end user then? As you pointed out, there will be many trade-offs. I am not sure how users will understand the different guarantees they have and all the means, if any, they can use to detect/stop a fraud. This brings back the question of decentralization and the trustless nature of those protocols, as also mention in @Ajibike_Jimoh’s and @Astrid_CH s comments.', '@jyezie thanks for your response. I plan on digging into the different L2 options a bit more and will use the two articles you reference as well as the Notable Works in Blockchain Scaling as my jumping off points, so many thanks! Down the rabbit hole…', 'Arbitrum is an optimistic roll-up solution to scaling Ethereum just like starkNet, but anyone who has actually engaged and used arbitrum for transactions would encounter some current challenges including, high transaction fees to bridge assets, 7 days transaction finality window, and lack of dapps listed on the platform this has contributed to a declining transaction rate on the platform. What can be done to mitigate these challenges and improve the current state of the project?', 'Arbitrum is one of the methods for achieving fast transaction speed via a layer-2 technology. But what if layer-1 could do this all by itself?I just read a brief but fascinating article on Cardano’s forthcoming “Input Endorsers,” 1 which are a layer-1 innovation that splits each block into two parts: the consensus block and the transaction block.This separation, Cardano claims, will allow its proof-of-stake Ouroboros protocol to stream transactions constantly, rather than waiting for consensus.Input Endorsers aren’t here yet; they’re scheduled for future release. But if this is accurate, it could be a big advantage for Cardano, and conceivably knock over the applecart for the many layer-2 schemes in existence (or in development) whose only raison d’etre is to speed up the inherently slow transaction rate of layer-1.The only other article on Input Endorsers I’ve been able to find is here 1. Does anyone have thoughts on this, or more information on “Input Endorsers”?']"
                Research Summary: SoK: Applying Blockchain Technology in Industrial Internet of Things              ,https://www.smartcontractresearch.org/t/research-summary-sok-applying-blockchain-technology-in-industrial-internet-of-things/866,Scaling,82,['https://eprint.iacr.org/2021/776.pdf'],"['iot', 'summary']","['SoK: Applying Blockchain Technology in Industrial Internet of ThingsDescription:Industrial Internet of Things (IIoT) platforms need to integrate blockchain technologies to overcome challenges such as poor interoperability, privacy, security, and resource constraints in addition to heterogeneous smart devices, networks, and data types.TLDRTraditional IIoT systems employ centralized cloud-based architectures that may be unsustainable for industries’ projected exponential growth.The drawbacks of using a centralized architecture include having a single point of failure, massive data processing requirements, and lack user-access controls.Decentralized architectures may address these shortcomings, however, integrating them requires addressing several challenges: standardization, interoperability, and scalability.This study explores recent advances in tackling those shortcomings and examines several practical applications of blockchain architectures in IIoT platforms.The author proposes three architectures for integration: IIoT-IIoT, IIoT-Blockchain, and Hybrid Approaches.Core Research QuestionWhat are the architectures that IIoT platforms need to integrate to sustain industrial advancement?Citation[1] G. Wang, “SoK: Applying Blockchain Technology in Industrial Internet of Things”, Cryptology ePrint Archive, 2021. [Online]. Available: https://eprint.iacr.org/2021/776.pdf 9. [Accessed Sept. 18, 2021]BackgroundIIoT is a branch of the Internet of Things (IoT) that stems from the increasing prevalence of smart devices across industrial applications. At current rates of growth, there will be 75 billion IoT devices by 2025, with IIoT devices making up 17% of the total.Since IIoT deals with massive amounts of data, the ubiquitous adoption of IoT devices will demand efficiency improvements. Current centralized solutions won’t be able to keep up with increasingly large and complex networks of IIoT devices.Blockchain technologies may offset many of the traditional limitations of IIoT with decentralized mechanisms, immutability, and transparency.Blockchain infrastructures can eliminate single points of failure and reduce overhead by distributing data processing [1].Still, the scope of blockchain applications within the industrial sphere is relatively limited. Though integration presents considerable opportunities for advancement, no universal standards for integration currently exist.SummaryFeatures and Challenges of IIoTThe primary function of IoT is connectivity. In industrial settings, IoT devices are Cyber-Physical Systems that link machines to control systems, interfacing physical operations with their cyber counterparts to reduce the need for human interference and allowing automation.Most IIoT devices are lightweight nodes with resource constraints such as limited computing power, bandwidth, energy supply, memory, and storage.IIoT devices often have unpredictable connections to one another due to device mobility, faulty wireless networks, and device standby settings.Traditional IIoT platforms deploy centralized cloud-based infrastructures, which provide ample computing power and storage, yet when joined with IIoT technologies and devices they present significant drawbacks.These include privacy and security vulnerabilities, access control issues, expenses and risks associated with third parties, and bottlenecking and scalability concerns.IIoT platforms experience poor interoperability mainly due to their heterogeneous make-up. The uses of IIoT devices vary per application, as do network topologies and data types. Each industrial sector and organization may implement different protocols, making data and service sharing complex and inefficient.Features, Benefits, and Challenges of BlockchainBlockchain is a shared ledger that verifies and records transactions through decentralized technologies and distributed networks. All participating nodes within blockchain networks store identical copies of the ledger and maintain consistent timestamped records.Data stored on the blockchain is immutable and tamper-resistant, as any alterations are reflected in successive blocks’ overturn. These qualities, coupled with blockchain’s transparency and traceability, make it a viable structure for business relations.Blockchains generally apply asymmetric cryptography and digital signatures for security and privacy purposes.Blockchain’s pseudo-anonymity provides a degree of user privacy, though privacy remains an open issue.Blockchains are vulnerable to 51% attacks, partitioning attacks, and smart contract scripting exploits.Scalability is an ongoing challenge. As the blockchain expands, performance diminishes and resource requirements increase.Integration and Advantages of Blockchain and IIoTAs the number of IIoT devices on a network grows, the amount of data generated increases. Likewise, the demands on bandwidth and computational power also increase.Consequently, centralized systems experience congestion and delay, resulting in expensive hardware solutions and steep maintenance overhead.Using a blockchain’s decentralized structure as an overlay on existing IIoT frameworks can evenly distribute computational efforts among participants in a network.Smart contracts stored on the blockchain serve as real-time auditors and may eliminate third-party actors in IIoT systems, thereby lowering expenses.Blockchain’s composite layer acts as an interface that streamlines different data types and masks underlying heterogeneous layers. It connects the communication layer to industrial applications, bridging diverse mobile and industrial networks, enhancing interoperability, and facilitating reciprocal data exchange. Refer to Fig. 5.Current industrial solutions employ lightweight IIoT devices to accelerate performance, yet this poses a growing risk to the industrial landscape. End-devices are subject to attacks since they cannot host robust security mechanisms and firmware updates are intermittent 1.Integrating blockchain may enrich IIoT’s security systems through robust encryption primitives and multi-digital signature requirements. Moreover, blockchains are resistant to single points of failure attacks and provide a measure of fault tolerance.Industrial applications typically employ consortium or private blockchains which enable smart contracts to regulate user access controls and data provenance. This improves privacy, security, and detection of unauthorized user access.Omission of device authentication services potentially compromises IIoT operations.Blockchain identification technologies can authenticate and monitor devices to assess and manage their integrity.985×539 194 KBChallenges of IntegrationIIoT devices have limited computing power, energy supply, and bandwidth capabilities. They cannot meet the requirements of blockchain mechanisms and sustain high levels of throughput.The storage requirements of blockchain are too significant for lightweight nodes. However, devoid of the entire blockchain and its data, participating lightweight nodes cannot validate peer transactions.Since industrial operations are time-sensitive, the latency introduced by consensus protocols may debilitate system performance and timestamping accuracy.MethodThe author employed a qualitative approach to explore architectures that IIoT platforms need to integrate to improve their current architectures. The researcher collected data through a comprehensive literature review consisting of 244 sources.ResultsWhile identifying integration challenges and potential solutions, the author provides some generalizations without necessarily connecting each problem to a solution and vice versa. Therefore, this summary only covers the issues and solutions that directly align.Potential SolutionsThe author proposes introducing structures similar to a blockchain that improve scalability and throughput and provide comparable services such as “decentralization and immutability” [1]. These include Directed Acyclic Graph 1, Tangle, and Greedy Heaviest-Observed Sub-Time.On-chain storage may be reserved for critical data, whereas minute data may be stored off-chain and retrieved through distributed hash tables like Kademlia 1.Latency can be improved by reducing transaction size and using powerful nodes such as edge devices to mitigate confirmation delays.Discussion and Key TakeawaysBlockchain Storage and BaaS PlatformBlockchain communication requirements and massive data inhibit end-device performance and are too costly for on-chain storage solutions.Decentralized blockchain storage run on cloud-based infrastructures can provide more efficient, secure, and cost-effective models for IIoT platforms as illustrated in Table IV.Using Blockchain-as-a-Service (BaaS), industrial operations can integrate specific blockchain applications into existing cloud-based platforms with fewer complexities and investments. Essentially, this entails customizable blockchain capabilities to manage and store data with service providers that oversee blockchain operations. Table V. provides a list of various BaaS platforms.977×389 84.4 KB460×841 114 KBIntegration Approaches and Blockchain SelectionThe author presents three models for blockchain integration: IIoT-IIoT, IIoT-Blockchain, and Hybrid Approaches. Table VII. provides a comparison of these approaches.In the IIoT-IIoT model, blockchain has minimal interaction with IIoT and functions as an immutable archive, accounting for partial IIoT data and sustaining low latency levels, making it ideal for time-sensitive applications.The IIoT-Blockchain model emphasizes the interconnectivity of blockchain and IIoT, recording all data via consensus mechanisms. This approach warrants more resources and values data as its greatest asset.The Hybrid model secures partial data on the blockchain and requires a calculated design to capture critical data. Without heavy communication between IIoT and blockchain, it reduces burdens on constrained devices.910×206 41.5 KBImplications and Follow-upsOptimization on PerformanceIndustrial scenarios must sustain high levels of throughput for devices to function synchronously. However, existing solutions for blockchain integration increase throughput at the expense of scalability. Decreases in latency may also have adverse effects on scalability.Most IIoT devices are resource-constrained, making direct integration impractical.Integration solutions must accelerate scalability and throughput, lower latency, and accommodate the limitations of end-devices.ScalabilityPoor scalability interferes with the adoption of blockchain in IIoT networks.Unstable wireless connections also contribute to scalability issues.Areas of research that may surface potential solutions include sharding and side-chaining.Security and PrivacyBlockchain and IIoT are predisposed to different vulnerabilities.Smart contract scripting lacks standardization, making discrepancies and oversights exploitable.IIoT wireless networks are susceptible to eavesdropping and Denial of Service attacks.Privacy preservation is an open issue for blockchain-IIoT platforms. The author proposes “decentralized record-keeping that is completely obfuscated and anonymous by design” [1].Editable BlockchainConventionally in a blockchain, all records should be stored by all nodes. However, some industrial data eventually becomes useless and can be deleted or transferred to secondary storage. Also, fraudulent data should be nullified.Editability allows modification and deletion of irrelevant, incorrect, and generally undesirable data stored on the blockchain. However, there must be a compromise between editability and security of the system.Edge ComputingIIoT devices frequently cannot meet the requirements posed by IIoT applications when blockchain is applied, specifically in the context of computation and networking. Their storage is limited, and there are also limits on their interoperability and authentication standards.In edge computing, edge devices are placed close to edge servers. Edge servers are less powerful than cloud servers but have closer proximity to edge devices which in an ideal situation allows for minimization of latency and transmission delay.Powerful gateways can also act as consensus nodes in the blockchain to resolve the storage and computing problems of lightweight devices.This paper proposes a potential research direction in which blockchain technology is implemented on the IIoT edge and minimizes networking and computational overhead.Standardization on Blockchain-Based IIoTLack of or inconsistency in standardization results in the inability to reach service agreements for integration processes.The Institute of Electrical and Electronics Engineers (IEEE) and the International Standards Organization (ISO) have made attempted standardization efforts.Blockchain standardization will help to redefine future technologies and assist both users and developers of blockchain.ApplicabilityIndustry 4.0Total industrial automation can be realized through the convergence of IoT, Cyber-Physical Systems, and blockchain.With their immutable services, decentralized systems, particularly blockchain, can enable secure, trustworthy, and cost-effective interactions among autonomous agents.Quality of Service monitoring on the blockchain can improve latency through its updating requirements and coupled with smart contracts, it may actualize chaining in real-time.Smart ManufacturingSmart manufacturing predominantly relies on centralized infrastructures and third-party auditors, which pose considerable disadvantages: poor interoperability, increased overhead, and security vulnerabilities.Interoperability may be improved through blockchain’s ability to meld disjointed IIoT systems into a distributed network.Overhead can be lowered with smart contracts as real-time auditors.Smart contracts can provide automated firmware updates to leverage security.Through blockchain-enabled IIoT platforms, smart manufacturing could achieve advances like machine self-monitoring, self-diagnosis, and automatic maintenance requests as outlined in the Blockchain Platform for Industrial Internet of Things 1.Smart GridCentralized Energy Management Systems (EMS) have been shown to be less efficient and secure for peer-to-peer (P2P) energy trading.Decentralized blockchain EMS has potential for more efficient and trustworthy P2P large-scale energy trading, especially with regards to the security and privacy of energy exchange and transmission.Decentralized EMS applying bi-level algorithms 3 indicate more practical operations for renewable energy source distributed generators in microgrids.Supply ChainBlockchain may be implemented as a form of quality control in the supply chain. Its identification technologies translate physical assets to digital identities associated with immutable timestamps, providing traceability and tamper-resistant proofs.A product can be traced from its source to the shelf to uphold food safety standards in the food industry.', 'Hi @rlj  -You mentioned that centralized systems experience congestion and delay, and blockchain’s decentralized structure is supposed to make it faster because it distributes computational to different participants.How does this hold? Isn’t it commonly held that blockchain is computationally intensive and much slower than other data transmissions on the internet?This is just speaking from my personal experience. Blockchain transactions take as long as half an hour to process. I would be concerned if any IoT device I own updates its data with that kind of latency, let alone the cost of computation.Thanks!', '@rlj thank you for posting this terrific addition to our scaling category. When we talked about this summary one-on-one you mentioned that IIoT + blockchain technologies had tremendous potential, to the point that the author describes it as being able to ignite a fourth industrial revolution. How come? How might these networked things improve industrial processes?', '@Twan I echo your concern about the latency built into consensus mechanisms. See also: rlj:Scalability is an ongoing challenge. As the blockchain expands, performance diminishes and resource requirements increase.And: rlj:Since industrial operations are time-sensitive, the latency introduced by consensus protocols may debilitate system performance and timestamping accuracy.However, with IIoT specifically, there seems to be a resistance to the “shift of mindset” necessary to flip constraints into opportunities. Note this passage: rlj:In edge computing, edge devices are placed close to edge servers. Edge servers are less powerful than cloud servers but have closer proximity to edge devices which in an ideal situation allows for minimization of latency and transmission delay.This captures the point: Of course edge servers are less powerful than cloud servers. But when designed to have “closer proximity to edge devices” they can help conquer problematic latency.This leaves me thinking that working with the benefits of decentralized, P2P structures is a “mindset” that conventional thinking hasn’t yet embraced.', 'Hi @Twan, thanks for your question. To clarify that matter, distributing computational efforts reduces expenses associated with “installation and maintenance in centralized infrastructures” and “networking equipment” [1]. That is not to imply it would reduce latency, though you raise a key issue in this research. Poor scalability largely interferes with blockchain-IIoT integration. Traditional IIoT platforms and blockchain both face this challenge. However, the latency introduced by blockchain’s decentralized mechanisms is unacceptable by industrial standards, which require massive amounts of data to be processed in a timely manner. As noted by the author, “solving scalability in blockchain will serve as a huge advance toward creating a practical decentralized infrastructure for IIoT applications” [1]. I would have to agree with the concerns you express. In its current state, blockchain comes at the expense of suboptimal industrial performance and with considerable computational and networking overhead.', 'On the bright side, innovations in blockchain computation efficiency come out gradually. It’s not standing still. Researchers and engineers are working hard to reduce energy consumption. Ethereum even promised to reduce energy consumption by 99 percent on Ethereum 2.0.If it’s going faster every day, maybe we can get lucky, and see blockchain overcome this barricade in the future.', 'Hi @rlj: Thanks for an excellent summary of a paper which reveals many of the contradictions presented by the history of IIoT, as well as the possibility of mitigating those contradictions with blockchain—if industry can bring itself to adopt the necessary new “mindset” around the issues at hand.Here are a few key points that leapt out at me: rlj:IIoT platforms experience poor interoperability mainly due to their heterogeneous make-up. … Each industrial sector and organization may implement different protocols, making data and service sharing complex and inefficient.This highlights a key failing across most industries and OEMs: Attempting to achieve customer “lock-in” with core proprietary protocols and technologies rather than adopting common-sense standards and then differentiating products on top of those standards. rlj:Conventionally in a blockchain, all records should be stored by all nodes. However, some industrial data eventually becomes useless and can be deleted or transferred to secondary storage.The assumption that all data is equally valuable, to be preserved forever, is simply in error. A huge percentage of industrial data can be collected and processed at the edge, acted upon appropriately, and then discarded.The paper characterizes “edge” devices correctly… rlj:Most IIoT devices are lightweight nodes with resource constraints such as limited computing power, bandwidth, energy supply, memory, and storage.…but it doesn’t say that if we adopted a fully networked, P2P, decentralized state of mind, the limited computing power, storage, etc. of IIoT devices might be seen as an opportunity not a constraint.For example, the paper notes: rlj:Current industrial solutions employ lightweight IIoT devices to accelerate performance, yet this poses a growing risk to the industrial landscape. End-devices are subject to attacks since they cannot host robust security mechanisms…But then it goes on to say: rlj:Integrating blockchain may enrich IIoT’s security systems through robust encryption primitives…Why can’t these “robust encryption primitives” be implemented on edge devices themselves?Finally, the paper makes an intriguing point: rlj:This paper proposes a potential research direction in which blockchain technology is implemented on the IIoT edge and minimizes networking and computational overhead.This is a fascinating area. Can you say a little more about it?', '@GanouTeikun welcome to the forum! With a mechanical engineering background, I was thinking that this post may interest you.', '@Gearlad how does this overlap with some of the work you and @Sean1992076 have been working on? @rlombreglia mentions embedding primitives in IoT devices, can you talk about some of the work your lab has done with turning IoT into lightweight ETH nodes and routers – would that be a solution ot some of the issues he’s brought up?', ' rlj:The IIoT-Blockchain model emphasizes the interconnectivity of blockchain and IIoT, recording all data via consensus mechanisms. This approach warrants more resources and values data as its greatest asset.Having worked in this field, I had an experience that using blockchain for storing all the data is quite a terrible idea (from an implementational perspective). Instead signed Hash Tables can be the preferred way since we had like more than 50 GBs of data only from one industrial site per day. Imagine storing it as multiple copies stored on all participating nodes that too through consensus!And for immutability of data (partial?), as long as at least one party can provide full copy of data along with the corresponding signed hash table from blockchain, we are good ', '@kanad really good to have you on the forum again! Can you tell us a little more about the challenges of combining IoT and the blockchain – do you agree with the article’s basic premises about the increasing complexity of the networks requiring some kind of decentralized structure? Or are there other solutions that might work temporarily (especially in an industrial environment)?', 'First of all, I admit that I’m still a newbie in blockchain and I’m still familiarizing myself with IoT and blockchain and their implications in Industry 4.0.I was having an interesting discussion with @Gearlad in my lab in the Mechanical Engineering department at National Taiwan University. Industry 4.0 is all about leveraging IIoT technologies to allow for cities to become smart cities and for factories to become smart factories. Let’s just say this: our lab plans to become a smart lab.We collect all kinds of experimental data including temperature, pressure, size of material, ratio of different elements in an alloy, current (Ampere) density of materials, performance as an electrocatalyst for C02 reduction, so on and so forth. Currently, not all of the data that we collect is done with the help of machines, and some of it is based on human observation. Moreover, for the data we do collect, we store it on two mediums: Google drive and an external hard disk. An external drive is safer than Google drive in terms of privacy but nonetheless is still prone to damage and is less accessible than cloud storage. Unfortunately, Google is planning to shut down its unlimited storage plan for institutions and enterprises. What we need is a new form of data storage that is both secure and easily accessible, and blockchain is promising in overcoming these limitations.Currently our lab has more traditional equipment that is not yet connected to the Internet. However, we have begun modernizing the lab with the application of AI and IoT. The purpose of this is to autonomously sort all of the data that we collect and to find different trends in this data. By using these trends we can predict the change in parameters such as size of ligaments, morphology structure, and electro-catalytic performance in porous metals. We use a Scanning Electron Microscope (SEM) to observe the morphology of porous metals; however, due to the limitation of the equipment, we need to adjust the focus of the images manually. Anothing thing worth mentioning is the facility is unable to generate mappings of elements on the porous metal, unless a better SEM machine is purchased (very expensive!). By applying IIoT, we have been planning to further upgrade our lab’s furnace so that it can monitor and control fluctuations in temperature more efficiently. In addition, automation can also increase safety as we’ve been relying on human observations to stop the furnace when the overshoot of the temperature in the furnace rises too high. Also, since we heat up materials in vacuum chambers, IoT may warn when the pressure of the chamber rises above the acceptable level. The examples above are also connected to the Industrial Control System (ICS) since we are aiming to create a system where all devices are able to do feedback control when some abnormalities arise.trendmicro.comIndustrial Internet of Things (IIoT) - DefinitionThe industrial internet of things (IIoT) refers to the extension and use of the internet of things (IoT) in industrial sectors and applications. The IIoT has a strong focus on machine-to-machine (M2M) communication, big data, and machine learning.', 'Thank you so much for this contribution!  In the summary, it is mentioned that edge devices may be utilized to consolidate transactions so a node can validate without having to be onboard each IIoT device.  I am not sure if the author made any suggestions how to get to this point of standardization without regulation or industry cooperation?  There seems to be an assumption of a common goal driving the theoretical IIoT blockchain layer; but in reality, what would compel a private organization to join a non-government regulated public network as a means of monitoring their devices if they did not have at least SOME influence over that network layer?While the proposed IIoT layer does sound useful in theory, has there been any indication that any private organizations have a desire to add a non-proprietary edge layer to their IoT devices, or has that notion mainly been coming out of the blockchain theoretical space?', 'Great post, really enjoyed reading it.I thought @rlombreglia replies were quite interesting about embracing constrains as opportunities.Decentralized disjointed layers can help organize data for some type of central processing. First, you would have to characterize your industry/orgnization into nodes/layers and have extreme understand of how they communicate with one another.  Local nodes can have set data standardization among the locale to improve scalability. Only using the distilled data from the points you reduce bottlenecks. rlj:IIoT platforms experience poor interoperability mainly due to their heterogeneous make-up. The uses of IIoT devices vary per application, as do network topologies and data types. Each industrial sector and organization may implement different protocols, making data and service sharing complex and inefficient.The ground level of one organization will most likely never look like another- so why even try to solve that problem? Differences are what makes organizations/sectors special; the overarching overlaps are where leaps and bounds will happen: getting product from A to B is ubiquitous. Though, I do not think a solution will make itself present until tested.Showing decentralizing your business to align with the blockchain structure is where improvement using blockchain starts being obvious. rlj:This paper proposes a potential research direction in which blockchain technology is implemented on the IIoT edge and minimizes networking and computational overhead.It makes sense to me if the benefit of the blockchain is to be decentralized, so why not decentralize your industry? This will probably be backwards for a lot of current industries but newer ones for example with AI- helping managing datasets as smaller pieces of a whole could possibly improve transparency.An example that shows this is mentioned in the Smart Grid section about how centralized energy management systems (EMS) are less efficient than decentralized blockchain EMS. rlj:Decentralized EMS applying bi-level algorithms indicate more practical operations for renewable energy source distributed generators in microgrids.The more decentralized the system the better a decentralized technology like blockchain benefits it.I would like to learn more about this topic as it has huge implications, thanks for writing it!', 'This was an exciting read which shines a light on the Industrial Internet of Things as one of the key components of Industry 4.0. I really love how this summary highlighted some of the issues and challenges of IIOT and this includes the heterogeneous nature of IIoT, the complexity of the network, the poor interoperability system, the massive data management, and the risk of being vulnerable to a single point of failure. There seems to be an assumption that the application of blockchain technology could help improve the efficiency of  IIoT. I am not sure if the author answered the question on how Blockchain technology will solve the challenges of IIoT and thus, drive Industry 4.0.Reading the article [1], the author opines that the decentralized nature of blockchain could eliminate a single point of failure, save operational costs, and enhance trustworthiness, and  the immutability nature of Blockchain could make the data of IIOT difficult to be tampered with. However, we all know that Blockchain technology has a scalability issue, and also, the  Storage issue, which is interconnected with the scalability issue because as the size of the chain grows, nodes require more and more resources, thus decreasing the system’s capacity scale. Is the trade off with integrating blockchain technology to IIoT really worth it?I know there is a Current Use of Blockchain, the hyperledger Fabric, in the supply chain industry. for instance, Walmart used Hyperledger fabric to trace a batch of mangoes in 2.2 seconds, something that typically takes 7 days to do [2]. My last Question is thus, if Walmart were able to improve its supply chain efficiency with Blockchain, how, then, is latency a challenge blockchain needs to improve upon?[1] G. Wang, “SoK: Applying Blockchain Technology in Industrial Internet of Things”, Cryptology ePrint Archive, 2021. [Online]. Available: https://eprint.iacr.org/2021/776.pdf . [Accessed Sept. 18, 2021][2] Tan B., Yan J., Chen S., Liu X. (2018) “The Impact of Blockchain on Food Supply Chain: The Case of Walmart”. In: Qiu M. (eds) Smart Blockchain. SmartBlock 2018. Lecture Notes in Computer Science, vol 11373. Springer, Cham. The Impact of Blockchain on Food Supply Chain: The Case of Walmart | SpringerLink 1', 'This was an interesting read which gives a take on the core implications and potential of Blockchain as a service being the cornerstone to large-scale automative architectures. I really love how this summary gives me further perspective on the scaling trilemma of scalability, maintainability, and ease of use problem that Blockchain is facing! This may be a recurring theme of blockchain and the core hurdle for integration and adoption of blockchain for conventional commercial activity.Ensuring decentralization over a vast perimeter of automation may hinder scalability and may introduce further layers of abstraction that make such automation unnecessarily difficult to operate. Ensuring Scalability inevitably makes tradeoffs of blockchains decentralization and makes maintenance of said blockchain possibly irreconcilable. This goes the same for ensuring ease of use. Conveniences and an emphasis on good user experiences may as well make the entire application architecture so simple that blockchain may be the last technology any developer would want to use in their application.In terms of the scalability issue of data management, would you think the concept of a “blockchain database” be appropriate to facilitate further development of Blockchain as a service oriented automative applications?Blockquote El-Hindi et al. introduced the concept of Blockchain DB with the aim of coming up with a shared database on blockchains [2]. Blockchain DB introduces a database layer on top of the existing blockchain framework that extends the blockchains by classical data management techniques. The aim of blockchain DB is to increase the performance and scalability of blockchains for data sharing but also decrease complexities for organizations intending to use blockchains as DB.An overview of blockchain scalability, interoperability and sustainability 12. El-Hindi et al. (2019). “BlockchainDB - Towards a Shared Database on Blockchains.”', 'Thanks for your comment @jmcgirk.Industry 4.0The fourth Industrial Revolution (Industry 4.0) is driven by the goal of complete industrial automation with minimal third-party interference. Industry 4.0 has emerged through the convergence of Cyber-Physical Systems, the Internet of Things (IoT), and the digital technologies outlined in Fig. 1. In this paper, the author emphasizes that blockchain will have a profound impact on Industry 4.0, which may be attributed to underlying principles like better efficiency and trustworthiness. Blockchain-enabled Industrial Internet of Things (IIoT) platforms may enhance the main features of Industry 4.0 illustrated in Fig. 1. This is discussed with more depth as follows:841×376 116 KBDecentralizationAs a decentralized ledger technology, blockchain enables self-regulating capabilities and eliminates reliance on centralized authorities. In blockchain networks, smart contracts are real-time auditors that perform self-executing functions once specific conditions are met. Integrating blockchain networks with IIoT platforms may advance the autonomy of smart machines in industrial manufacturing by securing machine-to-machine communication. Further, removing third-party actors reduces overhead costs and risks associated with intermediaries and centralization, such as single-points-of-failure attacks. Considering that Industry 4.0 seeks to achieve full industrial automation, implementing a decentralized system may support its intent. This makes blockchain infrastructure and its autonomous capacities an appealing prospect for Industry 4.0.TrustworthinessThe use of blockchain may enhance data transparency and increase trustworthiness in Industry 4.0. In this paper, the author emphasizes that having autonomous agents communicate directly provides several key advantages: greater efficiency, lower expenses, and fewer risks involved. However, in centralized industrial systems, the trustworthiness of the agents is an open issue [1]. Blockchain can address this concern. With all data on a blockchain stored in an immutable and transparent manner, participating entities can monitor and verify transactions across multiple industrial sectors and organizations. Additionally, consensus mechanisms ensure that all participating agents within a network share the same account of the truth while smart contracts running on blockchain regulate data provenance, ownership, and user access controls. Since data stored on the blockchain is visible to the authorized users of a network, it is resistant to manipulation, and tampering is more readily detected. With more reliable data, the technologies deployed in Industry 4.0 are better equipped to “extract value” to improve learning patterns and autonomy within smart factories.DigitizationAs “\u200b\u200bthe backbone of any industrial sector”, the supply chain plays a critical role within industrial processes. Blockchain technologies can be implemented for quality control and more efficient supply chain management. One example of this may be found within the aerospace industry, where blockchain will potentially increase revenue by up to $40 billion. With a single aircraft consisting of up to millions of individual parts, blockchain technologies can assign each one a digital identity and retain its “digital twin”—a complete record that includes its provenance, service maintenance history, configuration, etc. This record is continually updated and available in real-time, making problems easier to diagnose and thereby reducing the frequency of manual testing and inspection. For instance, Airbus was able to take a preemptive maintenance approach by embedding sensors into its machinery. Blockchain’s utility within the aerospace industry can generally be applied to industrial ecosystems. Industrial assets like smart devices and machinery may be digitized and tracked. With parts originating from many different suppliers, traditional systems face challenges with counterfeits, communication barriers, and manual record-keeping. In Industry 4.0, blockchain can help to overcome drawbacks such as these by providing a secure digital accounting system that allows for greater traceability and tracking.ConclusionThe topics discussed here only touch on a very small portion of blockchain’s utility within Industry 4.0. Anyone interested in further exploring blockchain’s applicability in Industry 4.0 should read “Blockchain technology applications for Industry 4.0: A literature-based review 1.” With regards to this response, the key takeaways of blockchain within Industry 4.0 are: decentralization that allows for greater automation of industrial processes, trust through transparency and immutability of industrial data, and digitization that allows for traceability and tracking of industrial components.', '@Twan, I’m interested to learn how Ethereum 2.0 will reduce energy consumption by 99%. What measures are being taken to achieve this level of efficiency?@kanad Thanks for sharing. The author of this paper mentions that the use of hash tables is a possible storage solution for resource-constrained IIoT devices. Can you elaborate more on how distributed hash tables are beneficial for industrial applications and end devices based on your experience? Also, I’m curious to hear your thoughts on whether you think one of the author’s proposed models of integration or BaaS is appropriate for industrial uses at this point in time.@GanouTeikun Thanks for your comment. As the lab integrates more technologies that introduce connectivity to traditional equipment, what do you see as possible challenges or risks posed by this scenario? How might blockchain advance automation in the lab? What are some potential drawbacks that blockchain may present? Lastly, can you provide some insight on the uses of blockchain and AI convergence in the lab?', '@rlombreglia Thank you for your thoughtful additions to this conversation. While this paper has limited discussion on edge computing, it most certainly is an area that deserves attention given its tremendous growth, relevance to time-sensitive applications, and potential to accelerate industrial performance.Edge computing processes and transmits local data, meaning that the bandwidth requirements are lower in comparison to dispatching all data to a centralized data center. Edge computing can reduce cloud storage requirements by filtering out “excess data”, and sending the essential data to the cloud. Limiting the data processed by the cloud mitigates latency, making edge computing an emerging solution for the timing requirements of industrial operations. Gartner estimates that around 75% of “enterprise-generated data” will be “created and processed outside a traditional centralized data center or cloud” by 2025. As industrial applications continue to incorporate more smart devices into their networks, finding an efficient and secure means to manage the massive data that they generate is increasingly imperative. Edge computing shows promise in addressing scalability challenges and lowering operational overhead in Industrial Internet of Things (IIoT) platforms, yet the inherent risks of centralized systems prevail.Blockchain technology may be implemented on the IIoT edge to secure communication and minimize networking and computational overhead. This is observed in the following use cases:Guo et. al tested a “distributed and trusted authentication” model that deployed a Practical Byzantine Fault Tolerance consensus algorithm to record and authenticate data on a consortium blockchain. In the experiment, edge authentication services were provided via smart contracts, an asymmetric cryptographic framework was applied to secure the edge nodes and terminals, and an edge-based caching scheme was used to accelerate the hit ratio. The results suggested that the proposed model outperformed current edge computing models by reducing delay, improving hit ratio, and providing a more cost-efficient communication and computational mechanism.Jangirala et. al developed a simulation that used blockchain identification technologies to verify the identity of reader tags within supply chains. By applying a “lightweight blockchain-enabled RFID-based authentication protocol” within a “5G mobile edge computing environment” with bitwise rotation, bitwise exclusive-or, and linear cryptographic hashes, the researchers observed enhanced security and improved communication and computational overhead compared to existing models.Nkenyereye et. al proposed a private blockchain-based “lightweight multi-receiver signcryption scheme” for emergency driven messages (EDM) in 5G vehicular edge computing. By implementing blockchain into edge nodes, EDMs were securely transmitted to edge servers within close proximity to reduce response time. The findings of the study indicate that the edge nodes were shielded from various attacks and that the model reduced communication overhead.To reiterate the material discussed in the summary, blockchain removes centralized solutions used to increase throughput, i.e. resorting to expensive networking equipment, and it eliminates fees associated with third parties. Though the author of the original article does not specify as to why blockchain may lower these overhead expenses, generally it seems that incorporating blockchain into edge nodes, adopting lightweight protocols, and using blockchain as an overlay for heterogeneous devices may allow for communication to become more efficient, secure, and cost-effective.At the edge layer, poorly-secured edge devices are an entry point for attackers. Moreover, resource-constrained edge-IoT devices in centralized systems face challenges with implementing “fine-grained access controls” and “encryption-based data protection.” Integrating blockchain into the Edge of Things (EoT) networks enables security services such as “access authentication, data privacy preservation, attack detection, and trust management”. The convergence of blockchain, edge computing, and the internet of things creates the novel paradigm coined the blockchain edge of things (BEoT). In the BEoT, data may be secured through encryption algorithms, yet one thing that remains unclear to me is if the “robust encryption primitives” are actually integrated into the edge devices themselves. From what I gather, encryption mechanisms are carried out in the network layer after the blockchain authenticates the devices.Per conversations with @Gearlad, Francis has experience with edge computing. @fmendoz7, is there any insight that you can provide on this topic?', 'Hi @rlj, thank you for the thoughtful response. Our lab lacks personnel with a strong IT background. In addition, it’s a fairly new lab and we are still in the process of acquiring new resources.Our research is more related to material science; we’ve just started getting into applying automation and AI technologies to our research recently. Some of the softwares we use include OriginPro, Jade (for XRD), CHI6273D electrochemical analyzer, AutoLab, etc. There are many cases where the application of AI is beneficial to all parties involved, and there are ways that it could optimize the softwares we use. With that being said, there is always an inherent risk with using AI and taking humans out of the equation.Risks involved with humans operating the labHuman error in measurementsHuman safety hazards: burns (acids, bases, fires, etc.), toxic gas, cuts (especially when using drills or cutters)Causing damage to equipmentIncorrect operational proceduresRisks involved with automation of lab processesVariables not programmed to be factored in by AI - these cases need human interventionDulling of blade, malfunctioning parts, etc.Introducing machine to dynamic or rapidly changing environmentAs you mentioned in your summary, end devices can be attacked or hacked as they lack sufficient security mechanisms.Blockchain can be used for material sciences in the context of supply chain management. Quality assurance and ensuring that data is tamperproof is something that the blockchain can provide; blockchain has the potential of tracing the usage of materials from source (manufacturers, miners - of metals, not of hash values, etc.) to consumer (researchers and different industries).linkedin.comBlockchain for Materiel Science and EngineeringMaterials science and engineering are key areas that deliver the products and infrastructure that people use and love all around the world. Here are the problems that currently plague the materials science and engineering space, and how to overcome...Reading time: 2 min read']"
                Collaborative Research Proposal: A taxonomy and genealogy of decentralization              ,https://www.smartcontractresearch.org/t/collaborative-research-proposal-a-taxonomy-and-genealogy-of-decentralization/1133,Meta,32,[],"['discussion', 'governance']","['In SCRF’s first community gathering 2 for this season the topic “what is decentralization” came up which sparked a very nuanced and insightful conversation on the limits and pitfalls of the usage of the term. People seem to be naturally interested in this high-level sort of discussion and have a lot to say from various different perspectives. What is more, I think many can relate to the feeling that these are conversations that we normally don’t have in web3 but should have.For this reason, I think it might be a good idea to start a research side-project that is a deep-dive into the different faces of decentralization, the historical roots of the term, its diversity of meanings in various scientific disciplines and its problematic usage in web3. A very brief preliminary foray into the territory reveals that the term is very multifaceted with a presence in urban planning, anthropology, systems biology and political theory way before it had started being used in finance. But it is only recently that the term has broken the surface tension of academia and reached the mainstream.As of now, there is no scholarly field called “decentralization studies” or a good foundational inquiry into decentralization and I think it would be very nice if we made some first steps towards that direction.This can become a cool discussion, a passion project, an SCRF initiative or it can be directed to external sources of funding such as Metagov’s grants 4 for research proposals if people deem that the project has value.I think @eleventh and @chris bates have already voiced their interest in something like this and it would be a good chance to engage in more critical discourse about the state of web3.What are your thoughts on this?', 'I am definitely interested in getting something more concrete going.', 'Thanks for joining the community call yesterday! I’m glad you enjoyed the conversation. It was a fun one to start the year with!We have actually discussed the idea of what a research project for decentralization could look like and @Sami_B put together a notable works on the topic 8. He also worked on a research prospectus on the idea and we’re going to catch up about that in a few weeks. We can chat in a future community call (or set up a breakout) to dig into the idea more.One thing to figure out is whether it makes more sense for us to support something like this or to run with it as a SCRF project. If we do it ourselves, what does review/oversight look like. We’re generally starting to explore what peer review means in a decentralized research context and don’t want to rush into anything before we dig into that question more.', 'You’re right. Decentralized Peer-Review on research about Decentralization definitely sounds like opening Pandora’s box.', 'I definitely am in line with this thinking concerning the process.  Sami’s research was more concerning the threats to decentralization, which necessitated him initiating research into the landscape of decentralization.  I think this gives us an opportunity to either explicitly deal with only a few types of decentralization on the forum while SCRF decides how to approach “decentralization” as its own subject.  On the one hand, it may not be SCRF’s responsibility to explore “all” of decentralization.  On the other hand, this may be one of the most appropriate places for that type of research to take place.  I just don’t want to get three years into SCRF’s existence and have all the forum posts not share a definition of decentralization.  That is a strong possibility if the discussion about “how” we are defining decentralization does not take place.It would feel intellectually dishonest to be years into SCRF’s existence to have a significant amount of posts about “decentralized” anything, and still not have had the discussion about “which parts are decentralized” and “what does decentralized mean in that context”?I only frame it this way because almost all of the projects I’ve ever seen claim to be a “DAO” or “Decentralized” had many aspects of their organization that were clearly centralized and it’s debatable whether they achieved decentralization at all.  If having a token on a decentralized platform is enough to qualify as “decentralized,” then does the term really have any meaning at that point?', ' Hermes_Corp:deep-dive into the different faces of decentralization, the historical roots of the term, its diversity of meanings in various scientific disciplines and its problematic usage in web3I think you’re bringing up some great topics for us to delve into during the community calls. Also if you, or anyone else, is interested in setting up a community event (or series) such as a reading group on the topic where we delve into different papers and write up a summary of the discussion on the forum, we should talk about that. Even if we don’t do a formal research project, there are other ways that some of these questions can be explored.We’ll most likely bring up the question of what decentralized peer review looks like in one of our coming community calls too.Also, there might be some research DAOs 2 that might want to fund research projects along those lines.', ' Larry_Bates:It would feel intellectually dishonest to be years into SCRF’s existence to have a significant amount of posts about “decentralized” anything, and still not have had the discussion about “which parts are decentralized” and “what does decentralized mean in that context”?Yeap, I definitely resonate with that sentiment. Abstracted notions of decentralization can be very misleading. An other important question is “what purposes does decentralizing this specific part serve?” and “would it be better of it is wasn’t?”. Larry_Bates:I only frame it this way because almost all of the projects I’ve ever seen claim to be a “DAO” or “Decentralized” had many aspects of their organization that were clearly centralized and it’s debatable whether they achieved decentralization at allI’ve noticed that too. I mean, decentralization is supposed to get rid of the managerial levels in organizations but supervision/management roles are becoming more and more important in well-established DAOs. What gives?@Larry_Bates how do you see this project going forward? Would you be interested too in setting up a sort of reading group about these issues as @eleventh proposed?', 'As others have pointed out, there is no reason to retread what @Sami_B has done, so maybe the initiative will need to do an examination of the notable works that Sami identified to then have discussions about how to move forward from there.  I know Sami’s interest was more in threats to decentralization, so this aspect of “the nature and context of decentralization” as a continuing line of examination seems to be a subject that needs to be examined further for the sake of getting some clarity before decentralization becomes vaporware.', ' Larry_Bates:As others have pointed out, there is no reason to retread what @Sami_B has done, so maybe the initiative will need to do an examination of the notable works that Sami identified to then have discussions about how to move forward from there.I think @Sami_B’s post was about measures of decentralization, not about the thing itself, as @Hermes_Corp is suggesting. I think it’s a shortcoming. If you don’t have a clear concept of the decentralization you want to measure, then you can pretty much pick and choose the metric that is most convenient for you, and make up a post hoc justification for your choice. Furthermore, the relationship between decentralization and its benefits (greater egality, trust, security, …) isn’t always super clear—clarifying that might tell us where centralization is a worry, and where it’s not, and therefore which metrics are relevant and for what.I agree that we can’t pursue decentralization for itself—indeed, it would lead us astray, as there’s many ways to understand it. But if we don’t pursue it at all, we’ll get confused, and we’ll lack the means to determine which tools work for which concept.Maybe, to avoid Larry’s worry, we could make it a more focused review. Like, focus on decentralization in the context of transaction security, or collusion, or democratization of the financial system.', ' locha:I agree that we can’t pursue decentralization for itself—indeed, it would lead us astray, as there’s many ways to understand it. But if we don’t pursue it at all, we’ll get confused, and we’ll lack the means to determine which tools work for which concept.Maybe a good first step you’d be something akin to a bibliometric analysis of where the word decentralization is found and how frequently. Related to that, what would be very fruitful would be defining a history of the term so that we can figure out from where it was initially adopted and reappropriated. I am pretty sure that finance is not where this originates (I have come across literature in political science talking about decentralization that dates back to the 70s or 80s). So we have to see what its original formulation was and why this notion was imported to finance.', 'Interesting, we (RnDAO) have started a conceptual framing of DAOs through an SCRF grant (towards creating a framework on analysis for the “common paths towards becoming a DAO”).Our research touches closely upon the discussion here, so at least I’d be happy to join such a discussion/reading group and cross-pollinate ', ' Larry_Bates:it’s debatable whether they achieved decentralization at allI’d add, or should.I would argue that there is a fine definition for ‘decentralization’ but that you have to contextualize decentralization to create insight for this space (or any space). Decentralization of what? Hermes_Corp:deep-dive into the different faces of decentralization, the historical roots of the term, its diversity of meanings in various scientific disciplines and its problematic usage in web3.Decentralization is just a concept. I’m not sure you could deep dive into anything, again, without contextualizing it. You have to frame it: Technology? Politics? Public Affairs? Social Structures? Management?I’m interested in exploring this if you want more help  . I agree that the usage is problematic in web3. It’s too entrenched in a political ideology that says nothing about the technology. I’m not sure you need a whole research study to break this down critically. I think there is value alone in critical analysis of political ideology in web3 sentiment and use of the word ‘decentralization’.', ' valeriespina:Decentralization is just a concept. I’m not sure you could deep dive into anything, again, without contextualizing it. You have to frame it: Technology? Politics? Public Affairs? Social Structures? Management?Exactly. This is the problem with people in web3 ranting about Decentralization without anchoring it to any context or reference whatsoever.But I would also argue that to have a better grasp of the concept, a more generalized notion of what Decentralization is, that is more general and can apply to most context is also needed. For example, decentralization can manifest as an actual tangible process in all kinds of systems, not only social and technological but also cognitive, biological and physical. Slime mold for example has a very decentralized structure with no central controller, but is still able to engage in intelligent behaviour, being able to very efficiently solve complex optimization problems.In my own opinion it is always a good strategy to look at the simplest of systems first (physico-chemical, biological) and then try to apply the observed phenomena to ones in higher levels of complexity (socio-cultural, cyber-physical etc). The reasonig is that patterns exhibited in simpler systems are also exhibited in more complex ones in a fractal fashion. See for example the HKB model where the same dynamics and phase transitions can appear in simple finger-wagging experiments can appear in simple mechanical oscillations, brain neurodynamics or the motion of crowds in cities.And beyond mathematically rigorous definitions, a philosophical investigation of decentralization is also incredibly important because philosophy can add a normative dimension to all these cases. Beyond being able to describe when a system or organization is decentralized and to what degree, we should, more importantly, be able to clearly define the value system according to which the object of study or observed behavior can be assessed and how to act.This can help to decouple the meaning of the term as currently used by technocratically-minded web3 folks from its unexamined ideological underpinnings and bring awareness to the biases and presumptions involved. Vitalik actually took a very good first step with his “The Meaning of Decentralization” article, but there is much more complexity and nuance involved in the philosophical inquiry into decentralization that ought to be uncovered. As you very rightly mention @valeriespina, there is value in the critical analysis of political ideology in web3 but critical analysis can only show what is already happening. We also need an ethical framework to show what ought to be done!', '@Sami_B and @Fizzymidas – you both worked on a project covering similar territory, what do you think of @Hermes_Corp proposal?', 'One of the issues becomes the definition of “decentralization” is not consistent between the computer science space and everything else that is non computer science’s definition of decentralization.  I will demonstrate.The image used in the definition of “decentralization” on Wikipedia that is not the computer science definition is as follows:image1920×1080 87.8 KBThe issue then becomes that a “distributed network” by the definition of “decentralization” would be MORE decentralized than “decentralized,” if the basic definition/interpretation of “decentralized” is“the process by which the activities of an organization, particularly those regarding planning and decision making, are distributed or delegated away from a central, authoritative location or group”This issue is always raised not to undermine the notion of decentralization but to point out that “decentralization” as a finite point creates its own paradoxes when you factor in “distributed” networks.  This is the main reason I have continued to push for more specific “types” of decentralization within conversations that are revolving around the notion of decentralized frameworks.']"
"                Paper: zkKYC - A solution concept for KYC without knowing your customer, leveraging self-sovereign identity and zero-knowledge proofs              ",https://www.smartcontractresearch.org/t/paper-zkkyc-a-solution-concept-for-kyc-without-knowing-your-customer-leveraging-self-sovereign-identity-and-zero-knowledge-proofs/839,Privacy,48,['https://eprint.iacr.org/2021/907.pdf'],"['zero-knowledge', 'summary', 'ethereum', 'privacy', 'network-security', 'iot']","['TLDR:zkKYC presents an alternative solution concept for KYC, one that is more human-centred, does not rely on upfront sharing of personal information with businesses and still enables a customer to be identified if and when that is required. To achieve this outcome, it leverages self-sovereign identity and zero-knowledge proofs, together with proper ecosystem design.Core Research QuestionHow can we remove the need for customers to share any personal information with a regulated business for the purpose of KYC, and yet provide the transparency to allow for a customer to be identified if and when that is ruled necessary by a designated governing entity (e.g. regulator, law enforcement)?CitationPauwels P., “zkKYC: A solution concept for KYC without knowing your customer, leveraging self-sovereign identity and zero-knowledge proofs”, July 2021, ia.cr/2021/907 39BackgroundPrivacy is a multifaceted topic and exists on a continuum. Some people are very protective of their privacy, even at a high cost. Others care very much in principle, but easily trade off their privacy for the benefit of convenience. Wherever one sits on this spectrum, it is obvious that privacy is typically a matter of trade-offs. At times transparency is required for regulatory reasons (e.g. to fight money laundering and terrorist financing) or for the greater good (e.g. medical research) and sharing personal information is considered an acceptable and necessary sacrifice. Simultaneously maximising privacy and transparency is the challenge for legislators, regulators, thought leaders and for each of us as individuals in our daily choices.SummaryBusinesses that are subject to AML/CTF regulation must meet their KYC obligations. In this context, to establish and verify a customer’s identity, the customer is currently required to share personal information with these businesses. This creates a Pareto dominated situation where a customer’s privacy is typically traded off for the mandated transparency requirements. In addition, this privacy erosion also reduces the security and safety of the customer as shared personal information can be passed on or stolen and used against the best interest of the customer (e.g. identity theft).Recent innovations in self-sovereign identity and zero-knowledge cryptography, along with smart ecosystem design, allow for a novel approach to KYC that protects the customer’s privacy without reducing transparency. The proposed solution concept, zkKYC, removes the need for the customer to share any personal information with a regulated business for the purpose of KYC, and yet provides the transparency to allow for a customer to be identified if and when that is ruled necessary by a designated governing entity (e.g. regulator, law enforcement, community governance council/DAO).A customer can privately prove they meet the eligibility criteria set by the business (or its regulator) such as domestic residency, minimum age requirement, no presence on sanctions list … all without disclosing any personal identifiable information as such. A designated ecosystem governance authority can however reveal the identity of an individual, but only when multiple parties (i.e. verifier and issuer) are agreeing to the need to do so and actively collaborate to such request. This makes identification at scale very hard by design, to make sure this effort is only focused on those events where it is absolutely required; on identifying bad actors.In summary, zkKYC breaks the traditional privacy vs. transparency trade-off and provides structured transparency, resulting in a net positive outcome for all parties involved.ApplicabilityAs zkKYC is built on top of a decentralised (self-sovereign) identity model, it is well suited to be applied for decentralised KYC. The obvious candidate use case for this is Decentralised Finance (DeFi).SCRF Presentation material on zkKYC used during community call 22 September 2021:zkKYC-SCRF-v0.01.pdf (710.8 KB)', '@darco Thanks for a fascinating research summary. I’ve seen the presentation you provided and the architecture of the solution certainly seems more elegant and useful that the current approach.Question: How would this become widely available — i.e., for “free” or for profit?', 'Welcome to the Forum, @darco. This is a great addition to our Privacy category, thank you so much for posting it! I understand that this is your own paper, so I’d be really interested to hear about your background as a researcher, and how you came up with this idea – where you working on similar projects in the past or is this an entirely new idea?', 'Hi @darco,Thank you for contributing. I’m curious what roadblocks to the inclusion of zkKYC in organizational structures as opposed to just “standard” KYC measures?Also, if companies begin to adopt this solution for data minimization, would it become easier for them to adhere to regulations such as the GDPR in Europe or other burgeoning versions of data protection regulations in other world powers such as India?', 'Hey @darco - Is there a way to reuse the verifier’s work? Suppose I did my KYC in the first bank, then went I visit the second bank, is there a way for them to not conduct KYC by themselves, and just quote the work done by the first bank?I’m thinking that in this way, we would be taking advantage of blockchain’s strength in enabling people to share and update information smartly.', '@darco First, thank you for the excellent work, I really appreciate your efforts on protecting people’s privacy and preventing mass data leakage!I’m just here to raise an example on how to make government more likely to accept our “GIP, Government Improvement Proposal”.jothon.g0v.tw揪松網 / 零時政府黑客松活動入口網帶上你的腦袋與雙手，直接到活動現場找尋志同道合的提案吧！零時政府黑客松由 g0v 揪松團舉辦，兩個月一次的單天活動，百名與會者來自不同的城市、背景、專長與年紀，利用三分鐘的時間將自己想做的事快速發表，並在活動現場尋找志同道合的夥伴，發揮各自所長，同時透過線上與線下協作的方式，用弄髒雙手的精神來「讓事情發生」。First, this is a civili hacking group in Taiwan, which held hackathons periodically. Instead of introducing it formally, I’ll tell a story and you’ll soon find out how our government started to embrace the projects generated here.In the beginning, it’s a bunch of geeks who wanted to build a better UI/UX to better represent government’s open data visually. It was: housing sales price data, court’s judgement data, traffic ticket data, or tax/household income data…And of course the news started to pick up projects, some misunderstandings from gov side also appeared “You’re accessing our data illegally.” Ppl started to realize that their work are being used, not just some hacky incomplete projects. Something like a diaster map(typhoon, flooding, etc) were also generated, and even used by the commanding center in the governement. (better UI/UX than internal system and whiteboards)The key takeout here is: even if the participants of the civil hackathon are decentralized, by having a regular hackathon schedule and a person/a placeholder org, where media and gov can contact - it forced the voted politicians to pressure government employees to understand such organization/hackathon, and their projects/mandates, etc.Later, ppl starting to realize that it’s not enough to reinvent needs for your customer if you were never one of them. Some of the projects are for usual people, and this is still ok cause we are all users to such project.However, in order to increase the leverage of the projects - there must be some projects that focused on improving government’s internal system and framework.So here comes a big one…spiler alert: some of them joined the government temporarily, and accepted a huge paycut (for the public!).MIT Technology ReviewThe simple but ingenious system Taiwan uses to crowdsource its lawsvTaiwan is a promising experiment in participatory governance. But politics is blocking it from getting greater traction.and its repo:GitHubGitHub - g0v/vue.vtaiwan.tw: 數位經濟法規線上諮詢系統 界面 3.0 - https://vtw.link/數位經濟法規線上諮詢系統 界面 3.0 - https://vtw.link/. Contribute to g0v/vue.vtaiwan.tw development by creating an account on GitHub.Civil hacking is not a new thing, IRRC there’s one in the UK, in Singapore, and in the Baltic states.Back to our main topic here: how can we make government accept this seemingly radical new paradigm of KYC?My answer will be: do it in a non-profit way, and do it frequently.  After you’ve gain that moral highground and news coverage…government and private sectors will contact you automatically, without the need to even persuade them.This may sound VERY farfetched - and it indeed is.We, as blockchain ppl, are essentially educating people of a new set of framework/paradigm. You never expect the time scale to be small…it’s 3yrs+. It’s also the same to educate government bureaucrats. And you better do it in a non-profit way so that ppl won’t question your motivation. (don’t worry about the competitors, trust me)Of course this is just one way of doing it…however it’s the most efficient way as far as I know - step by step.Hope that we won’t have another equifax data leak, and we can all have a future with kyc protected by zk primitives!', 'Hi @rlombreglia, thanks for your feedback!A good question (i.e. also a “hard” one). This is obviously an ecosystem play. I mean that not one party can build something and then can sell it. By nature this is a multi-actor ecosystem. However, I think you can start small where, for example, 1 party can take up multiple roles. This can help to get people used to the concept, and then gradually grow and extend.Regarding for “free” or for “profit”, there is a certain incentive model that is required:Issuers must be incentivised to perform high-quality identification of the Holder prior to issueing them credentials. This effort must somehow be rewarded.Similarly, Verifiers would very much benefit from reliance on high-quality credentials from trusted issuers. Being able to rely on them without having to build extensive identity verification capabilities themselves, is valuable, so worthy a fee.Last, parties that build these technology solutions would have to be compensated too for their effort and support.All this makes it clear that some kind of commercial model is required, but it should be fair, balanced and proportionate. Otherwise the ecosystem is out of balance and it will collapse. Tricky but worth pursuing, I would argue.', 'Thanks @jmcgirk!I have been working on some Digital Identity related initiatives in the past few years, with a particular interest in the Self-Sovereign Identity model as well a strong passion for Privacy Enhancing Technologies that I believe are very necessary in our increasingly digital world.Having worked in a regulated industry (Finance/Banking) before, I observed that even with DI and SSI, we still had to share personal information at many businesses we interact with and in my personal time I started thinking if (and how) there could be a way to avoid that from happening, but at the same time adhere to some principles such asIssuers not knowing where you use your credentials,An individual’s interactions across Verifiers should not be correlatable (i.e. a different pseudonym for each relationship)I loved the challenge of solving that “puzzle”, just for myself - feeding my curiosity. It was a long journey in my time off, with different ideas popping up (and being squashed). The hardest part was finding a way to hide who you were, yet providing a way for your identity to be revealed - especially if you were a bad actor (i.e. you would be dishonest). The Zero-Knowledge Proof technology is critical for this - you can hide, but not cheat.Once that became clear to me, I made it a personal project to structure and document this, hoping it might demonstrate and inspire people to consider alternative ways of doing what we have been doing for a while.', 'Hi @shoule!Great question re: the roadblocks to adoption of zkKYC. The biggest challenge I see is regulation. It would be valuable for regulators to clarify that for purpose of KYC (AML/CFT) an organisation can rely on cryptographic proofs based on credentials issued by third parties, without collecting personal information themselves. The lack of clarity on this topic will hold many organisations back from considering this.Also for regulators themselves, it will be a journey to get comfortable with such new technologies.Last, the ecosystem design of zkKYC also assumes that parties that take up the ‘Government’ role have a Digital Identity (i.e. a DID identifier with associated pub/private key). This is also something that I think will take some time to realise.Regarding your question on GDPR, absolutely! It is a great motivation for businesses to switch to a zkKYC-style approach. Solely being able to rely on cryptographic proofs and seemingly random tokens that are specific for your business only (i.e. no incentive to get hacked to steal them) is potentially very valuable: reduced cost, risk and liability exposure.', 'Hi @Twan! Great question.The short answer is Yes! See the concept of ‘KYC Issuers’ at the end of the paper (p.15). I can imagine that you go through full KYC verification at a regulated business (e.g. bank, KYC provider …) and then that business issues you (Holder) a ‘KYC Credential’. You can then use that KYC Credential to onboard at other Verifiers (businesses).This is also a good example of how 1 party that takes up the role of Verifier in one context, can play the role of Issuer in another. Great illustration of the difference between Parties vs. Roles.I hope that answers your question.', 'Hi @Jerry_Ho, thank you so much for your feedback. I appreciate it a lot and it gives me something to think about, thanks! I am aware of the digital innovation that has been introduced in the public governance process in Taiwan. It is truly inspirational. Any lessons learned from that experience are definitely worth considering! As any type of “revolution” of how things are done typically, it usually happens from the edges inward (and rarely from the centre outward). I agree that step by step, bit by bit, is the right approach. You implement it in smaller ecosystems, for free, and let the benefits demonstrate themselves, it is indeed the best possible sales pitch!Therefore, I think that in crypto/DeFi/… there are unique opportunities to leverage the zkKYC approach (I’m actually in the process of finishing a follow-up paper that focuses on an approach to implement zkKYC in DeFi). Crypto/DeFi is greenfield and has specific characteristics (e.g. the inability to store personal information) that would make for a suitable environment for zkKYC. I’m not arguing for KYC in DeFi, but merely that IF a DeFi project would consider KYC, THEN zkKYC might be a very suitable approach. This can also be helpful to inform and assure regulators that are looking at this space.It is a fascinating time and I like discussing the options to break the transparency vs. privacy trade-off. With modern technologies we can rethink our assumptions, rewrite the rules and redefine what is acceptable.', '@darco: Thanks for your reply to my hastily stated question. I agree with your points about a multi-actor ecosystem, and the need for a fair, balanced and proportionate solution.Obviously, individual actors (issuers, verifiers, developers) deserve to be compensated for any value they add. The potential problem, especially with issuers and verifiers, is monopolistic control. But this is possibly a non-issue in a genuinely decentralized system.What do you think?', '@darco thanks again for bringing this project to us, and being so generous with your time in answering our questions here. One of the reasons that the Smart Contract Research Forum exists is to connect engineers, systems architects and academics together. With that in mind, are there any technical or research obstacles that you face right now that you’d like to ask the community about? If you’d like to see an example of how another researcher brought an issue to the SCRF community, please visit Deep Diving into PRBMath, a Library for Advanced Fixed-Point Math 2', 'Hi @rlombreglia, I think you touch on a great and important point here. I believe this is the difference between technology design and governance design. We can design the technology such that it is decentralised, but when the governance of the services/solutions on top of this decentralised technology is centralised and favours monopolistic powers to particular actors, then we have gained only a part of what is possible.Verifiers are in theory permissionless in zkKYC, i.e. anyone can stand up and perform this role. For Issuers, this is rather different - those are the parties that are fundamentally trusted authorities. Verifiers will not accept proof of credentials from just any Issuer, but only from highly trusted Issuers, Issuers that have a proven high level-of-assurance (LoA) of their identification processes (see NIST Special Publication 800-63-3 for IAL, as well as AAL and FAL) prior to issuing verifiable credentials to the Holders.There is a risk that this reduces the number of such Issuers and that we see “centralisation” here, “gatekeepers” appearing. If Holders must get a credential from a specific Issuer in order to participate in the ecosystem, then we have still a sense of centralised gatekeepers. That is why a healthy ecosystem relies on a diverse and wide set of trusted Issuers. Some of them could focus on non-traditional forms of identification, in order to establish inclusiveness and minimize the chance of excluding people from participating in this digital ecosystem. I would say that ecosystem governance has a strong role to play here, to make sure these objectives are met and we avoid centralised gatekeepers.', 'Thanks so much @jmcgirk! I appreciate the welcome reception of the RSCF community, the curiosity and support. At the moment I have no particular plans to implement this solution, or research specific implementation challenges. Given the real-time interaction pattern for Holders, I can imaginen that finding and optimising the best suitable ZKP system for generating the Validity Proof is an interesting challenge. There are many varieties, with lots of innovations happening over the last few years. I believe the focus would be on speed of proof generation and verification, not the proof size as such. That is a possible interesting next topic to research.', '@Darco Thank you so much for a wonderful piece. The solution of privacy with transparency will be highly advantageous in this new web3.0 world. The KYC/AML/CFT requirements have thrived due to the prevalence and consequences of acts such as fraud, hacking, terrorism and the like. The concept of zkKYC extends the Self-Sovereign Identity (SSI) model, and allows data to be controlled (as the increased value of it has been realized). Removing third parties, and putting the user in control of their identity information will solve a number of privacy issues, as highlighted in the article. My concerns while reading, surrounded the monopoly that could be enjoyed by the Issuer and the Government (abuse of decryption power). You however allayed my fears with the proposition of the threshold cryptosystem by building in additional security of multi-person access before the use of private keys to access and decrypt the zkKYC. The issuers on the other hand seem to still enjoy some monopoly. How do we go about reducing that loophole? Could a hybrid decentralized approach work? I know you are working on research with the zkKYC token and its operability with DeFi.', 'Hi @LTTOguns, thanks for your thoughtful feedback! I much appreciate it. In addition to the threshold cryptosystem, I also want to note that Government needs multiple parties to collaborate to be able to reveal someone’s identity. First they need a Verifier to agree to share with them a zkKYC token. Then, assuming they pass the threshold to decrypt the token, Government also requires the specific Issuer to share the (personal) information associated to a Decentralised Identifier (i.e. DID(HI)) that they revealed after decrypting a zkKYC token. This makes a total of 2 parties (outside of Government) to collaborate with them in order to reveal 1 person’s identity. While this is all easier to accomplish in a digital world with automation and APIs and stuff, I believe it introduces multiple opportunities to check the legitimacy of such a request and mitigates the risk for large scale identification of individuals.To your second point, the monopoly by Issuers; that is a valid concern indeed. This can be mitigated by standardising levels of assurance for identification, so that multiple types of parties are able to issue (and be trusted to do so correctly) verifiable credentials that describe certain attributes of the Holder. While the SSI model is decentralised, there is a fundamental trust axis between Verifiers and Issuers. Verifiers need to know and trust the Issuers, they have to be reputable and trusted authorities in their field. Otherwise the trust that can be put in the credentials they issue is limited. I can only think of standards and accredation processes to broaden the set of trusted Issuers and address the risk of centralisation or even monopoly. We see the rise of Trust or Digital Identity Frameworks pop up, which aim to standardise these processes, in order to have multiple Issuers onboard in the ecosystem. Last, the commercial model should incentivise Issuers to step up and participate. Considering their critical role, they should be rewarded for the value they contribute or else the system is not sustainable.Keen to hear your thoughts on this.', 'For more information about zero knowledge and its applications, please visit SCRF’s zero knowledge tag 2. You’ll find a detailed breakdown of REDSHIFT, by @Sean1992076 explaining how the tech uses list polynomial commitments; there’s also @Jerry_Ho’s summary of Impact Award-winner @Ariel_Gabizon’s  PlonK: Permutations over Lagrange-bases for Oecumenical Noninteractive arguments of Knowledge ; @jasonanastas’ discussion about Zero Knowledge Proofs - an ethics perspective, @tina1998612’s summary of Zerocash’s 2014 Zerocash: Decentralized Anonymous Payments from Bitcoin  and many more!', '@darco was kind enough of to give us a presentation of this paper during one of our Community Calls. If you get a chance, be sure to check another paper of his that has just come out in the most recent Research Pulse 2 which proposes an AML/KYC verification system that could be layered atop DeFi decentralized exchanges. @Twan @Jerry_Ho @LTTOguns – you had some terrific questions, perhaps you’d consider extending the dialog to his latest piece?']"
                Mini-post: The Art of Auditing              ,https://www.smartcontractresearch.org/t/mini-post-the-art-of-auditing/519,Auditing and Security,88,[],"['discussion', 'summary', 'defi', 'testing', 'scalability', 'network-security', 'flash-loans']","['CTA: In these threads, we attempt to further the discussion of a key problem in this category and evolve our understanding of the domain space where research work has not yet answered the specific problem or question being considered. These posts are living documents, and it is our hope that the community will continue to contribute to their structure and content.Key Problem / Topic AreaThis post discusses the basic approach to a security audit, using Ethereum smart contracts as an example.Specific Question or Problem StatementHow should one perform code audits?BackgroundAn audit is an independent assessment of a project, seen through the lens of a particular concern. Examples of these concerns include security, compliance, economics, etc.In the crypto space, audits have largely focused on security, with their primary goal being to assess correctness (with respect to a specification), as well as to identify potential vulnerabilities in the implemented code. These are known as code audits.A code audit assesses the security quality of a target code, typically as a means to convey trust to the community at large.As in any other business-to-business relationship, an audit has an agreed timeline and scope, and its findings point to specific improvements in the code, ranked according to a severity scale.NOTE: If you are interested in improving the understanding of what is an audit, please visit the “What is an audit” 8 thread. Most of the definitions herein presented stem from that post.Approach / MethodologyEssentially, a code audit relies on two things: automation and manual review of the target code.Automation concerns the use of push-button scanners that automatically attempt to discover pre-defined bug types. The latter includes arithmetic overflow/underflow, reentrancy cases, assertion failures, transaction order dependencies, etc. For auditing of Ethereum smart contracts, two main tools are widely used: Slither 5 and Mythril 3. These tools simulate the execution of the target smart contract, attempting to find execution paths matching specific issues. In the realm of formal verification and testing, the underlying mechanism used in such simulations is known as symbolic execution.Slither and Mythril (and symbolic execution tools alike) can only identify a set of generic issues that are independent of the contract logic and business rules. Most of these tools also report a significant number of false positives (which get easier and faster to spot the more experienced an auditor becomes). Hence, findings are limited in scope. Human reviews generally aim to inspect the code in a line-by-line fashion, attempting to find issues dependent on the contract implementation and underlying business requirements. The quality of such reviews directly reflects the experience brought by the audit team.Unfortunately, the road to learning how to do a quality manual code audit is not straight. This is due to the fact that a generic and deterministic audit script (proprietary or not) does not exist; manual code inspection and the creation of an underlying mental model of the code and its related issues depends on human abstraction, which in turn depends on subjectivity and training. Hence, outputs vary from auditor to auditor.The best approach to becoming an auditor is to learn by doing. For training purposes, I recommend the following resources:Ethernaut 8 a game in which one hacks smart contracts, proceeding through phasesHack solidity 14: an online and hands-on list of videos showing on how to execute specific contract exploitsSmart contract security best practices 9: a curated list of security related-issues in smart contract development.From those, an aspiring auditor should be able to memorize the major issues in smart contract development.As a general yet non-deterministic guideline, an auditor should at the very least be able to perform two steps when manually auditing a smart contract:Given a specification (e.g., white paper, internal docs, external docs), an auditor should verify that the code is doing what the specification states. The goal is to check whether the code is in line with the business requirements. This is an intensive and tedious task. Essentially one reads the code, understands it, and makes sure it meets the given specification.From the previous step, the auditor should have been able to glean a mental model of the target code. The next step requires more insight. It involves investigating how a user might put the pieces of a contract together in a way that the original developers never intended. Here an auditor needs to have (or develop) an attacker’s mindset. This comes with experience. A good way to bootstrap that mindset is to read post mortems and replay the attack itself. Having built a mental model of the target code, one starts investigating potential threats, by means of asking questions like:Is it possible for any actor to put the contract in an unusable state?Is there a sequence of transactions that could cause the smart contract to misbehave?Are funds secure?Are there underlying assumptions made by the contract that may not hold?Are there interactions with external contracts? If so, are they trusted? If not, how could they exploit the current code?etc.Conclusions / Key TakeawaysWhile there is no single prescriptive approach, there are a few basic strategies one can take to start a smart contract audit. Additionally, in the context of having general strategies, it becomes unlikely that any two auditors will find the same set of issues.CTA: Future Work / RFPIt will be useful to continue to add to the list of auditing tools and strategies to ensure that auditors build upon each others’ experiences.', 'We tend to hear about audits and auditors after a major failure. How often do projects fail their audits? What happens then?', '@jmcgirk Thanks for your interest in this post. To answer your question properly, could you please elaborate what you mean by “projects fail their audits”?', 'I meant something along the lines of ‘how often do projects have major security issues that slip past auditors unnoticed’ but please feel free to interpret it how you like – I’m trying to understand what role auditors play in the environment', 'That is a great question, and currently, I am unaware there is any data available assessing the effectiveness of audits in actually preventing hacks. But in all honesty, such data can be misleading. For instance, can one conclude that an un-hacked audited project is indeed secure? The answer is a plain no, as there are always unknown unknowns.To assess the effectiveness of an audit in actually preventing attacks, one must first be aware of the scope of the audit, and that is often not done. As discussed in “What is an audit post 2”, there are different audit types, each focusing on a particular concern. Examples of the latter include: code audits, economic audits, legal audits, etc.Generally, audits in the crypto space are limited to code audits, but I dare to say that the latest hacks we have seen in the DeFi space are a mix of economic issues (e.g, the attacker manipulates a coin price in a given exchange to buy it cheaper elsewhere), as well as flaws in the code itself (e.g., use of unreliable oracles, missing slippage limits, etc).If we continue to limit audits to code audits only, while  disregarding the composabilty of the comprising components and the resulting economics, hacks will continue to make big headlines. To conclude, code audits can only go as far as their scope of analysis. If the project at hand gets hacked, that is not to say that its audit failed.  An audit is NOT a statement that a project is safe and free from bugs. An audit reports what has been found to be wrong with the best effort possible given the agreed scope and timeline.', 'This is a really astute point! lnrdpss:If we continue to limit audits to code audits only, while disregarding the composabilty of the comprising components and the resulting economics, hacks will continue to make big headlines.There would remain a critical need for code audits, but as an industry, what would be needed to go beyond code audits as the accepted standard? It seems like economic or law audits would introduce subjectivity or probability that might be interpreted as being less concrete than a code audit.', 'There’s an entire movement/practice geared towards better understanding the game theoretical/economic dynamics and pressures associated with the creation of novel economic systems, and providing economic audits called “Token Engineering.” Its a fairly novel area of study and collaboration and is pretty fascinating. There are a number of folks and organizations working in the space, and its something that I’ve personally stayed interested in learning more about. The Token Engineering 3 community are a decentralized group of contributors, and folks like Lisa Tan 2 are working actively to contribute to education on the subjectI’ve been wondering what other resources are available to folks to better understand typical smart contract exploits, the Consensys Dilligence Smart Contract Security Best Practices repo is a great one, they also have a Smart Contract Security Newsletter 1, primarily (exclusively?) penned by @root, there’s also Open Zeppelin’s List of Ethereum Smart Contracts Post-Mortems and the less academic but very informative https://www.rekt.news/ 2.What others are out there?', '@zube.paul Since we are largely an unregulated industry, the accepted standard shall be whatever users end up accepting prior to locking their money in a given crypto product. Right now no one launches a product without a code audit, but economic audits are catching up and to some extent so is formal analysis. But ultimately, the market pressure sets the tone, and no one will go above and beyond until the market forces and underlying risks say so.However, I do acknowledge that the answer above is somewhat shallow. To mitigate that and give you a more objective reply, I believe the standard should be along the following lines:layer 1 products, i.e., any implementation of the a given blockchain network (consensus layer, p2p layer, storage layer, etc) should have different types of audits depending on the component under analysis. For instance:- consensus layer: economic (generally based on game theory analysis) + code audits + some level of formal verification- p2p layer: code audit + simulations (stress testing some scenarios)- storage layer: code auditExample of products that did an excellent job here include Ethereum 2, Cardano, and Casper Labs (note: this is not an exhaustive list).as for smart contracts (layer 2 audits), I would recommend:- critical parts: formal analysis- code audits for every other part- if the product at hand depends on economic factors brought by external components, then have an economic audit as wellCompound and Aave are great products close to fully meeting this criteria.Hope that helps.', '@Eric Thanks for bringing in such a rich set of references (note that some are already part to our list of notable works). The Token Engineering 1 resource seems particularly interesting. Thanks for sharing that.For code audits, you pretty much covered all the main ones. I would only add a few more:246 Findings From our Smart Contract Audits: An Executive Summary 4All major audit companies publish their audit reports whenever authorized to do so. This includes Quantstamp public certificates 1, Consensys Diligence public audits 1, OpenZepplin’s 1, and Trail of Bits smart contract audits 1. These are great resources to learn in the wild, as issues are ranked according to their severity. Also, exploit scenarios are sometimes presented depending on the severity of an issue.For economic audits, I highly recommend looking at the work that Gauntlet is doing. They do list some use cases that are similar to code audit reports, but from a financial and economic perspective. As an example, consider looking at their report for the Compound protocol. Other reports are also publicly available.Hope that helps :)', 'Pretty interesting thread and ideas here. Here are some random thoughts:Regarding the separation of “code audits” and “economic audits”, as you mentioned with DeFi composability (DeFi Legos), it’s becoming challenging to separate those two, as many of the recent hacks use some kind of price manipulation that falls outside the code audit but is expected from auditors to find.Indeed this is an unregulated space but as an auditor, and no clear guideline on what is the full extend of an audit. One approach is to try to get developers more security-aware which some resources has been posted in this thread, as well as Token Interaction Checklist 1. That being said, my opinion is that one of the main challenges auditors face is time-boxed audits. To expand on this, how would you scope a project and give an estimate sufficient time for the audit? As an auditor you may want to spend more time reviewing the code, but as the developer team, you want to pay less and launch fast.As an experiment, go through the public audit reports and notice how many of them mention “time constraints” or “best effort”. It’s more than you think. Sometimes it’s mainly due to Curse of Knowledge 1, meaning the developers think their code is simple enough to be reviewed in one week, however, it takes at least a few days to understand the overall system, not to mention doing proper code audit (and on top of that economic audits). Eric:I’ve been wondering what other resources are available to folks to better understand typical smart contract exploits, the Consensys Dilligence Smart Contract Security Best Practices repo is a great one, they also have a Smart Contract Security Newsletter, primarily (exclusively?) penned by @root, there’s also Open Zeppelin’s List of Ethereum Smart Contracts Post-Mortems and the less academic but very informative https://www.rekt.news/ .What others are out there?I took over Smart Contract Security Newsletter 1 from @maurelian and ran it for 2020, however mostly due to packed schedule, too many DeFi hacks to cover, and having similar newsletter out there, there hasn’t been any new issues in 2021. For those missing the newsletter I suggest checking out Blockchain Threat Intelligence 5.Another resource we maintained for a while but hasn’t been updated recently is Blockchain Security Database 1, trying to create a human & machine readable (JSON) database of audit reports for researchers to use. It’s open source on Github for anyone that is interested to keep this database alive.', '@root For the Blockchain Security Database, I see the full list of json files in blockchainSecurityDB/projects at master · ConsenSys/blockchainSecurityDB · GitHub 2. Is it the case that all these files have the same structure? If so, where can I find a schema for them? By the way, I love this initiative of having a machine readable and open database of audits.', 'they all follow the below structure, and the html is generated using a script:{  ""project"": ""Project Name"",  ""project_url"": ""Project URL"",  ""description"": ""Description of your project"",  ""bounty"": ""Bounty program URL"",  ""bounty_max"": ""Max bounty payout"",  ""security_contact"": ""Security contact email"",  ""audits"": [    {      ""repos"": [        {          ""url"": ""URL of the repo pertaining to audit""        }      ],      ""title"": ""Audit title"",      ""date"": ""mm/yy"",      ""auditor"": ""Name of auditor"",      ""url"": ""Audit URL"",      ""effort"": ""Person-weeks effort of audit""    }  ]}Would love to see the database resuscitate :)', 'Oh, I thought the schema would define a format for the findings as well, which is the most important part of any audit. Without that, one cannot easily aggregate data of the findings, nor query them. We, as a community, should start thinking of a schema that could be used across companies, and ideally, have a public (and queryable) database of findings. Thoughts?', 'That was part of the initial goal. However lack of standardization in the audit report made it impossible to automate. I think it is still manageable to do but requires collaboration between audit firms and manual work to input the findings in the defined schema.If we can create that schema here, I commit to spend sometime to populate the findings from ConsenSys Diligence’s public audit reports.', '@root I was wondering if first we should consult the various market players and see if there is interest in defining a common schema. Alternatively, we could define our own and then try to create scripts to automate the export task. Not an easy task, specially when handling pdfs. I would still lean towards the first option. Thoughts?', '@Astrid_CH @Twan this thread might also be of some interest to you – it’s a nice overview of auditing in the crypto world. It would be interesting to hear about how it might overlap with auditing and compliance in the traditional, centralized banking systems.', 'I really appreciate this detailed explanation.  Pardon my ignorance, but are there any opportunities for blockchain auditing for those who have a non-technical background? I’m completely new to the (blockchain, coding, crypto, etc.) space.  My background is in title examination for real estate underwriting, and I recently started exploring IT Auditioning.  Blockchain technology is really intriguing to me, so I’m just researching how to get in the door.', 'Hi Dorrii, it seems that you have prior experience on legal documentations writing.When it comes to auditing, broadly speaking there’s two form of it in the IT industry.Audits for ISO/ERPe.g., internal IT systems, regulations, compliances, for publicly traded companies.Deloitte CroatiaAudit | Deloitte | Audit Services | Solutions 1Accounting Advisory Services Changes in accounting legislation present new challenges.Of course, something more technical such as risk assessment, penetration testing and maybe red team exercises will be part of this compliances auditing.But, most of the time, it’s more of a “if you have this in place, tick it” kind of auditing.This kind of auditing is less common in the blockchain realm. You can still see that in some defi projects, they have to follow rules like the imfamous travel rule. AFAIK some services like the Perpetual Protocol banned user from the US completely, in order to comply with this rule.Anyway, although this seems off-topic, you can still understand from the example that this kind of audit is hugely different from the code audit.I myself have no idea what kind of companies do these for the blockchain industry. Maybe try the big 4 (accounting firm)?Audits for codeThese are almost all covered in the original post.For starters, you can try to play some CTFs(capture the flag) for beginners aiming on smart contract security.Or you can even try to rediscover vulnerbilities in some famous incident, such the parity multisig wallet, or the dao incident…, by following the guides/tutorials in the original post. Try to come up with your insights before looking the detailed write-up posts.Generally, after you accquired some of them, you can try hackathon by Quantstamp or other famous auditing company in the Blockchain realm, to see if they’re interested in your progress. AFAIK, Quantstamp is willing to train junior level people for contract auditing.Disclaimer: I’m not affiliated with Quantstamp in any way.', 'Thank you, Jerry.  Your response is very much appreciated!', '@lnrdpss very well said.  The trend in technology will definitely impact auditing systems.  The key aspect is that the auditors need to be abreast with the trend and develop measures to review systems as they continue to evolve.']"
                Research Summary: A Comparative Analysis of the Platforms for Decentralized Autonomous Organizations in the Ethereum Blockchain              ,https://www.smartcontractresearch.org/t/research-summary-a-comparative-analysis-of-the-platforms-for-decentralized-autonomous-organizations-in-the-ethereum-blockchain/955,Governance and Coordination,33,['https://www.researchgate.net/publication/349034784_A_Comparative_Analysis_of_the_Platforms_for_Decentralized_Autonomous_Organizations_in_the_Ethereum_Blockchain'],"['dao', 'summary', 'discussion', 'governance']","['TLDR:This paper analyzes data from 72,320 users and 2,353 DAO communities on three main platform ecosystems across four dimensions: growth, activity, use of the voting system, and the funds owned by DAOs.By active DAOs, Aragon is three times more active than DAOhaus, and eleven times more active than DAOstack; by active users, Aragon is 27 times more active than DAOhaus, and five times more active than DAOstack. (Faqir-Rhazoui Y., et al., 2021)Within the scope of this research, Aragon is the biggest platform in many aspects, thus we may conclude that the ability of Aragon for customizing DAOs may reveal some essential features for developing DAOs. (Faqir-Rhazoui Y., et al., 2021)The cost, decision-making system, and customizing functions of each platform may significantly cause the adoption and activity of DAOs on them.Core Research QuestionHow is the performance of DAOs on the main platforms that provide DAO deployment as-a-service so far?CitationFaqir-Rhazoui, Y., Gallardo, J. A., & Hassan, S. (2021). A Comparative Analysis of the Platforms for Decentralized Autonomous Organizations in the Ethereum Blockchain. Available online: https://www.researchgate.net/publication/349034784_A_Comparative_Analysis_of_the_Platforms_for_Decentralized_Autonomous_Organizations_in_the_Ethereum_Blockchain 6.Faqir-Rhazoui, Y., Gallardo, J. A., & Hassan, S. (2021). A Comparative Analysis of the Adoption of Decentralized Governance in the Blockchain Through DAOs. doi: 10.21203/rs.3.rs-166470/v1 2.Faqir-Rhazoui, Y., Gallardo, J. A., & Hassan, S. (2020). An overview of decentralized autonomous organizations on the blockchain. Proceedings of the 16th International Symposium on Open Collaboration. Association for Computing Machinery, New York, NY, USA. doi:10.1145/3412569.3412579 1.LinkA Comparative Analysis of the Platforms for Decentralized Autonomous Organizations in the Ethereum Blockchain 6A Comparative Analysis of the Adoption of Decentralized Governance in the Blockchain Through DAOs 2An overview of decentralized autonomous organizations on the blockchain 1BackgroundGas: The amount of computation required by an operation on Ethereum. Its amount depends on the size and type of each operation. The price for each unit of gas is determined by the market.Decentralized Autonomous Organizations (DAOs): There are many definitions of a DAO. The working definition that the authors use in this article is:“A DAO is a blockchain-based system that enables people to coordinate and self govern themselves mediated by a set of self-executing rules deployed on a public blockchain, and whose governance is decentralized, that is, independent from central control”. (Hassan, S., De Filippi, P., 2021)The authors break down the definition of DAOs further into interrelated parts:Decentralized: A DAO exists on a public blockchain that is serverless, censorship-resistant, and cannot be controlled by a central party. A DAO and its activities must depend on the consensus of its participating members, which is achieved through a voting structure.Autonomous: A DAO is autonomous because its existence relies on its members. The members determine the set of rules that the self-executing codes the DAO must follow.Organization: A community that interacts towards shared and agreed-upon outcomes.Aragon 3: It is the largest and most mature of the three platforms, with a variety of decision making apps (sets of smart contracts) to choose from as well as customizable organizational templates. The voting apps range from basic functionality and parameters in their native Vote app to more experimental ones such as Conviction Voting where individual preferences are captured using tokens and continually expressed as votes on relevant proposals. (Faqir-Rhazoui Y., et al., 2021)DAOStack 3: It introduces the Holographic Consensus system in an attempt to address one of the scalability issues associated with DAO growth, where the requirement of an absolute majority becomes less practical as proposals increase. The idea is to use tokenomics as a filter on proposals where members stake coins to make bets on if a proposal will pass or not. Proposals that reach a baseline will pass via relative, not absolute majority. Thus in theory members are incentivized to be aligned with the DAOs best interest as their bets depend on choosing the right outcomes. (Faqir-Rhazoui Y., et al., 2021)DAOhaus 2: It is built on top of the Moloch DAO framework. The Moloch v1 voting system does not require a quorum and a relative majority can pass a proposal. Proposals consist of a combination of requesting shares from the members or offering tributes which function as a donation or a deposit. In Moloch v2, one of the main updates is that non-DAO members can submit proposals for voting only through sponsorship (deposit) of a DAO member, where the sponsoring member will receive part of the deposit back after voting has been completed. The DAOhaus platform has a unique feature that grants members the the ability to “rage quit” which is the option to exit a DAO and keep all of their vested funds. If more than a ⅓ of members rage quit on a proposal, it will not pass. (Faqir-Rhazoui Y., et al., 2021)xDai network: It is a sidechain of Ethereum that provides fast and inexpensive transactions compared to the Ethereum mainnet but sacrifices decentralization to some degree. As the gas price soars, the DAO platforms search for alternatives for Ethereum to avoid its expensive cost. The xDai is one of the successful cases. The table below shows the comparison of the prices of two DAOhaus operations and the average speed in both the Ethereum main net and xDai networks in October 2020. (Faqir-Rhazoui Y., et al., 2021)SummaryThe researchers start with an overview of the DAO platforms. Including the introduction of DAO and the three main platforms that provide users with customizing DAOs templates tools service.The general approach of this paper is to retrieve and compare relevant statistics from platform inception to Nov 30, 2020. The data are split into four categories: growth, activity, voting system use and total funds.Growth and activity are helpful indicators of adoption levels and the paper gathers available data on the aggregate and active number of DAOs and their users. Voting system use is measured by voter participation and proposal statistics. Total funds provide further insight into overall adoption.The authors also include DAOs on both the Ethereum mainnet and xDai network in this analysis.MethodThe authors compare the performance of the three main platforms: Aragon, DAOstack, and DAOhaus by quantitative analysis. They aggregate data then give an explanation.The authors accessed the blockchain data of these DAO platforms using the indexing protocol called The Graph. These platforms each have “subgraphs” that present the data as an API and the GraphQL language is used to fetch and run database queries.The datasets produced and used in this paper are available in a Github repository and using an online visualization tool. Unfortunately the server for this tool is down as of 09/2021 but the source code for the tool can be found here. The DeepDAO web tool was used to gather cryptocurrency funding data.ResultsThe table below shows the data of the three DAO ecosystems in terms of their number of DAOs, users and proposals. It’s noticeable that Aragon started using xDai in July 2020, DAOhaus started using xDai in July 2020, and DAOstack started using xDai earlier, in February 2020. The authors argue that this may explain why the adoption of xDai in terms of users and DAOs in DAOstack is significantly higher than in the other two platforms.741×131 20.5 KB(Faqir-Rhazoui Y., et al., 2021)GrowthTwo metrics are used for the comparison of the platforms concerning growth over time: the number of DAOs and the number of users.Lacking data:The timestamp of the DAO creation for DAOstack.The timestamp of the user registration for Aragon DAOs.These data do not reflect when a xDai DAO is new or the result of a migration process.The authors point out that the number of DAOhaus’s users could be higher because 311 people used the ’rage quit’ option during the period analyzed and such an option is not available on other platforms, where users just abandon their accounts and do not use them.857×625 140 KB(Faqir-Rhazoui Y., et al., 2021)863×632 127 KB(Faqir-Rhazoui Y., et al., 2021)ActivityThe authors define the meaning of ‘active’ for both a DAO and a user as the same definition in a piece of their previous paper An overview of decentralized autonomous organizations on the blockchain 1 that considers that “a DAO or a user were active in a given month if at least they performed an action in that month. The available actions to be performed depend on the platform”.The authors point out that the approach followed may cause a highly conservative estimation of Aragon. So its number should be higher than the figures showsThe authors believe the peak in October 2020 of Aragon in figure 4 could be due to a migration to xDai.859×641 167 KB(Faqir-Rhazoui Y., et al., 2021)856×638 173 KB(Faqir-Rhazoui Y., et al., 2021)Voting systemThe authors take four metrics to compare the governance and voting system of each platform:The percentage of users who vote — for the observation of the engagement of the DAO community.The number of cast votes per voter — for the observation of how active voters are in terms of participation.The percentage of proposals that are approved— for the observation of how the voting system may influence the results.The percentage of positive votes among those cast.The authors explain that the inactivity of a DAO with around 4,000 users may cause the percentage of voting users statistics for DAOstack, while there is a high number of inactive DAOs in Aragon.The authors provide a potential explanation for the percentage of users who vote in xDai is smaller than in mainnet is that xDai is a younger alternative, even if it is cheaper.Pertaining to the ratio of votes per voter, DAOhaus has an xDai ratio almost double that of the mainnet. The authors argue this could mean that xDai boosted participation.In order to reduce costs, proposals may be discussed off-chain within the community, then be put to on-chain vote if it is likely to be approved. This explains the high percentages of approved proposals on every platform.The authors present hypotheses for differences in of approved proposal percentages between every platform, while also pointing out that these conclusions should be validated through further studies:DAOstack voting system: Because it requires either an absolute majority (51%), or enough staking for a proposal to be ”boosted” and thus able to be approved by a relative majority, it shows a lower number of approved proposals.DAOhaus voting system: It requires no quorum, a relative majority can approve a proposal, so it shows a higher number of approved proposals. Furthermore, in DAOhaus v2, a proposal requires sponsorship of a community member, this also bats proposals that may be rejected.Aragon voting system: Its standard voting app requires a minimum number of members to approve a proposal, so it makes sense that its approval rate is lower than DAOhaus. On the other hand, its voting system leads to lower rejection rates than the DAOstack system.917×184 31.8 KB(Faqir-Rhazoui Y., et al., 2021)FundsThe table below shows the Top 10 cryptocurrencies in terms of DAO adoption, which is calculated by the number of DAOs that use them. In this summary, we also update the tables in the paper (Table 4 & Table 5) which were retrieved on the 1st of December 2020, and the latest accessible statistics on the same websites which were retrieved on August 2021.The authors point out that it is important to bear in mind that the funds of a DAO are dynamic as it has inflows and outflows.Many of those crypto-currencies are stablecoins (DAI, SAI, USDC, or USDT).857×356 56.4 KB(Faqir-Rhazoui Y., et al., 2021)(Retrieved August 2021)852×321 58.6 KB(Faqir-Rhazoui Y., et al., 2021)(Retrieved August 2021)Discussion and Key TakeawaysAs Table 2 shows, Aragon is 10 times larger than DAOhaus and 79 times larger than DAOstack. While it is noteworthy that from Aragon’s 1,700+ DAOs (2,000+ including xDai DAOs) and 41,000+ users (68,000 including xDai), there are only 100 DAOs and 330 users that are active each month. So the authors conclude that by active DAOs, Aragon is three times more active than DAOhaus, and 11 times more active than DAOstack; by active users, Aragon is 27 times more active than DAOhaus, and five times more active than DAOstack. (Faqir-Rhazoui Y., et al., 2021)Implications and Follow-upsAs the paper was written, eight of the top ten wealthiest DAOs rely on the Aragon platform, so Aragon’s ability for customizing DAOs may reveal essential features for developing DAOs. However, the number of active DAOs within Aragon has declined since May 2020. (Faqir-Rhazoui Y., et al., 2021)DAOhaus had the highest percentage of proposals (92%) passed which may be due to its voting system. (Faqir-Rhazoui Y., et al., 2021)The problems in Genesis DAO, a DAO aimed to promote the use of DAOs through DAOstack, may cause the stagnation of activity and adoption of DAOstack. (Faqir-Rhazoui Y., et al., 2021)The arrival of Ethereum 2.0 which is expected to reduce gas prices, could have a significant influence on the relevant consequent quantitative research. (Faqir-Rhazoui Y., et al., 2021)ApplicabilityThis quantitative analysis of the three main DAO ecosystems shows how the different mechanisms and functions of those platforms may affect the adoption and activity of the DAOs on them. This information could provide industry with clues about the direction of DAO development.The authors also try to explain every statistic in their paper. This could serve as a reference for those platforms to evolve.', '@Astrid_CH thank you so much for posting this fascinating summary, which is particularly interesting to me as I explore creating a DAO of my own. Could you please expound upon what some of the potential limitations of this study are? Are there other factors, such as major investments or marketing campaigns that might explain the difference between Aragon and the other two platforms? Are other reasons for Aragon’s ‘victory’ besides an appealing basket of features?', 'Interesting research. Let me know more about DAOs.', '@EasonC13 I’m so glad to know you like it. @jmcgirk  Thanks for your questions and comments. From my own observation, some other factors that might explain:Aragon is the earliest one between them, which means that it could catch eyes with some blasting form. It also performs development progress at a good pace and updating of codes.Aragon has greater social media community attention, such as Twitter and Reddit users. For example, the followers of Aragon, DAOhaus, and DAOstack’s Twitter today (2021/11/6) are 90.4K Followers, 10.2K Followers, and 8,753 Followers, respectively.Aragon is more focused on providing general and easy-to-use modules that are both applicable to traditional centralized entities and decentralized communities, while others are more dedicated to providing infrastructures for decentralized forms.Aragon provides better UX for non-technical background users with clear and readable (from a non-technical perspective) UI.The promotion of their tokens, such as the number of exchanges their tokens listed, tokens’ capital, the advisors’ backgrounds, famous use cases projects, all further affect the investors’ confidence and users’ choice, and it’s just like a trust spiral. As the authors said, “Given DAOs are an early field relying on a novel technology, growth by early adopters is critical for the future mainstream adoption”. It’s probable that before something big happens, their development may follow rules like “The winner takes it all”.There may be some potential limitations of this study:The authors mentioned that many DAO cases on these platforms are built for the purpose of experiments. I believe this may affect the result, especially for platforms that are easy to try.It seems that the paper omits Aragon Chain, which may affect a little bit in the context of the high gas fee on Ethereum.As the structures behind DAOs may not be unveiled thoroughly to the public, it couldn’t discuss the DAOs for decentralized communities and DAOs for centralized entities separately.The authors also mentioned some limitations of the collection of data, including (Faqir-Rhazoui Y., et al., 2021)xDai-“there is no available data about the proposals in xDai in Aragon”Growth-“the timestamp of the DAO creation currently is not available for DAOstack DAOs, while the timestamp of the user registration is not available for Aragon DAOs”, “our data does not reflect when a xDai DAO is new or the result of a migration process”Activity-“for Aragon xDai actions, we can only consider data from the Transaction app because the API does not provide data from the Voting app in xDai…may result in a highly conservative estimation in Aragon, where DAOs can be customized with different apps, and, hence, exhibit other types of activities”Voting system-“In the case of Aragon, DAOs may have multiple voting systems. However, for the sake of simplicity, we just retrieved data from the standard voting app”Funds-“DeepDAO does not provide information from all the DAOs in the considered ecosystems”The definition of ‘active’ may also play a significant role.', 'The metrics prompted me to ask what a hypothetical product manager should prioritize to gain a larger market share. I’m still thinking and remain open.However, that question would be only a piece of the whole picture. The next steps on improving a metric could be challenging.Judging by Astrid’s previous comment, Aragon’s advantage does seem to go beyond the metrics included in the summary, such as a better developing experience, and more existing users.Yet it’s not clear to me now what the key differentiating feature of these DAO platforms would be between each other. It feels like a million-dollar question.', 'Thank you so much for a really detailed, interesting answer. It’s really interesting to think of this study in the context of the marketing, UX, and early mover advantage that Aragon has. What is the Aragon Chain? Is that their token? And how does it compare to $HAUS and other DAO tokens?We’ve run a couple of different summaries about DAOs, including @kelsienabben and @quinndupont’s work. Nabben established a framework for defining and analyzing DAOs, what might her work have to say about the relative success of the three platforms? DuPont’s work looked at the aftermath of a notorious hack; how do the three platforms take security into account?', 'Aragon Chain is a blockchain built for optimized Aragon usage that is scalable, high-throughput PoS, and fully compatible and interoperable with Ethereum. It’s interesting to think about how do these peripheral infrastructure projects affect the popularity of platforms. It needs deeper investigation.Considering platforms present various functions and are ruled by different tokens, comparing tokens’ values we need to look into their functions and underlying assets. There are three tokens in Aragon ecosystem, including ANT, ANJ, and ARA. ANT is Aragon network governance token, the similar token in DAOhaus ecosystem is HAUS. ANJ is created for incenting and managing Aragon court’s jurors. ARA is the native token of Aragon Chain that takes ANT as its underlying asset.', ' Astrid_CH:The table below shows the data of the three DAO ecosystems in terms of their number of DAOs, users and proposals. It’s noticeable that Aragon started using xDai in July 2020, DAOhaus started using xDai in July 2020, and DAOstack started using xDai earlier, in February 2020. The authors argue that this may explain why the adoption of xDai in terms of users and DAOs in DAOstack is significantly higher than in the other two platforms.It seems that the adoption time of xDai influences the user percentage which uses xDai. However, the Aragon has more xDai user because it is bigger and easy to attract users to use it on xDai. Therefore, the total user amount is critical to the influence of the blockchain technology adoption.', 'Hi  @EasonC13 , I agree with you, the total user amount plays a significant role in many facets. As @Twan  said, the reasons for platforms adoption could be a million-dollar question.In DAOstack, there are more DAOs on xDai than on mainnet, while the number of users on xDai is less than the number on mainnet, and the number of users who vote on xDai is only one-third of the number of mainnet. I’m thinking if there are many experimental DAOs on xDai in DAOstack that cause this result. But I can’t tell the motivation from the statistics, so this is still a hypothesis. I’m looking forward to more researches into the causes of their popularity, which may need some interview methods with communities participants.', 'Hi, @Astrid_CH, I read your summary carefully and could get valuable insights and trends of recent Ethereum DAOs. Thank you.I agree that Aragon is one of the highest-profile and most capitalized DAO projects. Aragon works a bit differently than other DAOs, and it’s mainly due to its unique structure of decentralized digital jurisdictions.One of the uniqueness is Aragon Court. The legality of the Aragon network is given and sustained by the Aragon Court. It is a judicial mechanism composed of smart contracts. Community members can participate in the Aragon Court’s judicial process as jurors, plaintiffs, or defendants, but they must put down a minimum stake of 10 000 ANJ tokens. ANJ is an ERC-20 token that only has use within the Aragon Court. Participants have to create a smart contract and deposit ANT tokens to mint ANJ tokens before applying for an Aragon Court role. The decision-makers in the Aragon Court are the jurors who are randomly selected from a pool of users previously enlisted in the process. Their decisions are made independently and anonymously, so the risk of tilting the scale of justice one way or another is minimal. Jurors receive incentives in ANJ tokens for their participation. Therefore, users are motivated financially to take part in the democratic process.Aragon is working on issuing another ERC-20 token, called ARA, which will power the Aragon Chain. The Aragon Chain will be a second layer of the Aragon network that will use Proof-of-Stake (PoS) to enable quick, cheap, and low-risk transactions between the many communities Aragon platform. This protocol is still “under construction,” and its use of ARA tokens will be possible by bonding them to ANT tokens through smart contracts.', 'Hi @stayhungry07212 , thank you very much for your valuable comment and detailed introduction about Aragon. As a potential user of DAO platforms and a legal researcher, I’m curious about how Aragon Court reduces the risk of judgment with prejudice or preference. In democratic countries, judiciary departments are often independent and ensure justice by giving judges economic and positional assurance, if a judge conducts malpractice out of personal considerations usually commits a crime as well. But I can only see the ways Aragon court takes to prevent injustice from their jurors are by staking tokens and a random choosing process. I have participated in a team that is preparing to launch a DAO, what our team members are discussing is that what if someone gets close to those jurors (if he knows who is a potential juror)   and gives them incentives that are bigger than ANJ gives? When the decision-making mechanism is not “decentralized”, the trust issue on DAOs remains and the advantage of building it on blockchain would be blurred if the issue isn’t disposed of well.', '@Astrid_CH, The Aragon Court has the role to oversee the voting of community proposals, settle disputes between participants, and steer the network’s decisions according to its roadmap and align with the Aragon Manifesto’s core values. Aragon has the wild ambition of creating a purely democratic society where countless communities operate under their own governance rules, but still under the Aragon Manifesto. It sounds promising, encouraging, but idealistic. We know from real-world experience that a purely democratic system is nothing short of a utopia. Numerous societies have tried it, but they had to make small concessions to authoritarian rules because human nature makes it impossible for such a system to function for very long.', 'Eugene’s overview of scrf’s roadmap on the 3.10.22 community call put DAO governance research as a priority.  Using this article as momentum could be valuable.  The voting activities in this forum appear very similar to DAO activities.  Perhaps, SCRF would carry more weight in the DAO community if it were a DAO. Eugene mentioned Harmony Protocol as an avenue for creating a DAO.  Harmony has an excellent blogpost - The Way of Harmony DAOs - about forming a DAO.  Aragon is listed as a resource in the post.  Harmony also offers a HRC-20 token [like an ERC-20 token but minted on Harmony] as a tool for a DAO.']"
                Research Summary: A Systematic Literature Review of Blockchain Cyber Security              ,https://www.smartcontractresearch.org/t/research-summary-a-systematic-literature-review-of-blockchain-cyber-security/1299,Privacy,27,['https://doi.org/10.1016/j.dcan.2019.01.005'],"['iot', 'summary', 'privacy', 'network-security', 'ethereum', 'zero-knowledge']","['TLDRResearch on blockchain-related cybersecurity is relatively new. The authors of this paper conducted a systematic literature review to evaluate current trends in the cybersecurity applications of blockchains. Literature focused on IoT security (45% of all studies), data sharing and storage (16% of studies), networks (10%), public key infrastructure (7%), and data privacy (7%). Researchers found that most studies were experimental or conceptual and provided little quantitative data with few practical applications. Current research shows that blockchain cybersecurity is rife with unanswered questions and possible security vulnerabilities.Researchers found that blockchains offer no “silver bullet for [current and common] cybersecurity issues,” and cybersecurity professionals should be reminded that blockchains do not enhance individual participants’ security nor eliminate the need to follow other cybersecurity best practices.Blockchain developers must understand how to address and mitigate emerging cybersecurity threats. Blockchain technologies enable a new form of decentralized application that can serve as the foundation for critical elements of internet security infrastructure. As blockchain development continues, academics and practitioners should collaborate in R&D efforts and release open-source software and datasets to remediate issues collectively.Core Research QuestionWhat are the latest developments in blockchain security, and what research has been done to improve blockchain cyber security?CitationTaylor, Paul J., et al. “A Systematic Literature Review of Blockchain Cyber Security.” Digital Communications and Networks, vol. 6, no. 2, 2020, pp. 147–56, https://doi.org/10.1016/j.dcan.2019.01.005 3.BackgroundCybersecurity: The study of how to protect computer operating systems, networks, and data from cyber attacks. Cybersecurity applications and specialists monitor computer systems and mitigate threats when an attack happens.Cyberattacks: A cybersecurity breach that is usually aimed at accessing, changing, or destroying sensitive information; extorting money from users; or interrupting normal business processes. Implementing effective cybersecurity measures is particularly challenging because there are more devices than people, and attackers are becoming increasingly innovative. Cyberattacks happen regularly in every industry. Attackers typically employ tactics such as cryptojacking, phishing, ransomware attacks, and extortion to steal cryptocurrency.Trust in code: A common concern in cybersecurity and software development. Open-source repositories like Github, NPM, or Maven, have been fundamental to the development of blockchain code and the open-source development happening in Layer 1 (L1) and Layer 2 (L2) blockchains, but they present vulnerabilities.Artificial Intelligence: Computer systems that are able to perform tasks in a manner similar to human intelligence. This often includes programs such as speech recognition and language translation.Internet of Things (IoT): The interconnection of computing devices. This includes physical objects and hardware that are embedded with sensors, processing ability, and software. IoT devices are able to connect and exchange data with other devices and systems over the internet.Network: A computer network is a set of computers sharing resources. Networks use common communication protocols to communicate with each other.Security: Computer security or cybersecurity refers to the protection of computer systems and information from attack, theft, or unauthorized use.Security defaults: The default configuration settings in the code that are the most secure settings possible, but often not necessarily the most user-friendly.Encryption: Encoding information to secure it in its original form. Encryption helps prevent unauthorized access to information or data.Data privacy: The secure handling of sensitive data, often confidential or personal data, to meet regulatory requirements.Public key infrastructure (PKI): The governance of digital certificates to protect sensitive data. It provides users, devices, and applications with unique digital identities to secure communications from end to end. Blockchain applications in PKI allow users to authenticate their identity with another entity or service so they don’t have to rely on a potentially vulnerable central server.Data storage: Computing processes and technologies used to store data securely and with integrity. This can include the physical protection of hardware containing data as well as the security of the software.Data sharing: The practice of making data available to multiple users. It refers to the exchange, collection or disclosure of data to a user or organization.World Wide Web: The subset of the internet that can be accessed by a web browser.Wifi: Wireless technology used to connect computers and other devices to the internet. Wifi transmits a radio signal to a wifi router which connects to the internet.Domain Name System (DNS): Translates machine readable IP addresses into human readable text. When a user types in a web address, that address is converted into an IP address that the computer reads and accesses the internet location with. Blockchains can effectively host DNS records in a distributed environment to prevent malicious changes and distributed denial of service attacks (DDoS)Malware: A type of software that creates a cybersecurity attack designed to disrupt, damage, or gain unauthorized access to a computer system.Botnet: A computer or group of computers that has been infected with malware under the control of a malicious actor.Distributed Denial of Service (DDoS): A type of cyber attack that targets and attempts to disrupt a server, service or network by flooding it with an overwhelming amount of internet traffic.Sidechain Technology: A separate blockchain that runs in parallel to a main blockchain. The sidechain works in parallel with the main chain, increasing transaction throughput.Snowballing: A research sampling method that involves a primary data source nominating other potential data sources that will be able to participate in research studies.SummaryBlockchain technology and cryptographic-based distributed ledgers enable trusted transactions between untrusted participants in a network. This allows practitioners, developers, and researchers to use the technology as the foundation for critical elements of internet security infrastructure.To understand the interplay between blockchain and cybersecurity, this study conducts a Systematic Literature Review (SLR) of current blockchain applications to solve cybersecurity issues in various fields. Currently, there are very few SLRs. Most recent SLRs did not address cybersecurity and blockchain generally. This is the primary motivation of the authors for the study.The researchers reviewed 30 primary studies from an initial retrieval of 742 primary studies. Studies included practical security solutions that displayed innovative techniques to solve security challenges in data security, mutability, and authentication of users.Blockchain research on cybersecurity was found to be objectively new and focuses primarily on the security of IoT devices (45% of all studies), data sharing and storage (16% of studies), networks (account for 10%), public key infrastructure (7%), and data privacy (7%).Their findings show that research for blockchain cybersecurity in IoT technologies is expanding and in-depth, but still lacks practical solutions. The authors believe this may be due to the increase in the use of IoT devices in homes, military, healthcare, and the increased demand for IoT security solutions after IoT devices have become a security threat, for example as botnet nodes in a DDoS attack.The researchers call for a community-driven approach for practitioners and academics to collaborate on developments in blockchain cybersecurity. They suggest both parties enable public data sets and actively engage in research results on either side. The Bitcoin and Ethereum ecosystems can benefit from this approach.MethodThis paper used an SLR approach that can be summarized in three steps.First, they query a selection of primary studies by keywords (“blockchain” OR “block-chain” OR “distributed ledger”) AND (“cyber security” OR “cybersecurity” OR “cyber-security”)Then, results were filtered through inclusion/exclusion criteria.Inclusion and Exclusion criteria were conducted to ensure its relevance to blockchain applications and academic rigor. The criteria is summarized in the table below:Finally, authors ran primary sources through a snowballing process that was conducted until no further papers meeting the inclusion criteria were detected.Researchers addressed data quality for signs of research bias and validity of the data. They performed this through a quality assessment process that all selected studies were subjected to.Results622×605 115 KBThe researchers identified nine themes: IoT (45%), Data Storage and Sharing (16%), Networks (10%), Public Key Infrastructure (7%), Data Privacy (7%), Web (3%), Wi-Fi (3%), Domain Name System (6%), and Malware (3%).Note that Data Storage and Sharing consists of papers focused on peer-to-peer sharing, encrypted data storage, and searching. Networks focused on virtual machines, networking, and virtual network management.Of the 30 papers included in the study, the technical solutions presented called for changes at a system’s infrastructure level, reorganizing network architecture, or moving to a blockchain from a centralized server. Papers often have experimental or conceptual solutions that present practical concerns for the effectiveness of a blockchain solution over conventional and current security. Studies with the most practical and “ready-to-deploy” solutions were those that were tested on the Ethereum or Bitcoin platforms. Regardless of the ideas presented, the authors found that blockchain technology offers no “silver bullet for cybersecurity issues.” This is important to note, as there is a significant buzz in the blockchain ecosystem about its ability to offer security against common cyber security threats.The results suggested several trends in blockchain cybersecurity emerging in the three most prominent themes.IoT Research looks at the authentication of devices to an IoT network and of users to devices and the secure deployment of firmware through peer-to-peer updates.Data storage and Sharing research looks at ensuring cloud data remains resistant to unauthorized changes, searching and secure storage of data in hash lists, and verifying the data exchange from end-to-end within a transaction.Network Security research looks at illustrations of how blockchain technologies allow for critical authentication data to be stored in a decentralized manner.The study found primary sources concluded that IoT devices, which are typically designed hardware-lite and require little power, could benefit from new protocol solutions such as: Proof-of-Possession (IoTChain) – which defines that a user has a cryptographic key; Proof-of-Credibility – which achieves consensus by assigning a credibility score to nodes; or a hybrid of Proof-of-Work (PoW) and Proof-of-Credibility protocols.Discussion and Key TakeawaysBlockchain for IoT security: Further research should discuss the systematic adoption of blockchain in IoT systems. Research needs quantifiable guidelines and tools.Blockchain for AI data security: Researchers should explore blockchain technology for the protection of artificial intelligence (AI) data in Business-to-Business (B2B) and Machine-to-Machine (M2M) environments. Ensuring the security of AI data increases the credibility and reliability of the data. Therefore increasing the trustworthiness of the outputs produced.Sidechain Security: the authors found two fundamental questions that should be answered about sidechains: How do sidechains establish security defaults to prevent attacks? And, how can blockchain customers be assured of the integrity and confidentiality of their data through sidechains?Releasing open-source software and datasets and engaging with the community: Gaps in blockchain cybersecurity knowledge and research remain between academia and the developer community. The paper calls on academic researchers to release more open-source data sets, applications, and tools to be engaged by industry.Implications and Follow-upsThe paper suggests that the use of blockchain technologies for cybersecurity is understudied. Even with the extensive work in the decentralized financial (DeFi) sector, there is not enough research about the potential benefits to cybersecurity. The recent Wormhole  1attack should give DeFi users a renewed sense of the importance of cybersecurity in blockchain technology.Researchers lack empirical studies and available data sets to conduct blockchain cybersecurity tests. Studies in this paper were largely conceptual and lacked practical application. This is a testament to the speed of blockchain development. The technology needs critical study and testing to find applicable use cases, solving real-world problems, and addressing cybersecurity issues.Bitcoin and Ethereum are cited as potential test grounds for blockchain cybersecurity issues. The authors suggest that these ecosystems should consider targeted efforts to engage with cybersecurity professionals and academics, given the pace of development in their ecosystems. Furthermore, the authors note that blockchains have an advantage in securing against DDoS attacks. Industries, where DDoS attacks are frequent, may benefit from the decentralized nature of blockchain to enhance their cybersecurity.The authors present four research areas for follow-up work: 1.) Assessing network latency, power consumption, and data packet flows of blockchain-based IoT networks; 2.) Review of various ways in which Ethereum and/or permissionless/permissioned blockchain platforms have been or can be used to develop innovative cyber security solutions; 3.) Architectural design of a forensics-friendly cryptocurrency to facilitate lawful investigation of suspicious cryptocurrency transactions such as those used in cyber criminal activities; 4.) Design of blockchain-based solutions for time and delay-sensitive applications.ApplicabilityAs developers create a decentralized web infrastructure (web3), blockchains need to address cybersecurity concerns unique to public decentralized architecture. Permissioned blockchains will be easier to secure against attacks than globally distributed networks like Bitcoin and Ethereum. Distributed nodes, with different personal security measures and understandings of security, will create unique challenges for collective public governance and open-source developers. Both groups should be aware of basic cybersecurity literacy.Although the authors did not touch on new developments in hardware and software that may affect security, new inventions present an issue for security professionals as hackers may have new opportunities to expose security inefficiencies. For example, faster download speeds may encourage cyber crimes. Blockchains will not be siloed from larger issues in the cybersecurity field.', 'As a starting point for discussion:The Cloud Security Alliance (CSA) recently published their Blockchain Cyber Security Report. It’s farily comprehensive from industry but is focused on blockchain and distributed ledger technologies for cloud security. I think this is an important connection to make with what established industry is focused on vs what is happening on new innovation fronts and in web3.Published February 2022: BlockchainDLTRiskandSecurityConsiderations022822.pdf (3.3 MB)The authors of the research paper call for more collaboration between academics and practicioners. I’m interested to see how current cyber secuity professionals can bring institutional knowledge to blockchains in line with a vision towards decentralized internet infastructure (which I believe truly remains the goals of the developments happening here).“Some blockchain technologies use X.509 certificates to create encapsulated digital identities that could control permissions over resources and access to data in the cloud. In addition, blockchain’s immutability property ensures that data blocks have not been altered thus reducing fraud, data manipulation and data destruction risks…[but] The rising frequency of Distributed Ledger Technology (DLT) platform hacks, exploits and scams imperils confidence in blockchain technology’s ability to serve as the foundation for cloud security.”Questions to consider:What have dev teams not addressed that seasoned cyber professionals and academics would ask them to consider?Projects like Helium & FOAM might be enhanced from robust cyber security. Elaborate on why they would.What are some unique security features and challenges to blockchain technologies?I would love to hear researcher’s thoughts on these fronts. Especailly in context with what cyber security professionals might be thinking about blockchain & DLT.', 'Thank you very much for this fantastic summary!  This specific issue is very relevant to me for a few reasons, but the biggest reason being I am currently attempting to execute a qualitative study on the perspectives of decision-makers concerning their sources of information relative to cybersecurity strategy and implementation.Did the authors make any commentary or notation about case studies that reinforced these observations?Were there any suggestions concerning Proof of Possession or Credibility relative to integrated frameworks that might utilize more than one type of protocol to supplement gaps in chain-of custody?Did the authors indicate “why” the studies that were deployed on Ethereum or Bitcoin were “the most practical”?  This observation seems to be politically charged, but there may be data that supports this framing and I am wondering if you saw anywhere that would support this statement beyond being a value judgment from the authors?', 'Quoting the original study:More than simply becoming popular, [blockchain] has made a lasting impact on the world. … The value of a trustless, decentralized ledger that carries historic immutability has been recognized by … industries looking to apply the core concepts to existing business processes. … Most notably, there is an emerging trend beyond cryptocurrency payments: the blockchain could enable a new breed of decentralized applications without intermediaries and serve as the foundation for key elements of Internet security infrastructures.Note the conditional statement, “the blockchain could enable a new breed of decentralized applications…”We’re talking about the possibility of using blockchain as “the foundation for key elements of Internet security infrastructures,” and yet as the study itself acknowledges, blockchain technology has already “been commercially adopted [and] has influenced world currency markets.”Now, it seems to me that “world currency markets” deserve to be treated with at least the same level of maximal scientific precision as anything else truly important. Would we put humans on a rocket to Mars based on technology as provisional and unproven as that which (as @valeriespina points out) just suffered the second biggest DeFi attack in history?Before becoming fixated on the (no doubt worthwhile) minutiae of this study, I think it’s important to step back and ask a higher level foundational question.Whose aims are being served by the precipitous rush to adopt a technology for purposes that it has clearly not been proven for? Is the global financial market a bunch of tipsy cows drunk on fermented berries? Is it mass hysteria over a Dutch tulip bubble? Or have specific firms—like Wormhole, which just chose to sacrifice $326M to make one of its stakeholders whole—calculated that the risks are tolerable to get “first-mover advantage”? And in either case, is this any way to run a business?', 'Ralph, all of these observations and questions are what ultimately led me to start working at SCRF.  On the one hand, you’re absolutely right about every single observation.  On the other hand, the market will continue to move whether it is ethical to move or not.  These questions need to be asked, and these philosophical frameworks need to be examined; unfortunately, that will not prevent the market from continuing to experiment with real-world capital and real-world people.  It is the unfortunate reality of this market that makes SCRF even more valuable as a place to have these types of examinations and hopefully create dialogues to further the conversation.I would love to hear people’s responses to Ralph’s set of questions, as those specific set of questions are some of the most pressing and simultaneously most unaddressed issues that need to be unpacked.', '@Larry_Bates Thanks Larry, I like the idea that airing these ideas makes SCRF more valuable. Gives us something to live for. :) Larry_Bates:On the other hand, the market will continue to move whether it is ethical to move or not. These questions need to be asked, and these philosophical frameworks need to be examined; unfortunately, that will not prevent the market from continuing to experiment with real-world capital and real-world people.Of course you’re right that this is what happens, and I’m sure this “unfortunate reality” is not for mere mortals to question, but I think I’ll question it anyway. Why exactly will the market continue to be allowed to experiment with real people’s money and real people’s lives, without any regard for ethics? It seems so… unethical.One obvious answer is “this is our way of life.” That was my father’s expression for the dog-eat-dog world we live in. He said it many times, and yet if I had ever said in reply, “You realize you’re talking about capitalism, right?” he would have been shocked.Another obvious answer is that anything that can be technically done will be done, regardless of the economic arrangement you live under (which makes this explanation “not ideological”), and regardless of how good an idea it is. This might be called the “humans are builders and builders will build” theory.What’s yours?', 'I think you’ve come to a specific point where it ceases to be abstract and becomes a much more tangible conversation:Where/why/how are the legal boundaries within the market; as there ARE in fact limits and limitations on what a business can/can’t do.Where is the perimeter between unregulated territory, a regulatory sandbox, and regulation?In that construct, we can start to delineate where the regulation ends and where regulatory observation with the freedom to experiment begins in contrast to the completely unregulated operations that seem to be doing legitimate business in the public without having a clear jurisdiction for their operations.Even down to this conversation defaulting to the assessment from the American perspective, but due to our both being American.  Surely all cultures do not have the same predisposition towards exploiting all market avenues with no regard to ethics, so that framing is defaulting to the American example.  In that regard, it would be debatable whether American cybersecurity culture is a good model for the world and in that regard it makes the examination of cybersecurity literature that is only published in English inherently based in this cultural paradigm.Is the English language itself limiting our capacity to discuss these concepts outside of the framework of accepting western capitalism as “default” and everything else as “other”?I would assert that this conversation has many relevant aspects that result in an inherently politically charged landscape that also necessitates deconstruction of the cause of the political tension before the actual issue can be addressed.The conversation began with examining cybersecurity studies only to get to the question of whether the studies are examining practices that are even ethical to be happening, with the studies not necessarily raising that question.  Is it possible to deconstruct the speed of unethical business/academic practices?  A system that depends upon those things would argue it’s impossible; on the other hand, I would assert that ethics are a relatively new invention in western recorded history if we look at ethics and empathy as tools instead of naturally occurring states.With all that said, even SCRF had to initially stay away from the ethics discussion because it is so heated and heavily theoretical that it was agreed upon in the first six months or so that the forum was not yet established enough for a discussion like that to potentially take over every topic.  Ethics are a hot-button issue for a reason.  It’s very difficult to come to an agreement upon a shared set of ethics that go beyond a few values.So to bring it full circle: Is it “ethical”?Whose ethics?', 'Of course I agree about the relativism of ethics across cultures, or even within a single culture (“situational ethics”). I’m not disowning its importance, but I do want to point out that ethics per se was your own contribution to the conversation. My original complaint was with the strangely unscientific application of technology to weighty human concerns like global currency markets. And I still think that’s significant.To use my example of space exploration: Send up a rocket that explodes and vaporizes some astronauts and your space program goes on hold until you can demonstrate that your O-rings are fixed. But regularly lose hundreds of millions of dollars to DeFi hacks and nobody says a word about your technology being garbage.Why are we so relaxed about playing fast and loose with other people’s money, unless the point is to steal it?', 'My original response was trying to assert that this particular market will move “whether it is ethical or not” in the context of it being framed as “capitalism”.  This is where defaulting to american/western capitalism is what I was actually trying to get at, but had to use the framing of “whose ethics” to contextualize why the conversation is defaulting to a particular culture of market movement that disregards ethics, and by proxy disregards scientific practices that are rooted in preserving ethical experimentation.I think the question “Why are we so relaxed about playing fast and loose with other people’s money, unless the point is to steal it?” is inherently defaulting to the american/western capitalist perspective when that is not the only motivating force in the space.  The issue becomes whether the non-western capitalist frameworks can exist in the same space without becoming exploited in the western capitalist paradigm to then say “who is ‘we’ in the statement about playing fast and loose with other people’s money”?I wasn’t actually trying to bring “ethics” into the conversation for the philosophical debate of “ethics” so much as articulate that this conversation inherently defaults to a framework that will exist with a market that moves whether the moves are ethical or not.I think in your examples, it’s hard to articulate whether you’re talking about American political theater or problems that are inherent to operating in an innovative space.  I would think it is the former, rather than the latter, which is why I started to try to contextualize “whose” framing we are using throughout this conversation.  I believe specifically “American” and “Western” businesses are comfortable playing fast and loose with other people’s money.  I’m not sure that particular paradigm is true globally. That is what I was trying to express with the question “whose ethics?” not literally “whose ethics”.  It is a hard discussion to have in text.In other words: Does the fact that the study excluded non-English papers inherently make this conversation default to a western-capitalist framework which guides the business, scientific, and ethical practices thus shaping the trajectory of the conversation?“One obvious answer is ‘this is our way of life.’ That was my father’s expression for the dog-eat-dog world we live in. He said it many times, and yet if I had ever said in reply, ‘You realize you’re talking about capitalism, right?’ he would have been shocked.”“Did the conversation ever have the capacity to escape this particular paradigm because the original literature review excluded non-english papers,” was the effective goal of the original question.  I am NOT trying to derail the thread, as this is a serious question that I am concerned with as an academic and practitioner.  This question is constantly something that concerns me.  In being able to speak French, I have worked in France and in in post-colonial French speaking countries.  They have a different approach and are more about mutually beneficial outcomes. That is not to say it’s because of absolutely noble reasoning, but the business culture that you see in American DeFi does not actually represent even the majority of the development; it’s just the most visible in America.I worked with KfW bank in Germany to help them develop one of their real estate products, and even at that level of commercial development the rhetoric and willingness to take risk with other people’s money was just not present in the same way that you find in specifically American DeFi culture.', 'Very interesting Research. Thanks for summarizing this @valeriespina.It seems that the researchers only considered primary studies published up to early 2018, and have assumed that paper in 2018 would surpass that of 2017. They were also bullish about the application of blockchain and subsequently the number of new papers that fits under their criteria.As we have arrived at 2022, is there an easy way to figure out to what degree the boom had played out? What does Fig.2 after we take into account the most recent data?The same goes for the main conclusions of the paper: Do they still hold after nearly 4 years since? What is different and what do you find most unexpected or interesting? ', ' Larry_Bates:I think the question “Why are we so relaxed about playing fast and loose with other people’s money, unless the point is to steal it?” is inherently defaulting to the american/western capitalist perspective when that is not the only motivating force in the space. The issue becomes whether the non-western capitalist frameworks can exist in the same space without becoming exploited in the western capitalist paradigm…Fair points. Larry_Bates:I think in your examples, it’s hard to articulate whether you’re talking about American political theater or problems that are inherent to operating in an innovative space.Even if I was trying to think about “an innovative space” I was undoubtedly thinking also about “American political theater.” This isn’t simply explained by my being provincial.  It’s that America is the 800 pound non-benign hegemon. Larry_Bates:I’m not sure that particular paradigm is true globally.Again, I accept this point on faith, and hope you’re right. China is the country I would look at first, and indeed they do seem to be putting the brakes on “free enterprise,” much to America’s disappointment. Larry_Bates:Does the fact that the study excluded non-English papers inherently make this conversation default to a western-capitalist framework…I have no trouble accepting this premise. Larry_Bates:the business culture that you see in American DeFi does not actually represent even the majority of the development; it’s just the most visible in America. … the rhetoric and willingness to take risk with other people’s money was just not present in the same way that you find in specifically American DeFi culture.Glad to hear this.']"
                Research Summary and AMA with Kelsie Nabben (RMIT University): Can Algorithmic Governance turn a DAO into a Panopticon?              ,https://www.smartcontractresearch.org/t/research-summary-and-ama-with-kelsie-nabben-rmit-university-can-algorithmic-governance-turn-a-dao-into-a-panopticon/960,Governance and Coordination,57,[],"['dao', 'summary', 'discussion']","['Kelsie Nabben is a Researcher at RMIT University’s Blockchain Innovation Hub, a scholarship recipient from the Centre for Automated Decision Making & Society and a member of the Digital Ethnography Research Centre.She recently sat down with Chainlink Labs to discuss her paper on algorithmic governance in DAOs entitled “Is a DAO a Panopticon? Algorithmic governance as creating and mitigating vulnerabilities in ‘Decentralised Autonomous Organisations’”. 7She has offered to answer questions about her paper here for a limited time so please don’t hesitate to ask any questions you may have!DescriptionThis interview is a recent episode from the Chainlink Research Report series which features short presentations of working papers by blockchain scholars in computer science, economics, and related fields.In this episode, RMIT University researcher Kelsie Nabben 1 discusses her latest paper exploring some of the challenges and risks associated with algorithmic governance in DAOs.Video:Kelsie Nabben on Algorithmic Governance in DAOs | Chainlink Research ReportsTake-aways:Algorithmic governance, which frequently uses machine learning, solves many security issues inherent in DAOs.At the same time, algorithmic governance creates monitoring systems that change DAOs in fundamental ways, increasing the risk that they will function as panopticons.To strike a balance between security and privacy with algorithmic governance in DAOs, it is essential that the DAO community participate in shaping DAO governance.Kelsie Nabben background:Kelsie Nabben is a Researcher at RMIT University in Melbourne, Australia. She conducts ethnographic research on decentralized technology communities, digital infrastructure, blockchain community culture, and algorithmic governance. Kelsie is a recipient of a PhD scholarship at the RMIT University Centre of Excellence for Automated Decision-Making & Society, and a researcher in the Digital Ethnography Research Centre and Blockchain Innovation Hub.Some of Kelsie’s work:PapersNabben, K., 2020. Trustless Approaches to Digital Infrastructure in the Crisis of COVID-19: Australia’s Newest COVID App, Home-Grown Surveillance Technologies and What to Do About It 1.Nabben, K., 2021. Blockchain Security as “People Security”: Applying Sociotechnical Security to Blockchain Technology 1. Frontiers in Computer Science, 2, p.62.Nabben, K., 2021. Resilient Future-Making: How Cryptocurrency & Transhumanism Overlap for Immutable, Decentralised, Autonomous Futures 1.Nabben, K., 2021. Is a DAO a Panopticon? Algorithmic governance as creating and mitigating vulnerabilities in. Algorithmic governance as creating and mitigating vulnerabilities in ‘Decentralised Autonomous Organisations’ 7.Nabben, K., 2021. A Provocation on Privacy & Ethics in Blockchain-Based Systems: An Invitation. 1.Substack - “on the cataclysmia of digital infrastructure 1”Is a DAO a Panopticon? 1. August 31, 2021.DAO Vulnerabilities: A multi-scale DAO ecosystem mapping tool towards computer-aided governance. August, 13, 2021.Experiments in algorithmic governance continue 1. July 29, 2021.Towards a model of resilience in decentralised socio-technical infrastructure. July 7, 2021Relevant links and contact info for Kelsie:Email: kelsie.nabben@rmit.edu.auSubstack: https://kelsienabben.substack.com/ 1Twitter: https://twitter.com/kelsiemvnMedium: https://medium.com/@kelsie.nabben 1Academia.edu: https://kelsienabben.academia.edu/ 1', 'I sincerely love this framing and can’t wait to listen to this interview!  Applying the panopticon framework seems appropriate to highlight the potential pitfalls of this type of transparency, and I think the framing is succinct and clear.  Additionally, this seems to be the least confrontational means of raising potential problems with DAOS.', '@kelsienabben - This was a fantastic presentation, and @jasonanastas thank you so much for the interesting questions and excellent hosting. K.,I really liked your analysis of how token ownership can garble the participatory relationship between labor and ownership. It certainly reflects what I’ve encountered in these projects. Do you think DAOs then falling into the same structure as any other capitalist(?) organization or do DAOs as human-machine chimeras present a new paradigm? Has anyone made a connection with Haraway’s Cyborg Manifesto?I’m also curious whether you’ve encountered a difference in a community being observed by humans versus those observed by algorithms (i.e. are electronic panopticons scarier than straw bosses?). When you were looking at some of the monitoring and incentivization systems like SourceCred and Coordinape, did you notice significant shifts in people’s behavior once these systems were implemented? Were there algorithmic systems of demerits (perhaps similar to ones in classroom tracking apps)?  To your points about maintaining digital infrastructure - did you see any difference in the way ‘management’ was perceived in these organizations versus someone doing the digital equivalent of digging ditches?Thank you so much for taking the time to answer some questions!', 'Hi James,Thank you so much for engaging with this piece. I’ve spent a couple of days sitting with the questions you’ve asked, mostly because I would most like to describe and link to other works! (I will share links in this thread as possible, noting that this is a working paper, and there is ongoing research in the Metagov community with projects like Sourcecred).The piece mostly points out the panopticon dynamic of DAOs in that “trustless”, scalable coordination depends on governance by algorithms, which places people back under a political system, when they sought autonomy (as in, self-governance). I have tried to constructively point out these dynamics in DAOs, to as to help designers reflect on the algorithmic rules they decide on. One of the biggest surprises of this research has been the lack of awareness to, or conversation on, this surveillance by algorithms. The general consensus seems to be acceptance that they are objective, not subjective, and therefore can’t be questioned or changed.In terms of what’s scarier, that depends very much on your imagination of Artificial General Intelligence and what it means for this to be decentralised. I tackle these issues in the two papers that follow (under review at present).', 'Kelsie, thank you so much for that fantastic presentation!To give some context, I came up with “Decentralized Conglomerate Theory” as a counterpoint to a DAO with the understanding that DAOs really don’t have a good mechanism for keeping institutional memory.With that said, I have seen many DAO frameworks and theoretical constructions that all seem to assume a base level of understanding for the user.  My question is: considering DAOs were conceptualized by people that had never led a small group (let alone a global organization) to understand group dynamics; were DAOs effectively just reproducing the same governance system we have and replacing a central authority with an algorithm?As far as I can tell, DAOs do not actually seem to have innovated in “governance” and have just adopted the pre-existing governance model.  Further, the notion that an algorithm removes a centralizing vector assumes the algorithm is “unbiased”.  While the notion that “anyone can contribute to an open-source code base” is philosophically true, in practice the contributors to blockchain code bases are a very small part of the community.Also, as a cybersecurity specialist I know that we are taught that “physical security” is equally if not more important than digital security.  The notion that AI can solve problems is a concept that is NOT coming from “cybersecurity specialists” and is one coming from “cypher-punks” that aren’t actually cybersecurity specialists.  Are you seeing social engineering being referenced more in your DAO research, or is that still being ignored by non-cybersecurity specialists?  I would argue that the social engineering attack vector is likely the most dangerous and the most likely vector for attack in a DAO setting, i.e. Discord phishing, email phishing, fake websites attempting to gain access to web3 wallet, etc…In other words: If cybersecurity specialists are saying that social engineering is a problem, why are discussions of DAOs and their security seemingly devoid of the discussion of the physical access point or social engineering attacks?Access control is an integral part of cybersecurity.  It is hardly discussed in the crypto space.  My final question is, how are we supposed to seriously discuss cybersecurity if the people leading the conversation (Larimer, Buterin, etc…) have no understanding of social engineering attacks or how to protect physical access points?In other words, without proper access control education, could a DAO ever be feasible since humans as the weakest link will likely be the attack vector?Further:  Is a DAO more likely to be a panopticon when users have not been educated enough to recognize a panopticon potentially due to the presence of propagandistic social-engineering keeping them uninformed (or more appropriately “obedient good citizens”)?  Is it that the DAO is the panopticon or that this particular society cannot form governance mechanisms without surveillance being the default due to the habituated nature of the nationally constructed surveillance states?', '@kelsienabben, thank you for your fascinating paper and the video with @jasonanastas of ChainLink.@jmcgirk has raised this question: Is it an engineering problem or a social problem? The prominence of “algorithms” makes me think that we’re dealing with it as an engineering problem—at least here and for now. (Jeremy Bentham probably felt it was a social problem himself, but he applied plenty of engineering to “solving” it.)“Reputation” derives from personal history, yet if you believe in “education” you accept that people will change their minds—sometimes about significant things. At the same time, human beings (and probably many animals) revere “precedent” and past-performance, and often view personal growth and development as irrelevant or illusory.For this reason we turn to something “outside ourselves,” namely design and engineering, ideally with serious math behind it. The most revered current example is probably the domain of zero-knowledge proofs, which allow someone to “prove” something—say, their identity—without revealing what that thing actually is.What do you and Jason think about deploying something like “moon-math based” tools to maintain security with anonymity in DAOs? Is it just more technologist self-delusion, or might it point to a workable solution?', 'Hi @Larry_Bates.I’m really grateful for your perspective here. To respond to a few key points made:I agree - DAOs are discovering governance. The economists at RMIT Blockchain Innovation Hub that research team I’m in does great work on the institutional economics of blockchains and DAOs, and has for a number of years. I find that drawing on existing institutional dynamics to explain and compare certain aspects of DAOs is useful.“Are you seeing social engineering being referenced more in your DAO research, or is that still being ignored by non-cybersecurity specialists?”. I’d be very interested to hear more on your perspective on this, including AI security. I argue that the purpose of securing digital infrastructure is people before protocol in this paper on “Blockchain Security as People Security 1”.You are correct. The society makes the DAO. A DAO, or any institution, is not inherently a panopticon. In fact, as an observation, some DAOs have very relationally based accountability structures in working groups (e.g. Commons Stack). They draw on Ostrom’s “Principles of the Commons” for multipolarity and scalability here.', 'Hi Ralph,Thank you for your considered comments and questions.First of all, I’m not trying to diagnose a problem in this piece, I’m trying to point to what generations of technologists that understand information technology tools in the context of social political systems have said; ‘be careful what you build and how you build it’.In saying this, all engineering starts with subjective norms, beliefs, and values. In the discussion section of the piece, I point to the idea of “algorithms as policy”, in that writing algorithms in code is like writing laws, they are not “outside ourselves” or our societies. This perspective is a reflection on the blockchain mentality that society can be “engineered” because algorithms, and maths, is objective.In saying that, the task at hand is to engineer social systems using technology, but being very clear about why 1, and what principles guide this perspective. For example, ZKPs are great if you want privacy-preserving identification systems, but would have very different social implications if they are used to determine certain parameters (e.g. age verification for access), or identity completely (e.g. Stewart Brand and Howard Reingold vehemently warn against absolute anonymity, versus pseudonymity, this based on their experience of early online communities)', '@kelsienabben, thank you for the illuminating response.', 'Hi @kelsienabben ,thank you for such a fascinating presentation and a paper full of exceptional points! I really like your points that arouse the awareness of the influence of cultures in DAOs, which give further DAOs experiments progress and research a crucial sign and guide.Your observation of some DAOs can question and re-evaluate their algorithmic policies correspond to the point <An Exploration of Governing via IT in Decentralized Autonomous Organizations> (2021) proposed, that DAOs have tried to build more pluralistic and decentralized forms of algorithmic management through the mechanism of “taming algorithmic power.” When you are looking at some DAO cases that allow members to shape their rules, did you observe the boundary of this rule re-produce? Are there boundaries to the re-oriented of rules in these DAO communities’ cultures explicitly or impliedly?Thanks for @rlombreglia’s inspiration. I also think the tension between the code in DAOs and human power has some similarities to a hybrid of civil law influence and common law influence. The civil law system usually implements well-framed laws and creeds with statutory law, while the common law system usually reveres precedents, which are shaped continually by the discretion of judges. Actually, I regard the re-oriented of rules in DAOs more like the operation of the amendment process in civil law, since there have a set of rules already been set by the creators, and then the members discuss and affect its shape like parliaments, which is influenced significantly by the culture and interest behind it as well. Did you notice some similarities between the rules re-oriented process in DAOs and the traditional law amendment process?Thank you so much for taking the time to answer questions!ReferenceMini, Tobias; Ellinger, Eleunthia Wong; Gregory, Robert W.; and Widjaja, Thomas, “An Exploration of Governing via IT in Decentralized Autonomous Organizations” (2021). ICIS 2021 Proceedings. 1.aisel.aisnet.orgAIS Electronic Library (AISeL) - ICIS 2021 Proceedings: An Exploration of...A decentralized autonomous organization (DAO) is a distinct form of platform meta-organization that heavily relies on smart contracts running on blockchains to govern a distributed network of autonomous actors, thereby continuing the shift toward...', 'Thanks Astrid.Culture is a difficult one. It’s very intangible, yet, social factors are highly influential in communities.I tried to read the link you shared but its pay walled. Thus, I’m unsure what is meant by “taming algorithmic power” in this paper, however, your questions that follow are very relevant. The next paper I’ve drafted is on if and how DAOs can collectively shape algorithmic rules.Regarding DAOs and law, there’s some great scholarship in this area (see Di Fillipi and Hassan, for example). From an ethnographic perspective, a repeated theme in DAO communities is an unwillingness to question algorithmic rules, harking back to the idea that “code is law”, rather than a subjective set of choices. I write on this with Dr Michael Zargham here: Algorithms as Policy: - by Kelsie Nabben 3.', 'Hello, how can I get access to these materials?I am interested in the topic written about in the text', 'Hey @GloriaOkoba, here’s a link to the paper discussed in the post: Is a ""Decentralized Autonomous Organization"" a Panopticon?: Algorithmic governance as creating and mitigating vulnerabilities in DAOs by Kelsie Nabben :: SSRN 1.You also might want to subscribe to @kelsienabben Substack here: https://kelsienabben.substack.com/. At the top of the post there are also some other resources related to some of the work that Kelsie is doing.Hope that helps.', ' rlombreglia:What do you and Jason think about deploying something like “moon-math based” tools to maintain security with anonymity in DAOs? Is it just more technologist self-delusion, or might it point to a workable solution?This is a great question Ralph. I think this is possible using methods that are currently used in machine learning and social science for data analysis under privacy constraints. I’m not exactly sure how these methods would be employed but this is where I would start.Here are a few resources discussing these methods:Mendes, Ricardo, and João P. Vilela. “Privacy-preserving data mining: methods, metrics, and applications 1.” IEEE Access 5 (2017): 10562-10582.https://www.turing.ac.uk/research/interest-groups/privacy-preserving-data-analysis 1Lun Wang- ""Aegis: Privacy-Preserving Data Analysis Framework"" - YouTube 1', 'Hi. I’m not completely sure what you mean by “the materials,” but Kelsie Nabben’s “Panopticon” paper is linked to at the beginning of this summary.Some other scholarly papers by her are here: Author Page for Kelsie Nabben :: SSRNThat should give you a start. Does this answer your question?', 'Thanks , this will be extremely helpful', 'Oh yeah, I just didn’t catch a glimpse of her very own Article. I read a couple of comments and it became eye catching to me. Thanks for your guidance .', 'Thanks for this I learnt so much but I have a question.decentralized autonomous organisations are essentially a corporate governance structure built around crypto , this is the idea I was exposed to during my research .But to get access to this organisation one must have enough tokens to buy their way in, my concern is this the ideology of cryptocurrency hasn’t been embraced in a lot of countries yet, I’m from Nigeria an African country and in my continent we only have three countries that have central banks for digital currenciesI don’t know what DAO plans to attain in the future with Africa but wouldn’t this discourage a lot of Africans if they find out they are exposed to Sybil attacks?wouldn’t that bring about a reduction of value for some cryptocurrencies because most  Africans might most likely sell their coins out of panic?Please  this question calls for concern, I strongly believe it does.', '@jasonanastas Thanks for your kind response and pointers to the papers. Very interesting.', '@kelsienabben , thanks for your response and sharing the enlightening article. I’m very much looking forward to reading your next paper. I’m also curious what you’re referring to the “unsafety” when you mentioned “Related topics which we intend to explore include: how do we ‘safely’ test algorithmic policies” in that article?']"
                Smart Contract Summit 2021: Central Bank Digital Currency (CBDCs) and Blockchain Panel              ,https://www.smartcontractresearch.org/t/smart-contract-summit-2021-central-bank-digital-currency-cbdcs-and-blockchain-panel/623,Mechanism Design and Game Theory,86,"['https://scholar.google.com/citations?view_op=view_citation&hl=en&user=nRvPNfUAAAAJ&sortby=pubdate&citation_for_view=nRvPNfUAAAAJ:qxL8FJ1GzNcC', 'https://scholar.google.com/citations?view_op=view_citation&hl=en&user=nRvPNfUAAAAJ&sortby=pubdate&citation_for_view=nRvPNfUAAAAJ:YOwf2qJgpHMC', 'https://www.andrew.cmu.edu/user/azj/', 'https://scholar.google.com/citations?view_op=view_citation&hl=en&user=jNacU6wAAAAJ&sortby=pubdate&citation_for_view=jNacU6wAAAAJ:9LrdxYebArsC', 'https://scholar.google.com/citations?view_op=view_citation&hl=en&user=jNacU6wAAAAJ&sortby=pubdate&citation_for_view=jNacU6wAAAAJ:ult01sCh7k0C', 'http://twitter.com/philippsandner']","['discussion', 'panel', 'summary', 'defi', 'notable-works', 'governance']","[""SCRF was invited to host an independent research track as part of the 2021 Smart Contract Summit 7. We chose to present five panel discussions that touch on some of the most timely issues facing the blockchain space: “Identity and Reputation”, “Governance Theory”, “Governance Implementation”, “Privacy and SNARKS”, and “CBDCs and Blockchain”.In this series of threads, we will be providing some deeper insight into the panel topics, the participants, and where interested viewers can find their most relevant works.What is SCRFThe Smart Contract Research Forum (SCRF) is where academics, researchers, and industry leaders from all over the world come together to discuss research, solicit thoughtful peer review, and find new projects on which to collaborate. You can find additional information about our programs, grants, and initiatives in our repo 2; or feel free to join us in our chat.About the CBDCs and Blockchain PanelCentral Bank Digital Currencies (CBDCs) are currently being explored by a significant portion of Central Banks globally. This raises many questions from the specific problems that CBDCs are solving to what these solutions might actually look like. In this panel, we hear from academics from the EU and US on the current state of CBDCs, relevant concerns, and why this activity is taking place now.Full videoSmart Contract Summit 2021: Central Bank Digital Currency (CBDCs) and Blockchain PanelPanelist infoAndrew MillerAndrew Miller is an Assistant Professor at the University of Illinois, Urbana-Champaign, in Electrical and Computer Engineering and affiliate in Computer Science, where he leads the Decentralized Systems Lab. He’s also an Associate Director of the Initiative for Cryptocurrencies and Contracts (IC3), and a technical advisor for Chainlink Labs.His research interests involve computer security, specifically the design of secure decentralized systems and cryptocurrencies. He combines techniques from programming languages, cryptography and distributed computing.Some of Andrew’s work:Design choices for central bank digital currency: Policy and technical considerations 5. Sarah Allen, Srdjan Capkun, Ittay Eyal, Giulia Fanti, Bryan Ford, James Grimmelmann, Ari Juels, Kari Kostiainen, Sarah Meiklejohn, Andrew Miller, Eswar Prasad, Karl Wüst, and Fan Zhang. Brookings 2020.An Empirical Analysis of Privacy in the Lightning Network 2. George Kappos, Haaroon Yousaf, Ania Piotrowska, Sanket Kanjalkar, Sergi Delgado-Segura, Andrew Miller, Sarah Meiklejohn. Financial Crypto 2021.CanDID: Can-Do Decentralized Identity with Legacy Compatibility, Sybil-Resistance, and Accountability 3 Deepak Maram, Harjasleen Malvai, Fan Zhang, Nerla Jean-Louis, Alexander Frolov, Tyler Kell, Tyrone Lobban, Christine Moy, Ari Juels, and Andrew Miller. (to appear IEEE S&P 2021)Relevant links for Andrew:Twitter: https://twitter.com/socrates1024 3LinkedIn: https://www.linkedin.com/in/andrew-miller-43272917/ 3Website: https://soc1024.ece.illinois.edu/ 2Ariel Zeitlin-JonesAriel Zetlin-Jones is an Associate Professor of Economics at the Tepper School of Business at Carnegie Mellon University in Pittsburgh, Pennsylvania. His research focuses on the interaction of finance and the macroeconomy, including an examination of the causes of financial crises and the quantitative effects of disturbances in financial markets on broader economic activity. Since 2016, Ariel has been researching the economics of blockchains: how economic incentives may be used to shape blockchain consensus as stable coin protocols as well as the novel and economically large centralized markets that currently support cryptocurrency trading. His research has been published in the American Economic Review, the Journal of Political Economy, the Journal of Monetary Economics.Ariel holds both a Doctorate and Masters in Economics from the University of Minnesota in Minneapolis, Minnesota and a Bachelor’s degree in Political Economy and Mathematics from Williams College in Williamstown, Massachusetts.Some of Ariel’s work:Currency stability using blockchain technology 1 Bryan Routledge, Ariel Zetlin-Jones. Journal of Economic Dynamics and Control, 104155. 2021.Towards Understanding Cryptocurrency Derivatives: A Case Study of BitMEX 1 Kyle Soska, Jin-Dong Dong, Alex Khodaverdian, Ariel Zetlin-Jones, Bryan Routledge, Nicolas Christin, Proceedings of the Web Conference 2021, 45-57.Getting blockchain incentives right 1 Zahra Ebrahimi, Bryan Routledge, Ariel Zetlin-Jones. Tech. rep., Carnegie Mellon University Working Paper. 2019.Relevant links for Ariel:Website: Ariel Zetlin-Jones's Homepage 2Philipp SandnerProf. Dr. Philipp Sandner has founded the Frankfurt School Blockchain Center (FSBC). From 2018 to 2020, he was ranked as one of the “top 30” economists by the Frankfurter Allgemeine Zeitung (FAZ), a major newspaper in Germany. Further, he belonged to the “Top 40 under 40” — a ranking by the German business magazine Capital. Since 2017, he has been a member of the FinTech Council of the Federal Ministry of Finance in Germany. He is also on the Board of Directors of Avaloq Ventures and of the Blockchain Founders Group, a Liechtenstein-based venture capital company focusing on blockchain startups.The expertise of Prof. Sandner includes blockchain technology in general, crypto assets such as Bitcoin and Ethereum, the digital programmable Euro, tokenization of assets and rights and digital identity.Some of Philipp’s work:Can Adaptive Seriational Risk Parity Tame Crypto Portfolios? Jochen Papenbrock, Peter Schwendner, Philipp G Sandner. Available at SSRN 3877143. 2021.Cryptocurrencies, DLT and crypto assets–the road to regulatory recognition in Europe 1. Agata Ferreira, Philipp G Sandner, Thomas Dünser. Forthcoming in: Handbook on Blockchain, Editors: My Thai. 2021.KOSMoS Private Blockchain Toolkit: How to Use Hyperledger in an Industrial DLT Project 1. Martin Schäffner, Constantin Lichti, Jonas Gross, Philipp Sandner. 2021.Relevant links for Philipp:Twitter: http://twitter.com/philippsandner 2LinkedIn: https://www.linkedin.com/in/philippsandner/Key Questions for our PanelistsSome of the questions we explore during the panel include:What is your working definition of CBDCs or what do you see as the minimum design choices that define a CBDC?What is a CBDC and how are they similar or different from other blockchain based projects?What problem are CBDCs solving?Consumer vs banks vs capital markets vs other stakeholdersHow do blockchains and CBDCs interact with each other?Why is the time now for CBDCs?What is the current state of the CBDC narrative?What are the main challenges that CBDC projects face?Will CBDCs be built on top of public or private chains?How will CBDCs and decentralized cryptocurrencies coexist?Are there privacy considerations to be discussed?Do you see a ‘race’ to roll out a CBDC with over 80 countries actively exploring or experimenting with the idea?"", 'Can you post the date and time of this panel? Thanks!', 'I’m not able to find it on the public schedule right now. I’m told it’s on Thursday, Aug 5 on the innovation stage from 12:00 PM to 12:40 PM EDT.', 'I think there might be some last minute changes so it’s best to track their conference schedule 2 for now. We can add a comment with the panel times closer to it as the times are finalized', 'Some takeaways from the panel discussionFeel free to add/correct/contradictCDBCs allow people like you and me to use money from a checking account in a central bank. (Lend, borrow, spend, save, etc.)Financial services will be available by digital currency APIs from the central bankThis immediately changes the role commercial banks are playing. They will no longer be the only intermediary to many financial servicesThe technology would provide transparency to monetary policiesIt would also give insight into how money flows in the economyTop concerns in CDBCs include the way we handle privacy. As current policies are legacies rather than great designsSophistication of technology & administration power of banks are also important. It should protect asset holders from losing access to their rightsWe should also explore why some people in advanced economies are unbanked, to see if CDBCs might change the game for them', 'That’s a great video! Thanks for sharing so many wonderful thoughts and opinions about CBDC!The Bank for International Settlements’s definition of a CBDC as “a digital payment instrument, denominated in the national unit of account, that is a direct liability of the central bank.” is a good way to see CBDC issues from a legal perspective. It shows the distinctions between CBDC and money issued by banks, which represent banks’ liability to the retail customer. This implies there may be some saving and lending competition issues between central banks and commercial banks, and it might cause economic changes to a large extent, the systemic risk may arise from different dimensions. However, these potential problems can be solved by adjusting the CBDC issuing structure. If the status of CBDC is a substitution of cash rather than deposits may also alleviate the problems. So opinions of maintains multiple-layers system --banks as intermediaries between the central bank and retail customers-- still prevalent, it prevents large impacts mentioned above and remains diverse and competitive services among commercial banks or financial institutions, and the function of monetary policies can process without huge deviation. In this way, those advantages mentioned by speakers of taking CBDC could be more persuasive to those who have these concerns.On the other hand, to what extent the privacy right should be protected is crucial. As we can see, democratic countries are committed to developing a system that aligns with the requirement of privacy. We hope this could be solved by technical adjustment. Also, we people need to know what the purpose of some countries taking CBDC is. There are some privacy concerns, especially in those countries that want to collect information about people or want to capture information and influence from private sectors. It may be difficult to claim or argue if we use CBDC of countries that intend to invade privacy.The need for CBDC seems to become obvious based on the fact that payments services are prosperous. Many payment scenarios are asking for some kind of well-backed capital source. When private payment services grow like a monster, and if we don’t have CBDC, would it be another source of problems?In addition, I’d like to share Taiwan’s central bank 9/17/2020 research report regarding CBDC 1. It mentioned that there are limitations in DLT-based CBDC, especially in real-time, enormous, and high-frequency transactions. It cannot meet the efficiency requirements of the payment system mainly due to privacy protection. Could the latest DLT technology resolve it?', '@Fizzymidas brought up an excellent point about sovereignty in one of our Community Calls. I hope I can do this justice: apparently the Bank of Ghana is launching its CBDC project on their Independence Day but they’re using a third-party service (I think it was HyperLedger) to provide infrastructure. What would happen if the company folded? Or was bought out by a regional rival? Faith is also working on a project here at SCRF (with @Sami_B) that indexes decentralization, perhaps similar metrics or some other sort of auditing process could be applied to CBDC blockchains.', '@socrates1024 I really enjoyed this panel discussion, and I was delighted to find you on here – I’ll keep this brief because I know a couple of researchers have been preparing questions, but I was hoping you could explain to me why it would be unlikely for a CBDC to use a public blockchain? Wouldn’t it be riskier to rely on a third party who’d have to build the system? Thank you so much for your time.', 'The Central Bank of Nigeria (CBN) has adopted various policies and released guidelines in efforts directed to control the devaluing state of the Naira. Earlier in February 2021, the CBN instructed all financial institutions to withdraw all support services for transactions involving cryptocurrencies.The apex bank cited the need to protect the financial system and the youths from the risks inherent in crypto volatility.Further concluding in a press release that “… *cryptocurrencies are largely speculative, anonymous **and untraceable, they are increasingly being used for money laundering, terrorism financing and other *criminal activities”.Nigeria is battling dollar shortage largely due to the disruption of international trade by the COVID-19pandemic and dwindling oil prices. The apex bank has had to devalue the Naira thrice since 2020, ameasure that has failed to strengthen the value of the Naira.A contributory to the fall in the value of the Naira is the preference by many to hoard foreign currenciesdue to the various economic risk concerns across Nigeria. However, the continued adoption ofcryptocurrencies as store of value over this period and means of remittance may have also mildly sidelined the Naira as even the Bureau De Change Operators (BDCs) have been linked to exchange transactions to render cryptocurrencies into foreign currencies in response to restrictions trading against the Naira.BDCs are licensed to provide retail FX services such as buying from the public and selling to users forallowed transactions such as Personal Travel Allowance (PTA), Business Travel Allowance (BTA), medical fees etc.Based on available data, Nigeria ranks no. 1 currently as the leading country per capita for cryptocurrencytransactions at 32% adoption rateNonetheless, there is a need to review the various steps and guidelinesreleased by the CBN which have been ineffective in strengthening the Naira.Notable FX guidelines released in 2021• July 2021; Ban on the sale of Foreign Exchange to Bureau De Change Operators citing groundsthat BDCs facilitate graft and corrupt activities of people who seek illicit fund flow and moneylaundering in Nigeria.• 30th August; Directive on publication of names of defaulters of the CBN policy on the sale of Forexfor Personal Travel Allowance (PTA) /Business Travel Allowance (BTA)- Directed that all bankspublish on their websites, the names and BVN of defaulting customers that present false traveldocuments or cancel flight tickets without returning the purchased FX within 2 weeks.Notable is the fact that the limit to PTA and BTA is $4,000 and $5,000 per quarter respectively.• 28th July 2021; Restriction on licensing of new Bureau De Change Operators and refund ofminimum capital deposits/licensing fees for pending license applications.The President of the Association of Bureau De Change Operators of Nigeria (ABCON) has made astatement that the decision to ban sale of FX to its members is the driver for the continued inflation ofprices of goods and services in Nigeria. The ban has further restricted the supply of FX in the parallelmarket as banks.The CBN stated that the sale of $110 million weekly with allocation of $20,000 to each of the over 5,500BDC operators in the country was taking a huge toll on the national foreign reserves. However, BDCoperators fix rates in the foreign exchange market and this influenced the restriction of the sale of forex to BDC operators.However, the Naira has kept on losing value despite the increase in foreign reserves of the nation from arecord low of $33.09 billion in July 2021 to $34.26 billion by September 3rd 2021. This is deducible from the continuous fall in the value of the naira within a week from N520/USD to as low as N545/USD (48-year low), N745/GBP and N636/ EUR in the parallel market.Other factors include panic buying by Forex users and BDC operators hoarding FX in anticipation of rates fluctuations due the policies.The CBN needs to be cautious to ensure that the restrictive FX policies will not be counterproductive due to the rising demand for foreign exchange and its exploration of the national digital currency. Legitimate end users that are unable to access FX supply can rely on cryptocurrencies to remit payment for transactions that support crypto payments or cross-border remittance.Nigeria’s CBDC (e-Naira)The CBN engaged Bitt Inc, a Barbados Fintech based company as its Technical partner to pilot the national digital currency (eNaira). The apex bank stated that its selection of the project partner was based on thecompany’s prior experience with the successful launch of the CBDC of the Eastern Caribbean Central Bank (ECCB) earlier in 2021. The selection process considered technological competence, efficiency, platform security, interoperability and implementation experience.However, the eNaira transaction has a spending limit in three tiers (Tier 1, Tier 2 and Tier 3) with the Tier3 having the highest spending limit. Tier 3 wallets have a transactional (sending and receiving) limit of N1 million Naira, an equivalent of about $1,850 at the parallel market rate.With the advent of the eNaira, the CBN is targeting increased cross-border trade, accelerate financialinclusion, tax collection, monetary policy effectiveness, cheaper and faster remittance inflow. eNaira will not operate as a store of value for users as it is a form of payment with non-interest bearing CBDC status.Though, it is arguable that the sovereignty threat and infrastructural risks in appointing a foreign partnershould be a consideration and preference to be given rather to a local company.Based on Executive order No.5, Government Ministries, Departments and Agencies (MDAs) are to engage local professionals in the planning, design and execution of national security projects. A provisional exception is the situationwhere foreign companies with demonstrated capacity to handle such indigenous projects can be awarded the contract in preference to local professionals where they lack expertise.Reports in the media claim that the CBN has conditioned Bitt Inc to register as a Limited Liability Company in Nigeria and the CBN will own shares in the Nigerian entity.', 'Wow. This explains a lot. Thank you so muchThe guidelines no doubts points in the directions that the federal government have once again proven not to have trust in indigenous companiesAgain, the essence of the CBDC has been faulted with lot of possible privacy risks. The country’s economic flow will be controlled by foreign establishments which the federal government has little or no control over.Despite Bitt Inc, being licensed under the CAC, it doesn’t make them a Nigerian establishment controlled by the Nigerian government.Also, to what extent can the traditional financial institutions handle all these regulations?. With the continued naira devaluation?Finally, I think Nigeria is not ready for the CBDC project. This is a country of 200 million population with less than25 million of them having access to a smartphone. It is not a well thought project to begin with.', 'There is one question that most governments still need to answer. Excluding tracking of funds, of what use are CBDCs if they will be restricted to a particular country? Isn’t that the same as using the fiat of that country?Although countries like China have successfully deployed their CBDCs, I do not see a need for CBDCs for the African continent. What Africa needs is better monetary and financial policies to help fuel intra-continental trades and boost the African economy.We have seen countries like Venezuela fail with its petroleum-based cryptocurrency ‘Petro’. The coin failed to address the problem of inflation, although we can blame that on the U.S. sanctions, yet, it’s proof that CBDCs may likely not address inflation.Like Richard Brown mentioned in the panel, there is a lot of similarities between CBDCs and stable coins why not focus on stable coins or cryptocurrency like the government of Estonia.I believe that better policies will help the Nigerian economy in particular and Africa at large.Why Governments should consider alternatives to CBDCsIf the Nigerian government is so worried about its people using cryptocurrencies and not Naira or Dollar (as a policy banned banks from transacting with crypto entities earlier in 2021), why not tax cryptocurrencies?My thought:  Taxing crypto will not be possible when the current tax system isn’t functional.Why not utilise a blockchain such as Ripple it Stellar Lumens for cross border transactions thereby facilitating cross border transaction payments across Africa and reducing our reliance on Dollars or France CFP franc?—I must admit that there is a whole political reason why this idea may not be visible soon but it’s still a more viable option to fighting inflation.CBDCs place a limit on the users that cryptocurrency eliminates. Transacting with a currency such as the E-naira means that as a Tier three user which is the highest tier can only transact daily with about $1,850. What incentive does this give to the citizens above the normal naira? Why not create favourable regulations for cryptocurrencies?Using the E-Naira as a case study for CBDCs, I would say, that governments still have to do a lot of homework as to whether it fits into the economical terrain of their country or not.', 'This is an expository write up and a very good points that should be considered.Africa has no business with CBDC for now as there are other fundamental problems that needs to be addressed like the issues with financial and economic policies. CBDC will change little or nothing on inflation.I am also of the opinion that Africans should look out for Africans to solve Africans problem. Not some folks from a distant location telling Africa what to do.', 'Thankfully, the deployment of the Enaira CBDC has been postponed to an unforeseen future. This is good news as it would have served as an example of what not to do when launching a CBDC. Hopefully, the government and stakeholders can review the project better and make needed adjustments before redeployment.', 'I don’t think Nigeria is actually serious about this e-Naira, I think it’s just a strategy to divert our attention away from the Cryptocurrencies they banned and now the launching has been postponed indefinitely. Maybe they’ve done their findings and discover Nigerians won’t buy the idea of e naira since it’s equivalent to our Fiat. Nigerians would rather trade normal Cryptos over e naira but we await the launching of the e naira.', 'A couple of questions for next panelistsHow has the definition of CBDC’s changed?What CBDC’s are likely first to be active?Are there any correlation between tech used by other protocols ?When will now be a good time? What coverage in media has been published on CBDC’s?Would it accelerate a CDBD to start it from a more centralized environment?What would a bottom up approach to a CBDC look like?', 'It is quite interesting that since this post, the eNaira has been launched and banks in Nigeria were ‘instructed’ to publicise its use and include it as a service. Nonetheless, in my opinion, the e-naira died a quick death due to lack of use and trust in it. It is also noteworthy that the technical infrastructure put in place also did not encourage its use. For instance, there are people who did not receive the e-token necessary to complete the registration process. Also, the populace who trade cryptocurrencies definitely do not want the government surveillance that comes with the use of the e-naira. It would take serious governance and structural changes to resurrect the e-naira in Nigeria.', 'I think these are really great and insightful questions @winum.I would like to suggest a couple of questions;Has any recently adopted CBDC made any meaningful contributions towards financial inclusion?What are some challenges that have been faced by recently adopted CBDCs?How can these challenges be mitigated?', '@winum Thank you for raising so many great questions, they are definitely worthwhile to have further discussion.@Tolulope You mentioned government surveillance, that’s an interesting issue, the importance of privacy of people can’t be overemphasized. It also reminds me of the third-party doctrine in the USA court’s practice, which allows the government to access records and materials that are delivered to third parties and out of the defendant’s control much easier. Some people have concern about the excessive power it provides to the government. However, it seems that CDBC leaves the movement no space.']"
                Research Summary: Leveraging Blockchain for Greater Accessibility of Machine Learning Models              ,https://www.smartcontractresearch.org/t/research-summary-leveraging-blockchain-for-greater-accessibility-of-machine-learning-models/808,Tooling and Languages,83,[],['summary'],"['TLDRThe researcher proposes a framework for open-access decentralized machine learning algorithms that have shared databases and frameworks without centralizing data processing mechanisms.He proposes a collaborative trainer, an incentive mechanism, and an additional automated data handler that can be stacked to create a superior final model for a machine learning process.The researcher suggests that by creating blockchain-based incentives for contributing to data sets, it may be possible to improve the public’s access to high-quality data for improved machine learningCore Research QuestionCan a model for building decentralized machine learning (ML) algorithms that can help accelerate the evolution of AI be designed? What type of model for building decentralized ML algorithms would democratize artificial intelligence (AI), keep costs low, and keep its data updated and relevant?CitationMicrosoft, J. D. H., Senior Software Developer at. (2021). Leveraging Blockchain for Greater Accessibility of Machine Learning Models. Stanford Journal of Blockchain Law & Policy. Retrieved from https://stanford-jblp.pubpub.org/pub/blockchain-machine-learning 23BackgroundThe researcher proposes an open-access machine learning framework in which collaborators are rewarded for contributing data that is deemed “good” whereas those who have contributed data deemed “bad” lose their contribution fee.The researcher suggests that including a smart contract in the process of deploying machine learning training algorithms could be more secure than a purely open-source system which does not include incentive mechanisms in the design.He suggests that a small fee for contributing data combined with a reward mechanism for data that is validated as good can lead to improved ML algorithmic accuracy.The researcher uses the Internet Movie Data Base (IMDB) as a test set from which to assess the data and classify sentiment.The original framework was proposed in 2019 at an IEEE conference on BlockchainPerceptron 2: a linear machine learning algorithm used for binary classification (i.e. whether a film was given a positive or negative review). The model uses linear and logistic regression to take weighted data and break it into two classes.IMDB Sentiment Classification Data set: A list of 25,000 movies labeled by sentiment (positive/negative) IMDB movie review sentiment classification dataset (keras.io)Demo of Incentive Mechanism: Decentralized & Collaborative AI on Blockchain Setup + Demo - YouTube 6SummaryAdding data to a model in the Decentralized & Collaborative AI on Blockchain framework consists of three steps: (1) The incentive mechanism, designed to encourage the distribution of “good” data, validates the transaction, for instance, requiring a “stake” or monetary deposit. (2) The data handler stores data and metadata onto the blockchain. (3) The machine learning model is updated.705×499 107 KBMethodA Perceptron (an an algorithm used for supervised learning of binary classifiers) is used to train a model on IMDB data for sentiment classificationApproximately 8,000 training samples are used in the first simulation and 33,000 total training samples are used in the second simulationResults721×543 186 KBThe first simulation is conducted with the condition that users contributing data that is confirmed to be “bad” lose their submission fee.Lost submission fees are split among the contributors of “good data”The balance represents starting funds contributed to the pool to be able to update the algorithm.The initial decline in balances represents the accumulated fees being paid into the pool, while the uptick in good balances is the accumulation of rewards from bad submissions paid out to users that submit good updates to the algorithm.In this simulation, a contributor that has been found to have made a “good” contribution has their fee returned and a point added to their reputation score.This simulation shows a positive increase in the accuracy of the sentiment analysis over time.718×522 152 KBThe second simulation adds 25,000 more training samples to the data setThe second simulation operates under the premise that a nefarious actor is intentionally trying to add “bad” data to the test sets actively paying for the attackEven under the condition that an attacker is willing to pay to inject “bad” data, the “good” data contributions offsets the attacker and the accuracy of the ML algorithm does not drop.The tests took place in August of 2020; the researcher estimated the cost of each update to the algorithm to be roughly $.40 USD.Discussion and Key TakeawaysThe researcher suggests that open-access machine learning on blockchains have great potential for incentive mechanisms to improve the efficiency of training algorithmsThe researcher also suggests that the costs associated with updating the algorithms may be used to dissuade nefarious attackers and may eventually stop attackers when they run out of money to attack a specific object.Implications and Follow-upsBased on a rough estimate of price change since the publication of the article, the current cost of updating the training algorithm is approximately $2.88 USD under the assumption that the researcher is unsuccessful in decreasing the average cost of pushing an updateThe researchers address frequently asked questions about their publication at the following link: https://github.com/microsoft/0xDeCA10B/blob/main/README.md#faqconcerns 5ApplicabilityOpen-access machine learning algorithms may be beneficial for training AI assistants more quickly.AI depends on machine learning algorithms, and more accessible algorithms would translate into AI that learns more quickly.The nature of ML algorithms depends on perpetual training, which is partially the reason why the researcher looks for methods of making the long-term updating of a training algorithm as inexpensive as possible for contributors with a history of “good” data training submissions', 'Great read. Thank you for contributing.I noticed that you specified that the inclusion of incentives will help the public have better access to data that would help develop better ML practices. Assuming the members of the community that are attempting to improve the public’s access to “good” data are all benevolent, the incentives concept seems to have a lot of promise. However, is there a chance that someone could maliciously provide data that is meant to seem “good” but is fundamentally incorrect?If so, what would be some potential vetting mechanisms to ensure the data is valid for the purpose of furthering the efficacy of ML models?If not, what aspect to this proposed system would eliminate faked “good” data?I suppose this question is coming from the understanding that on some platforms bots can disproportionately affect voting and wrongly inflate the value of information.', 'Thanks for the questions!The researcher actually did a model in which they attempted to inject “bad” data.  Ultimately, since the malicious actor did not receive a reward, the “good” actors inevitably outweigh the bad actor and the accuracy is not affected negatively long-term.The second simulation that is posted addresses that specific question.  Effectively, a nefarious actor would have to have “unlimited funds” to outweigh a group of good actors.  A nefarious actor can cause a slight dip in accuracy, but over time the good actors will receive their fees back and continue to contribute in contrast to the bad actors eventually running out of funds.“Faked good data” is just “bad data”.  There is no qualitative difference, as a system that is designed to tell “good” from “bad” data will not be able to be “tricked,” i.e. how it is able to determine what is “bad data”.“How would one determine what is good and what is bad?”Consensus.There will be a point at which a certain percentage of nodes agree on the information as being “good” because they can check it against previous data sets.  When a model is being trained initially, that is the point at which “good” and “bad” will be determined.  Anything entered into the system will either be “good” or “bad”.  There is no “fake good” data.  That’s simply “bad data”.For example, if a node tried to inject data that was qualitatively identical “good data”, but was coming from an “untrusted node” the data would be classified as “bad data” due to the node not being trustworthy even though the “data” itself looked identical to “good data”.  That’s still just “bad data”, due to the fact that it’s coming from a node that has been compromised.In this context, “bots” would still be limited by the funds allowing them to update a database, and thus the injections of bad data would still be limited by the fund pool from which the malevolent actor is drawing their capital.', '@Larry_Bates did you get any sense of how effective these incentive mechanisms are when compared to other ones that the industry uses? I know that for years the Internet Movie Database had a contest for improving its algorithms (and Alexa does a similar thing with its algorithm, only they focus on college teams to create the algorithms). Would a prize mechanism work for cleaning, organizing and adding to databases? Or would is it more effective to have a smart-contract based vetting system in place?', 'Considering the niche demographic, I am not sure adding a prize incentive would necessarily increase the pool of contributors to the algorithm.  In this case, I believe a vetting system would be more appropriate as a means of ensuring that no nefarious actors were getting into the system.  In this case, the incentive mechanism seems to serve as a means of keeping contributors, rather than getting them to start contributing.  As contributing data to a perceptron is not a low-level activity, there is an expectation of some level of knowledge that has to be present for a person to contribute.  As that is the case, there is nothing to indicate that people who contribute to open-source machine learning algorithms are any more motivated by the incentive mechanism.  However, the data DOES show that the incentive mechanism can in fact prevent nefarious actors from continuing to contribute by taking their submission fees and giving it to a good actor.While it is called an “incentive mechanism” it serves more to discourage nefarious activity than purely to serve as an incentive mechanism.There is a positive reward AND simultaneously a negative punishment.  In that, a positive reward is rewarding someone by giving them something, whereas a negative punishment is a punishment that works by taking something away.  You can have a “positive punishment” which works by adding something unpleasant.  You can also have a “negative reward” by taking something unpleasant away.  In this case, the mechanism employs a positive reward and a negative punishment.', 'If I am not mistaken, the model that is undergoing training is the same model that tests if the data works.This is the quote from the paper that lead me to this understanding.Ongoing self-assessment: Participants effectively validate and pay each other for good data contributions. In such scenarios, an existing model already trained with some data is deployed. A contributor wishing to update the model submits data with features x, label y, and a deposit. After some predetermined time has passed, if the current model still agrees with the classification, then the contributor’s deposit is returned.If this is the case, suppose someone submits a very large amount of false or attacked data, tricking the model to take their data as good and others bad. The attacker will reap huge rewards at the cost of the effectiveness of the model and trust in the community.Sounds like a huge risk? How does the author address this issue?', 'I think you are misunderstanding.  The “training set” is closed and is not open to the public.   This is how the algorithm knows what is “good” or “bad”.  The scenario you’re talking about is not possible due to not having the training part “open to the public”.  If the algorithm is corrupted during the “training” process, then it would have never been accurate in the first place.  This is why the research author is explicit about the training phase coming before the algorithm is released into the wild.Someone would not be able to submit “fake good data” that tricks the algorithm once it’s trained.  If you try to elaborate on what constitutes “fake good data” you just end up with “bad data”.  This is a problem created from circular logic arising from misinterpreting the training stage.What you’re saying is not actually a “risk” and in fact they duplicated this very scenario in the tests.  That is stage 2 of the tests.  They did exactly what you’re saying.  They actively injected a ton of “bad data” into the algorithm.  That came AFTER the training phase.  It briefly made the algorithm less accurate, but because an attacker does NOT have unlimited funds, eventually good data rebalances the algorithm.The notion of “tricking the algorithm” in the way you presented is literally how they tried to attack the model.  An “abstract attack” does not have a legitimate threat vector until it is made into a tangible line of attack.  Your abstract framing was realized into the simulated attack.  Again, the attacker does NOT have unlimited funds.  Thus an attacker may be able to affect the algorithm in the short term (as it was shown) but the “good data” that is submitted over time outweighs the “bad” data, thus keeping the AVERAGE accuracy above 79.5%.“already trained with some data is deployed” this is a key part of the quote.  Once “training” is closed, an algorithm is going to have a consensus set that is “right” that can be checked against and not corrupted.  The “training” set stays closed while the algorithm learns based on new input.  While the training model “updates itself” based on new input, the actual original training model is not “changed” by the new data.  It is just added upon.  Thus “new bad data” can’t corrupt the training algorithm.  That’s why it happens in stages.There is no such thing as “fake good data”.  That is just misinterpreting “bad data”.  Fake data is bad regardless of whether it “looks good” or not.  It’s just “bad data”.  I think that construct is what is throwing you off.If someone had an “infinite supply of funds” from which they were drawing to inject “bad data” into the algorithm, then most definitely that actor could ruin the algorithm.  Fortunately, someone with “infinite supply of funds” does not exist i.e. why that “abstract threat” is not a “real threat.”', 'The initial model needs to be accurate to begin with. If that’s the scenario, wouldn’t the model author easily able to simulate & add in more training data on his own to improve the robustness of the model?If initial model is less accurate (highly biased model), wouldn’t this actually incentivize people to upload more biased training data?Is the whole system designed to “increase” the accuracy of the model or “maintain” the accuracy of the model? A model which actually “learns” should be able to classify more “ambiguous” samples correctly overtime. If initially the users are punished by providing ambiguous data, how do we improve the generalization of the model?', 'Thank you for the questionsUnder normal circumstances this is the case, but this is the point of making it an experiment about open-sourcing machine learning training.You have described an abstract concept without elaborating on what that means in this scenario.  What would “highly biased” mean when the data is being pulled from a data set?  How would you define that?  Are you implying the researcher left data out?What are you trying to define with “more biased training data”?You have not given a tangible enough definition for this point to be more explored.It’s designed to maintain, and hopefully increase accuracy with the expectation that it can never attain 100% accuracy.  There is no “ambiguous” in this model.  There is only “good” or “bad”.  There is no “ambiguous” data.I think people are approaching the data like a human.  Computers are not fooled by fake time-stamps or slightly modified data.“Ambiguous” or “fake” is the same as “bad”.', 'Thank you for answering.To add more context to point 2. A highly biased model means an under-trained model. For example, let’s say the model owner is only able to train with a limit amount of labeled data for his text classifier(he does not have access to any more data on his own), then the model will likely make poor prediction on new data (+/- sentiment). Then that same model will be used to punish the users who try to contribute data/label according to the wrong predicted label because they don’t want to be punished by going opposite of what the classifier “thinks” is right. Then overtime the model will be fed with more data with incorrect label and still thinks they are right. It will be a self-defeating model. Are the researcher exploring ways to make this task less exposed to the initial quality of the model? Or is this is not an issue at all?Example:Under-trained text-classifier gets uploaded → 2. User contributes by supplying a piece of data and label: {“data”: “the movie was ok”, “label”: “Negative”} → 3. Model says it should be “Positive”, in reality it should actually be negative → 4. User changed the label accordingly to “Negative” to avoid punishment-> 5. Model keeps learning towards the wrong label overtimePoint 3 about the ambiguous data is somewhat related to point 2. I understand there is no “fake data” and that ML models can never achieve 100%, but say if I upload a piece of data and label: {“data”:”this movie was really on another level”, “label”:”Negative”) and the model gives 51% (low probability) to Positive because it’s really hard for it to tell if the text is sarcastic, then the user is forced to mark it as positive, then wouldn’t this hurt the actual use case (generalized model) in the real world? Would make sense if the framework can discard ingestion of data if its confidence in the provided label is below a certain threshold, thus maintaining the quality of the model overtime?', 'Thank you for taking the time to expand upon this subject.  I am so glad that you did highlight this issue, because this is the difference between an abstract threat and a realized threat.  What you just explained is in fact a real threat.  This is not just a problem with machine learning, but any data analysis.  The reason the recent Facebook study data sets were problematic is because Facebook did not provide researchers with all of the data that was available.  This was likely an intentional attempt to weight the data towards a desired outcome instead of unbiased research.An “Under-trained” model would in fact be a problem that would need to be addressed to ensure that an open-source ML algorithm would not have wildly inaccurate assessments.  This is one of the reasons the researchers included the number of training sessions in their full explanation, but the maintained accuracy of the model in the results indicates the researchers did not undertrain the model.  The fact that they actively injected bad data into the data set without corrupting the accuracy long-term is also an indication that their model was not under-trained.In your final example, I think you have found a circular problem without knowing it.  If a user is not “clear” about their feelings, it is ambiguous and thus there is no clear way to know if it is ""positive or “negative”.  This data is effectively “bad” data so it’s likely going to be thrown out.  There is a threshold established which qualifies “good” vs. “bad”, and I believe you are asking about the movement of that threshold in real-time relative to ambiguously stated “sentiments”.  If I am understanding you correctly, then someone who is not clear about their sentiment cannot have their sentiment “accurately” assessed because it’s “bad data” and would likely be thrown out.I believe I understand what you are meaning by “ambiguous” now, and in this case “ambiguous” is “bad” because it is not “clear”.  If there is no clear way to establish “positive” or “negative” sentiment about something based on the “ambiguous” language, that is deemed “bad” data and either counted as “bad” or thrown out altogether.If that is the understanding, I apologize for misunderstanding the usage of “ambiguous” earlier.  I can see how you were trying to ask one question, and it looked like another, and I was slightly misinterpreting your statement.  Your clarification was very useful to get to a salient point, and your initial questions were valid.  I just wanted to make sure that was not lost in text, as I did not want to come off as aggressively asking you to define “ambiguous” in this context.  I hope you can understand now why I was slightly misinterpreting your question, but even further this thread would make a real-time example as to why ambiguously framed data usually does not get counted without further elaboration.For the record, the data sets analyzed were binary response so it was effectively “like” or “not like” with no room for misinterpretation.  However, if the data sets were free-response then the problem you raised would be more relevant.  IMDB data sets were used to train their algorithm in addition to being the data sets from which the open-source analysis would come.  The reason the researchers chose IMDB specifically is because their sentiment analysis is binary and not free response.', 'Thanks for trying to answer my questions. I appreciate that you took it seriously. Your discussion with xiaotoshi is also insightful. Here’s my follow up:–It is claimed that when the training set is not public, an attack is not possible. That is not obvious to me. To my knowledge, access to the dataset is not strictly necessary for a successful attack.The breakthrough had been around since 2016.arXiv.orgPractical Black-Box Attacks against Machine LearningMachine learning (ML) models, e.g., deep neural networks (DNNs), arevulnerable to adversarial examples: malicious inputs modified to yielderroneous model outputs, while appearing unmodified to human observers.Potential attacks include having...The authors successfully attacked models hosted by Amazon and Google (without knowing what it is trained on), demonstrating how vulnerable seemingly powerful AIs could be.As for the notion that there wouldn’t be enough funds, this is also addressed by another paper:Our results are alarming: even on the state-of-the-art systems trained with massive parallel data (tens of millions), the attacks are still successful (over 50% success rate) under surprisingly low poisoning budgets (e.g., 0.006%).arXiv.orgA Targeted Attack on Black-Box Neural Machine Translation with Parallel Data...As modern neural machine translation (NMT) systems have been widely deployed,their security vulnerabilities require close scrutiny. Most recently, NMTsystems have been found vulnerable to targeted attacks which cause them toproduce specific,...This shows that data poisoning is achievable at a fraction of the whole pool.Although this protocol makes attacks easier not only by letting anyone deposit data, but also provides a strong incentive for people to trick the model so that they could profit from not only the normal rate of reward but also the fees of other contributors.–Adversarial machine learning attacks are not the only threat in this protocol.Another problem emerges from the risk of over-fitting.For those who might be reading this comment be doesn’t know what over-fitting is in the AI/ML/DL context, here’s an analogy to put it plainly:You let the students (AI model) practice the same test bank (dataset) over and over again (train many rounds).Humans would gradually improve as they practice more. However, machines operate a little differently. It is observed that if trained too long on a dataset, it leads them to draw wrong inferences.Take this for an example: when the machine is what the color of the flower is (and showed a picture of a sunflower), it can correctly answer “yellow”.Although at surface answered correctly it was yellow, explainable AI analysis found that the machine just look for the word “flower” and answered “yellow”, not taking into the context of the question.The model mislearns when they are trained on similar data, over and over.So if the reward is based on whether the model deems the data correct, then the rational thing to do for someone seeking minimal cost and maximum returns, is to submit the same type of data repeatedly with only small modifications.That would be unideal. This is exactly why high-quality data is so important to performance. Yet the protocol goes for quantity over quality.', 'Your observations about over-fitting and additionally poisoning the data are spot-on.  The notion of poisoning data is a real threat that is taken seriously.  In that regard, one of the most practical and widely implemented solutions to this problem is to have a machine learning/human hybrid 1 quality-control combination at some point so that the ML is not completely automated.  Additionally, this is where “supervised 1” vs. “unsupervised 1” machine learning becomes where the “poisoning” becomes less or more viable for an attacker in skewing the data analysis or skewing the model’s fit.As a supervised ML algorithm will have its classifications and levels mostly set from the beginning, the issue of overfitting is not as much of a problem.  Additionally, it’s easier to isolate false positives in the data when the classification levels are already defined.  Conversely, unsupervised algorithms would be harder to manipulate in real-time in that it’s not entirely possible to know what classes have emerged unless one has access to both the data sets and the results of the algorithm’s output.As the field of “Artificial Intelligence” emerged as a subset of computer science, “Hybrid Intelligence 1” looks to be emerging as a subset of the AI field.Also, the “reward system” comes on a delayed schedule so that data can be analyzed for quality before payments were sent out.  This makes it so that an attacker would not know if their attack was successful for an amount of time that makes it more difficult to launch a “successful” attack.  The examples you posted don’t seem to have the monetary limitation on them which makes it possible that an attacker could spam a significant amount of data in a short period of time compared to the pool that is limited by submission fees.The last scenario you presented would not really be profitable or likely even “feasible” for an attacker due to the cost as a barrier to entry.  Effectively, if an attacker decides to spam this type of pool with “bad data” to make it look like “good data” so they can get a reward; the cost of attack starts to outweigh the benefits of the reward the closer the threshold of “poisoning” gets to 51% of the data set.  That is to say, if it took an attacker in the worst-case scenario the need to submit 51% of the data set to corrupt it to then influence the data set so they would get the most reward, 49% of the pool still gets the other 50% of distributed rewards.  The contributed data is not equivalent to the proportion of the reward, and the number of contributors to the pool is also a factor, i.e. why contributing 51% of data won’t result in receiving 51% of the rewards.  In this scenario, an upper limit exists on what percentage of a pool can be taken through malicious attacks before the attacks become unprofitable or even have the potential to cause the attacker to incur a loss.While an algorithm with no monetary fee may be able to be poisoned with a small amount of data, the same experiments would need to be run on ML algorithms that have fees associated with training.  In this case, the experiment shows that the fees eventually stop the attackers from continuing to contribute.  However, there obviously need to be more exhaustive attack scenarios explored before any definitive statements can be made about the benefits of this approach.While there may be “some” incentive to try and game the system, there is not actually a “strong” incentive due to the nature of the pool’s distribution mechanism and the weighting of payments not being winner-takes-all.  Albeit, the lines between “little”, “some”, and “strong,” incentive are the lines that the future research needs to determine.', 'The key point of this research seems to be the relationship of incentives/rewards/punishments to maintaining algorithmic accuracy. That makes sense to me.I can see why “a small fee for contributing data combined with a reward mechanism for data that is validated as good” can lead to improved ML algorithmic accuracy.However, the other point made in the research is about security, and that one seems less obvious to me.Why does “including a smart contract” make “deploying machine learning training algorithms” more secure than “a purely open-source system which does not include incentive mechanisms in the design”? Where does the enhanced security come from?', 'It is not inherently a “smart contract” but a “smart contract with fees associated with it” that effectively limits an individual’s capacity to update a training algorithm.  At the time of the publication, it was roughly $.40 per update.  In the case of an open-source machine learning algorithm that has no smart-contract with fees associated with updating it, a nefarious attacker could run as many updates/attacks as time allows.  This makes a “fee-based update system” inherently more secure than a system which does not require fees to make updates.  The smart contract itself is not arbitrary, but is part of the mechanism managing the fee acceptance and redistribution.  In this case, it’s not the “smart contract” necessarily, but the “rules by which the contract is operating” that incur a fee to update the learning algo, while then rewarding users that have submitted data that was determined to be “good”.  The fees effectively limit an attacker’s capacity, as there will be no attacker with “unlimited funds” and thus in a fee-based system “unending attacks” cannot occur in the same manner in which they can occur in a fee-less open-source system.I will give a further example:  if the rules of the smart contract were poorly written for example an extremely short reward evaluation period and additionally giving rewards purely for “data contribution” and not “good data contribution”, the algorithm would likely skew towards inaccurate noise data from attackers as being the most influential.Since the smart contract’s rules were written to reward “good” data with a long enough reward period to give the system time to validate if the data was in fact “good”, the results skewed towards maintaining an accurate model.  The rules of the smart contract will ultimately skew the data in a direction, but in this case the rules skewed the data towards a net positive outcome concerning data accuracy.', 'Speaking about security, asking for an initial deposit makes a lot of sense to me. The current summary I am working on quoted another paper that uses something similar to prevent a DOS attack. They introduced an initial fee that will be returned only after the protocol was successfully launched. This discouraged malicious participants from trying to take over.', 'Hey, I’m the main researcher working on this.  Good questions. The words “secure” and “security” are very special words with deep meanings to me. I use them very carefully because they compose many things 2. Unless my Ctrl+F is failing me, I don’t see them use in this article and they’re not used in the original paper 1 either. I see the term is used in the blog post (emphasis mine):Leveraging blockchain technology allows us to do two things that are integral to the success of the framework: offer participants a level of trust and security and reliably execute an incentive-based system to encourage participants to contribute data that will help improve a model’s performance.Here the security is about transparency. Open-source code is great but it’s not sufficient. For example, even if the back end code for this website was open-source, I don’t know what really happens when I enter my comment into this box because I can’t verify that the code running on back end machines is the same as what is in the open-source code that I read. Smart contracts offer a stronger guarantee that the code we see is the code that runs.', '@juharris That’s a good answer, Justin. Not knowing what code is “actually running” on servers, and the fact that smart contracts make it “more knowable,” are huge points.I wrote my question a month ago, and I don’t frankly remember where I was quoting from, but it must have included the idea of “security” or I wouldn’t have quoted it that way. It may have been in the blog post which you cited.Thanks very much for your response.**Edit: Actually, I just looked, and the language I quoted was in fact from this article under your byline 2, which I took not as a “blog post” but as the original research article being summarized above. That doesn’t invalidate anything you say in your response, but for accuracy’s sake I wanted to note it.', 'For the record, I summarized the recent blog post which summarized the previous articles and works that were published into a more concise explanation of the experiment.  The original work from the IEEE conference was referenced in the background section:"" The original framework was proposed in 2019 at an IEEE conference on Blockchain""', 'No problem. The word “security” was used in some comments.']"
                Discussion Post: Can Gambling Increase Savings? Empirical Evidence on Prize Linked Savings Accounts              ,https://www.smartcontractresearch.org/t/discussion-post-can-gambling-increase-savings-empirical-evidence-on-prize-linked-savings-accounts/698,Mechanism Design and Game Theory,52,[],"['defi', 'discussion', 'summary', 'oracles', 'flash-loans', 'game-theory']","['Call to Action: In these threads, we attempt to further the discussion of a key problem in this category and evolve our understanding of the domain space where research work has not yet answered the specific problem or question being considered. These posts are living documents, and it is our hope that the community will continue to contribute to their structure and content.BackgroundPrize-Linked Savings (PLS) accounts could offer a safe, potentially profitable alternative to gambling according to a recent paper in Management Science. “Can Gambling Increase Savings? Empirical Evidence on PrizeLinked Savings Accounts” 8 suggests that, based on a case study in South Africa, PLS accounts do not necessarily cannibalize from regular savings accounts, while directly contributing to a reduction in gambling.Variations on PLS accounts are also beginning to emerge from the Decentralized Finance (DeFi) space from companies like PoolTogether. Often these projects use incentive mechanisms to promote saving and dissuade token holders from engaging in value extraction that might hurt prices or promote unwanted behavior. The following discussion also contains the results of a primary investigation into PoolTogether.Prize-Linked Savings (PLS): A savings account that adds a periodic prize that is randomly awarded to a recipient(s) at a regularly scheduled interval. PLS accounts may also offer annual percentage yields (APY) in addition to prizes, in this case, the PLS would be offering two separate incentive mechanisms.Million a month accounts (MaMa): A savings account in which depositors are entered into a monthly “Million Rand Prize” for keeping their savings in a MaMa account.PoolTogether 3: A Decentralized Finance (DeFi) project offering a variation on a PLS account, in which various cryptocurrency tokens are pooled into separate contests. Pools may offer differing APYs. Prizes may be awarded weekly or daily.Key Problem / Topic AreaIn “Can Gambling Increase Savings? Empirical Evidence on PrizeLinked Savings Accounts” 8, Shawn Cole, Benjamin Iverson, and Peter Tufano looked at PLS accounts in South Africa to assess the effects of the new type of account on the local population’s banking and gambling behavior. A case study 2 prepared by BTIG, LLC looks at PoolTogether, an example of PLS accounts being used to determine the benefit of rewards on the saving habits of small accounts.Specific Question or Problem StatementCan prize-linked savings accounts offer a viable alternative to gambling? Do prize-linked savings accounts cannibalize from regular savings accounts?Approach / MethodologyThe following charts break down the impact of PLS on various aspects of account growth during the South African case study conducted by Cole et al.Growth Rates of Standard 32-day Savings Before and After MaMaSavings Balances of Bank Employees: MaMa Users vs. NonusersEffect of Winning Prize on MaMa Deposits|398.85321100917434x718.9527559055118370×655 42.1 KBEffect of Local Jackpot Prize Winner on Local MaMa Demand328×689 35.7 KBGrowth of the MaMa Product526×574 42.6 KBThe following charts break down the PoolTogether depositors and their accounts by pool.Starting Deposits compared to Current Deposits by PoolChart1600×989 43.9 KBPercentage of Depositors per poolChart1600×989 108 KBValues of Highest Winning Accounts and Lowest Winning AccountsChart1600×985 69.7 KBAnnual Percentage Yield (APY) By PoolChart1600×985 49 KBMean of Winning Account Value Compared to Highest and Lowest Value of Winners by PoolChart1600×991 64.5 KBReward X Winning Accounts X Number of Depositors X Value of AccountsChart1360×844 53.2 KBSUM of Average Reward Value (last 12)Type of RewardAPY (at time of recording)Number of depositorsPoolDaily   Weekly   Grand Total   042GUSD   167.42167.4242 Total   167.42167.420 Total   167.42167.421.681129UNI   2749.882749.881129 Total   2749.882749.881.68 Total   2749.882749.881.77389COMP   8588.258588.25389 Total   8588.258588.251.77 Total   8588.258588.255.951866POOL   5077.85077.81866 Total   5077.85077.85.95 Total   5077.85077.86.082321USDC   47749.1147749.112321 Total   47749.1147749.116.08 Total   47749.1147749.118.625398DAI   36193.1136193.115398 Total   36193.1136193.118.62 Total   36193.1136193.1111.693323USDT   493.07493.073323 Total   493.07493.0711.69 Total   493.07493.0715.93574SUSHI   4983.574983.57574 Total   4983.574983.5715.93 Total   4983.574983.57Grand Total493.07105509.14106002.21Conclusions / Key TakeawaysThe research suggests that PoolTogether may provide similar incentives to a traditional PLS account based on the growth of account balances over time. However, the gas fees associated with entering and exiting a pool may offset any earnings, but upcoming changes to the Ethereum protocol may affect that aspect of earnings in a way that would significantly improve net yield.Research also indicates that although the weighting of prizes favors the larger accounts, it was fairly uncommon for the largest account in a pool to be awarded a prize more than twice in a row. This suggests that while the mechanism deciding winners is inherently weighted towards larger accounts, smaller accounts winning indicates that the pool’s fairness mechanisms are designed to benefit smaller and medium-sized accounts over the long-term instead of larger accounts in the short term. In this context, the smallest accounts benefit the least due to the odds effectively making it impossible for them to actually win.There may also be a benefit to using DeFi PLS accounts that use pegged tokens as the definitive token for their pools. In the recent market downturn, accounts with USD-pegged tokens naturally resisted the loss of value associated with market volatility, while pools using unpegged tokens suffered losses of value directly correlated with the market.CTA: Future Work / RFPThis work looked at a case study and conducted primary research on PoolTogether. Further research on DeFi PLS accounts and the successes or failures of the pools in helping depositors build savings could provide additional context and clarity.', 'PLS accounts look like the alternative to get people saving up instead of buying lotteries. It provides a stronger psychological incentive and would be more appealing.I’m still waiting for more empirical evidence though. The crypto world is quite different from traditional finance. There are lots of instruments available here. Pool Together has a lot to compete.I skimmed the paper and found one piece of evidence that would be in favor of Pool Together.Compared to bank employees with a savings account in the bank that offered MAMA, bank employees WITHOUT one are much more likely to open a MAMA.This means that PLS accounts can incentivize people to open an account pretty well. That is, it has some potential to escape from the competition, and create its market.', 'You are correct that we need to have more empirical evidence from the defi space before we can come to any conclusions about the potential for PLS accounts in the defi space.  I did use primary research on Pooltogether, as the charts from Pooltogether had to be created from hard data.  There is a benefit to having open access to the pools so that the data can be analyzed in contrast to traditional finance where their reports are quarterly and not specific on individual accounts.All that said, the current data shows that PLS accounts can potentially be viable alternative to traditional lotteries as a mechanism to increase savings while reducing gambling.', 'This is an interesting paper, but I do question the strength of the evidence regarding the reduction in gambling claim being made. But in some ways, that isn’t really the big news here. The real value here is the incentivization of a positive behavior as opposed to the alternative to a negative behavior (at least from my understanding).I found the fairness mechanism observation interesting. Larry_Bates:In this context, the smallest accounts benefit the least due to the odds effectively making it impossible for them to actually win.If the goal here is to get more people saving (which it might not be), it seems that some other mechanics would work better. Or is the likelihood to win enough of an incentive to continue doing the desired behavior?', 'The reason they correlated the findings with “gambling” specifically is that the balances in the mama accounts would change relative to large national lottery jackpots.  When there were large national jackpots, the mama accounts would have fewer deposits; but as the mama accounts increased their monthly awards relative to smaller national lotteries, they saw increased deposits.  It is obvious you can’t draw conclusions about causation in the real world, but they were finding correlation in movement with mama accounts and national lottery prize sizes.  This is also specified in the context of South Africa.The mechanism you’re observing in the latter part was relative to “Pooltogether” and not the South African study.  I don’t think the defi space has a good grasp on building incentive mechanisms quite yet.  The original research on the South African case study did not include a chart relative to the state lotto, but it was alluded to in the paper.  One of the reasons I did not go into depth into that part is because the author gets into an unnecessarily racial analysis of the data, and I was not inclined to open that can of worms in this discussion post.I think the issue is not even about modifying behavior so much as the exposure to “prize-linked savings accounts” which amount to “risk-free gambling”.  If more people were made aware that these types of mechanisms exist, they may be inclined to put their money into a lossless prize pool rather than risk a prize in a system in which they lose their entrance fee if they are not the winner.This is a zero sum design compared to a positive sum design.  We are talking about a complete paradigm shift concerning capital management.  In a sense, a “lossless prize pool” is still “gambling”.  This is why framing it as “pls vs. gambling” is not quite accurate.  “Gambling” implies a zero-sum game, but as we can see with PLS accounts that is not always the case.  The outcome of the game is determined by the rules, and in this case a PLS system does not have the same type of “losers” as a traditional zero-sum lottery.  If you do not win in a PLS system, you do not “lose” anything.  That is a fundamentally different type of game, but it’s still a type of “gambling”.', 'If anyone is interested in learning more about PoolTogether, please check out our recent panel on Governance Implementation 1, where founder Leighton Cusack spoke to Connor Spelliscy, Seth Frey, Scott Moore and @kelsienabben discussed community, incentives and DAOs, among other topics.', 'First of all, thank you for the clarification, particularly regarding the PLS vs. gambling framing.Perhaps it is still too soon to comment, but do we have any indication about the impact that gas fees might be having on the value of earnings? Larry_Bates:However, the gas fees associated with entering and exiting a pool may offset any earnings, but upcoming changes to the Ethereum protocol may affect that aspect of earnings in a way that would significantly improve net yield.I also wanted to come back to your question: Larry_Bates:Do prize-linked savings accounts cannibalize from regular savings accounts?This is an interesting question because it might be where DeFi can appeal to more people. As you pointed out above, there is some strong gamification happening here that is appealing to people. Maybe more importantly, there is some analysis that could be done to identify tipping points between seeking a standard yield versus an opportunity to win. So, coming back to your question, what would cannibalization data look like? Certainly there will be some movement in the market if PLSs are present, but how much adoption is necessary to determine if it is changing the expectations and/or behavior of people looking to save?', 'I had thought about these questions, and I believe a way to tell if gas fees were cannibalizing the savings would be to look at accounts that had exited and average the difference between the yields and the gas fees at time of exit.  I think the gas fees at time of exit are more relevant than the “least fee possible to exit the pool at the optimal time,” because those that exit may have had an external pressure that forced them to get out of the pool and in some cases be forced to take an excessive gas fee.Since we can’t look at the Pooltogether users’ bank accounts, one may be able to look at their other coin holdings to see if any of them had been transferred into Pooltogether.  It might be relevant to do a study to see how many users switched from trading on Dexes where the trades were visible to using Pooltogether.  There is some way to glean behavioral shifts if they do exist.  In that regard it would only be possible on accounts that had a history of trading prior to joining Pooltogether.  While it would take a considerable amount of work, I do believe it would be possible to determine if Pooltogether cannibalizes trading market account balances.  While the term “cannibalize” sounds bad, the overall outcome would likely be a net positive movement for the market if Pooltogether PLS accounts cannibalized speculative trading accounts; but potentially an overall net negative outcome if the gas fees subsequently cannibalized the PLS accounts.', 'I absolutely agree that there is a potential net positive here for the market if speculative trading gets displaced by PLS accounts. I suspect this would bring some stability to the market.Thanks for digging into these questions a little more. Based on my understanding now, it seems to me that PLS mechanisms might be something that projects that are seeking to attract stakers could or should be using. More analysis of those tipping points of account holding and likelihood to win prizes is needed, but it does seem like something that could be alluring for potential project investors. Maybe @tjd233 could bring some perspective to this as well.', 'Hi @Larry_Bates. Fascinating concept. I see the potential for cross-overs to many spheres other than just personal savings with concepts like this, as well.I’m curious whether there is talk of a “critical mass” of sorts by which systems like these would start to lose effectiveness. Would there be other forms of incentive systems that could be attached to this general concept?', 'This was a great article. Firstly, thank you for posting and beginning the discussion on PLS accounts.I see the real value here in the incentivization that @zube.paul mentioned, specifically as it relates to platforming the unbanked and motivating low-income individuals to save with a legitimate financial product that resembles the outcome of gambling, but is far from it. Effectively, moving those that might be subject to the predatory practices of gambling schemes into the banking sector. My interest lie in this exact capability of new DeFi and TradFi products that can open the banking and financial services sector to low-income for new forms of wealth creation. Few TradFi products now cater to the low-income user, and leave large portions of communities outside of the financial sector – which we know continues to grow the wealth gap, as low-income individuals turn to non-traditional and predatory methods of “finance” (i.e., payday lenders and check cashers) in the lack of credit unions and banks, and therefore appropriate financial products. Furthermore, the authors are right when they mention: “a growing body of evidence suggests that a large percentage of households in developed and developing countries alike maintain little to no savings.” Even more US households had significant portions or all of their savings wiped out during the pandemic 1. Which highlights the growing potential market size and need for products like this in the US.Beyond the technical limitations (gas fees) and need for maturity in DeFi, I’m interested in the potential for social production to be the key focus in a lot of these products. What can be built to solve the failings of a current system that exclude low-income or the unbanked from wealth creation? Can we expand DeFi to those individuals in an effort to remedy the wealth gap? These are the individuals that have the most to gain from a transformed financial system. DeFi products promoting the benefits to low-income and attracting the unbanked can also engage in an Environmental, Social & Governance (ESG) framework that is lacking from the crypto-sphere in general. Ultimately, ESG says ‘are you helping the planet, people, and yourselves’…if yes, then consumers can have another signal that this product is appropriate for them and others.Lastly, wanted to input here some other quotes concerning the low-income benefits mentioned in the paper, as I found it impressive that the authors were able to see some behavioral patterns among this demographic:“We find that the lowest-wealth group are nearly 17% points more likely to open a MaMa account than those with a small amount of savings. These individuals are those for whom a large financial prize is a significant incentive, even if the chances of winning are small, as it represents a chance to significantly change their economic situation.” (page 11)“Individuals who feel this way [that their debt is never ending] may be more likely to use PLS because it represents a chance for them to pay off their debts and escape a poverty trap, whereas standard savings products do not accumulate enough interest to do so (Banerjee and Mullainathan 2010). In addition, financial constraints themselves could lead individuals to play the lottery (Haisley et al. 2008, Shah et al. 2012).” (page 13)“This finding [that low-income individuals are attracted to these PLS & MaMa] is also related to evidence from the Consumer Federation of America and The Financial Planning Association (2006), which found that 21% of Americans and 38% of those with incomes below $25,000 thought that winning the lottery represents the most practical way for them to accumulate several hundred thousand dollars. Individuals who feel that their dreams are extremely difficult to reach may feel as if the only way possible for them even to have a chance at reaching those goals is by winning a large prize. PLS differs from standard savings accounts by offering highly skewed payouts, making large wealth accumulation possible.” (page 12)', '@valeriespina Thank you so kindly for these fantastic questions and observations.  I am going to take a couple days to think about these questions and do some further research so I can give a more informed response, so please be patient with me as I think about these wonderful questions!', 'I want to address as many of the aspects of the questions and comments posted as possible within a reasonable and useful amount of time.  I will address the Defi potential upsides and pitfalls, some previous findings of PLS accounts, and some insights about the infrastructure and social adoption aspects.  Further, there are some opportunities for implementation of PLS accounts that might be easier to trial in the decentralized/open-source space before attempting to trial them in a municipal setting.First, the theoretical applications of DeFi projects show a lot of potential in the context of growing capital through algorithmic exchanges.  There was a couple of analyses of the theoretical framework that were posted on the forum -Theoretical framework:Whitepaper: Uniswap v3 Core - Mechanism Design and Game Theory - Smart Contract Research ForumAn analysis of Uniswap markets - Mechanism Design and Game Theory - Smart Contract Research Forum 1While this data has not been peer-reviewed, a cursory study of accounts participating in DeFi trading would have been better off just letting their capital stay in place instead of attempting to use DeFi trading mechanisms -Real-world outcome:Rekt - Uniswap V3 LP - REKTimage1200×637 76 KBThis excerpt from the blog post explains that roughly half of the liquidity pools using Uniswap were losing money over time. -"" A recent study suggests that around 50% of UNI V3 LPs are losing money compared to if they just held their assets (and the fees they earn don’t make up for it) .When V3 was launched, Uniswap promised their LPs increased capital efficiency via the use of concentrated liquidity positions.But after 6 months of use, it appears that the increased complexity (and risk) have left half of LPs losing out under the new system.""Although their study was limited to 17 pools, it is a strong indicator that the Uniswap liquidity pools are not necessarily as viable for generating revenue as initially anticipated.That said, there was a significant amount of data that showed PLS accounts can reduce gambling and increase savings among lower-income populations that either gambled, had problems accruing savings, or a combination of both.  The long-term studies showed various benefits to PLS accounts, including one study which showed a 3% reduction in gambling among the sample population.Long Term Study:Long-Term-Effect-of-Temporary-of-Incentivs-to-Save_Gertler.et_.al_March2018.pdf (povertyactionlab.org)Making Savers Winners: An Overview of Prize-Linked Savings Products | NBERTesting strategies to increase saving in individual development account programs - ScienceDirectSupply Chain Transparency for cost is preferred by consumers when available:Lifting the Veil: The Benefits of Cost Transparency by Bhavya Mohan, Ryan W. Buell, Leslie K. John :: SSRNWhile there are proposed benefits of bringing more transparency to supply chains, there have yet to be any successful case studies.  To the contrary, most of the case-studies I have come across that tried to implement a blockchain into their supply chain did not actually see significant enough improvements to make a case that blockchain technology will actually bring transparency to supply chains:On the quest for supply chain transparency through Blockchain: Lessons learned from two serialized data projects - Rao - 2021 - Journal of Business Logistics - Wiley Online LibraryThe Struggle is Real: Insights from a Supply Chain Blockchain Case - Sternberg - 2021 - Journal of Business Logistics - Wiley Online LibraryThat is all to say; a realistic implementation of a PLS account that also includes blockchain technology could potentially be a quicker way to get unbanked populations into interest-bearing savings accounts that could also randomly see them getting an outsized prize.While it is not currently clear what type of structure might yield the most positive outcomes a few issues would need to be addressed to reduce risk:It would need to benefit the local supply chainIt would need to be cost-effective (ethereum gas-fees would be prohibitive as an example)It would need to be resistant to volatile market swingsOptimally, it would be resistant to inflationIf it were local, it should not undermine the local economyIf it is part of a national initiative, it should not undermine the national economy (I.E. building on an extranational network)It can’t have a high barrier of entry into the global market (I.E. localized currency exchange gouging)It can’t be at risk of being banned by local governmentIt can’t be at risk of being shunned by the citizens because of cultural/institutional associationsWhile this list is not exhaustive, it represents a starting point to show that there are factors that would make blockchain/DeFi PLS accounts more likely to be successful if implemented in a real-world situation.If there was a hypothetical DeFi/PLS account that gave users the option to “save” their money in a digital version of their local currency and/or “invest” their capital into a government-matched bond which subsidized a local supply chain operation, while also having a lossless prize associated with these accounts; that would be more beneficial than gambling or investing in a non-local company.  It is difficult to get these types of constructs when maximalists will push single blockchain solutions, but inevitably there will need to be many experiments to get to a PLS/DeFi account model that would work in multiple countries.I have linked an example of a proposal for a sustainable blockchain infrastructure that was put forth in Bangladesh, as something like this would be necessary to ensure that the net effect was not negative.engrXiv Preprints | LOCALIZED SUSTAINABLE AND ECOFRIENDLY ENERGY GENERATION AND DISTRIBUTION USING BLOCKCHAIN NETWORK: BANGLADESH PERSPECTIVE 1As long as all of the relevant aspects are included, down to the storage and power consumption implications; there is potential for these types of accounts to be made accessible to their targeted population.  The issue becomes, these tools will not likely be made accessible until either someone can profit off the users, or a government makes a major investment into a digital infrastructure shift.', 'A thought I had recently might be worthy of piggy-backing onto your discussion re: government investment into infrastructure transitions…Would there any potential for an overlap between PLS accounts and funding public projects through state-based lottery?Honolulu Civil Beat – 11 Feb 21State Lottery To Fund Public Education Moves Forward 1The bill would create a commission to plan for a lottery to help pay for the university and public schools.Est. reading time: 3 minutesThis is a limited example of what I’m referring to, but it seems that there could be a future where the concepts intersect.I feel like @tebogonong could weigh in on this, as well, considering her work on public procurement as it relates to smart contracts.', 'Thank you for this framing!  I did some digging into this subject to get a little more insight as to some of the potential implications.  On the one hand, there is a potential for a “lottery-funded public project” path to increase the funds that citizens would give towards a cause.  One study showed that people were more likely to buy lottery tickets knowing that the funds would go towards a public good than to just donate directly to the cause.  While this is not generalizable across all situations, there is a clear potential to raise funds for specific state projects using lotteries where voluntary contributions are hard to come by.On the other hand, there have not been clear correlations between state lotteries funding projects and an actual increase in the funding for those specific projects.  One study showed a negative correlation between state lotteries and public funding, indicating that the money going into the lottery was effectively causing the budgets for affected programs to shrink.  Another study showed a five percent increase in public spending on higher education directly correlating with the state lottery.In these examples, there is the notion put forth by the researchers that while the state lottery can have benefits for society; “using it as an implicit tax can be regressive”.  In other words, there is a potential that using this mechanism as a means of funding public goods can reinforce high-risk behaviors with the implication that the behavior does a greater good for society.  PLS accounts were analyzed from the perspective of their potential to curb gambling.If there was a combination of a PLS account that somehow gave prizes but also funded public projects, it might be the most net-positive implementation of the mechanisms that promote risk-reducing behaviors.  It would seem based on data that regularly participating in lotteries inherently increases the likelihood that a person will participate in high-risk activities such as gambling.  If a design was able to leverage the lossless reward aspect of the PLS account with the capacity to take some of the pooled money to generate funds for public good, then it might be possible to combine them.After preliminary research, it would seem relevant to ensure that the implementation was not reinforcing high-risk behaviors that might be carried over to other environments.  Further, there does seem to be potential for an implementation that employs the proper behavioral triggers to motivate people to get involved in contributing to public project funding without reinforcing high-risk behaviors.', 'Seems like there are some interesting parallels between PSAs and state-run lotteries / casino compacts, including weighing their public benefit vs, costs. Since the publication of this summary, Pooltogether has been in the news because a Warren staffer has sued the company under a law against lotteries – from the WSJ “Although Mr. Kent’s lawsuit, supported by two plaintiffs’ law firms, is nominally focused on winning a potentially large pot of financial damages, it also appears to be a deliberate effort to put some of the DeFi community’s core doctrines to the test. A former technology lead for Sen. Elizabeth Warren’s 2020 presidential campaign, Mr. Kent is described in his lawsuit as someone “gravely concerned” at the prospect that cryptocurrency, which consumes voluminous amounts of electricity, could contribute to climate change, besides enabling bad actors to circumvent financial sanctions.”', 'I sincerely wish they had a better representative for the argument against these practices.  The aforementioned case is frivolous at best, intentionally obtuse pandering at worst.  When you look at the details of that specific case, it is clear that Mr. Kent is using generic arguments that are not necessarily specific to DeFi platforms and are more of the general concerns associated with Proof of Work mining.  Further, the notion that these platforms enable bad actors any more than cash is disingenuous, as there is no legitimate proof these transactions are coming from nations which are under sanction.There are legitimate problems with the prize-linked savings defi platforms, but this type of frivolous action will only serve to make officials look uninformed and in the process, it becomes painfully clear their attempts to cast a negative light on this issue were only coming from a place of political gain and no actual attempt to improve the technology.']"
"                Research Summary: A Sustainable, Blockchain-Based Peer-to-Peer Energy Trading System              ",https://www.smartcontractresearch.org/t/research-summary-a-sustainable-blockchain-based-peer-to-peer-energy-trading-system/1179,Oracles and Data,29,['https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9358144'],"['summary', 'oracles', 'scalability', 'iot', 'defi']","['TLDRDecentralized, peer-to-peer energy trading is growing in popularity due to the development of distributed energy resource (DER) technologies.Achieving optimal energy expenditure within these systems is challenging.This paper proposes a blockchain-based energy platform to maximize the efficiency of distributed energy resources.The proposed energy platform consists of two parts: (1) a blockchain-based trading system and; (2) predictive analytics which inform the trading system using smart contracts and secure decentralized oracles like DECO 4.Using machine learning to predict energy consumption data, the paper simulates a system that predicts energy consumption a day ahead and schedules energy disbursement to meet grid demand.Core Research QuestionCan a secure, decentralized energy trading system be designed using blockchain technology, smart contracts, and decentralized oracle networks to maximize efficiency and energy output with day-ahead energy scheduling?CitationJamil, F., Iqbal, N., Ahmad, S., & Kim, D. (2021). Peer-to-Peer Energy Trading Mechanism Based on Blockchain and Machine Learning for Sustainable Electrical Power Supply in Smart Grid 7. IEEE Access, 9, 39193-39217.BackgroundOverviewRenewable energy resources present opportunities for peer-to-peer (P2P) energy trading between homes and buildings.These are made possible by smart grid innovations such as distributed energy resource (DER) technologies.These technologies allow energy distribution to be decentralized by allowing energy transfer between energy users.They also change the dynamics of energy distribution through reconfiguring the roles of utility companies and energy consumers.Utility companies in this system do not distribute the energy from a centralized source, but rather serve as (1) providers of transmission line infrastructure that the energy is transmitted through and; (2) coordinators of automated energy distribution throughout the grid.Utility companies can thus be thought of as crowdsourcing platforms for energy distribution.A highly secure system utilizing blockchain technology, smart contracts, and secure oracles like Chainlink’s DECO 1 are the kind of tools needed to manage the complex transactions required for such a system.The main challenges for such a system are scalability and security, both of which can be handled with hybrid smart contracts and decentralized oracles.Definitions and Terminology1600×246 35.9 KBTable 1: Blockchain characteristics.Permissioned Blockchain – a centrally controlled, privately managed blockchain. This contrasts with decentralized, permissionless blockchains such as Bitcoin and Ethereum. Table 1 above illustrates how Hyperledger Fabric, the blockchain the authors model their system on, differs from Bitcoin and Ethereum.Practical Byzantine Fault Tolerance (PBFT) Consensus Mechanism – while the permissionless blockchains listed in Table 1 rely on Proof of Work (PoW) consensus mechanisms, the authors argue that some of their faults, which include high energy consumption and scalability issues, suggest that another approach is needed in this environment.The authors recommend a Practical Byzantine Fault Tolerance (PBFT) 1 consensus mechanism to improve scalability and minimize energy consumption. PBFT consensus tolerates byzantine faults 1, or component failure, in networks that are prone to attack and instability.Smart Contracts – smart contracts are programs hosted on blockchains that trigger outcomes when certain conditions are met. A breakdown of smart contract examples, features, and use cases can be found on the Chanlink blog 1.Prosumers – prosumers are providers and consumers of energy within decentralized energy systems. In the energy trading mechanism discussed in this paper, all energy consumers are prosumers.SummaryThe energy trading system proposed in this paper requires a number of components in order to effectively distribute energy:Blockchain network: allows for transaction and network management of the system. Modeled on Hyperledger Fabric, a permissioned blockchain.Smart contracts and oracles: allows for day-ahead scheduling and controllable loads of distributed energy resources (DERs) along with the flexibility to deal with local needs.Data analytics and machine learning prediction: data mining and machine learning prediction systems are needed to automate decision-making and predict long- and short-term energy needs.Blockchain network1600×702 81.2 KBTable 2: Comparison of proposed decentralized energy trading platforms.There are several blockchain-based energy trading platforms that use permissioned and permissionless blockchains along with a variety of consensus mechanisms. These are summarized in Table 2 above.1600×923 234 KBFigure 1: Energy trading platform overview.Figure 1 provides an overview of the role of the blockchain network in facilitating energy trading. Each node in the blockchain network is a prosumer who receives energy externally and exchanges energy with other individuals in the network via energy trading transactions (ETT in the diagram).1600×1137 222 KBFigure 2: Blockchain-based energy trading platform workflow.Smart contracts and oraclesFigure 2 provides a more detailed breakdown of the role of the smart contracts and oracles in the energy trading systems. Oracles bring in off-chain, external energy data to smart contracts which are programmed to make predictions about energy consumption on a short- and long-term basis. These predictions are then used to schedule energy disbursement to meet current and next-day conditions.Data analytics and machine learning predictionFigure 2 also contains information about the role that machine learning plays in this scheme. Here we see that machine learning predictions, which use off-chain data provided by oracles, are fed into smart contracts that perform each of the essential functions mentioned above.998×764 194 KBFigure 3: Energy trading transaction types.Energy trading transactionsThere are two transaction types in this system: Type 1 and Type 2, both illustrated in Figure 3. Type 1 transactions are between the utility provider and households, and Type 2 are between households themselves. Both are facilitated by rewards given to the energy provider.MethodsThe authors are able to implement their system using Hyperledger Fabric and simulate the internal performance of their system by feeding hourly energy consumption data from Jeju, South Korea between 2002-2018 into a simulation of the framework discussed in the paper. Simulations using this data were conducted using a framework called Hyperledger Calliper.Blockchain component performanceInternal performance of the blockchain-based framework was measured by transaction rate and transaction latency to assess scalability as the number of transactions increased.Machine learning and data analytics performanceThe authors used a number of machine learning algorithms to predict energy consumption. Some of these methods include:Long Short-Term Memory (LSTM) neural networks that are used mostly for predicting time series data.Bi-Directional LSTM (Author’s Proposed Model) is a variant of the LSTM model.Recurrent Neural Networks (RNN) that are also used for predicting sequential data.Random Forests which construct decision trees to make predictions.XGBoost which are a variant of random forests with optimization boosting.Performance for these methods was measured as the MSE or mean squared prediction error between the actual and predicted energy consumption along with other variations of MSE such as mean absolute error (MAE), root mean square error (RMSE), and R2 which measures model fit.ResultsBlockchain component performance1600×1326 197 KBFigure 4: Transaction Latency and Throughput Analysis. Send rate is in transactions per second (TPS). Latency is in milliseconds (ms). Throughput is in TPS.Machine learning and data analytics performanceFigure 5: Comparison of proposed Bidirectional LSTM approaches with traditional deep learning approach for energy consumption prediction.876×252 22.7 KBFigure 6: Comparison of the proposed approach with other machine learning methods.Discussion and Key TakeawaysThe authors conduct a massive undertaking in this paper. They create a framework for efficient peer-to-peer energy transactions in a decentralized blockchain-based system, design that system using a permissioned blockchain, and simulate how that system would operate using real, hourly energy consumption data from South Korea. They then go on to demonstrate that the system performs well under simulated conditions in terms of its ability to handle transactions and to predict needed energy consumption accurately.For these reasons alone, the authors should be commended for the contributions they’ve made here toward the future of sustainable energy.That being said, any energy distribution system must also be judged by how secure it is and the extent to which it reduces pollution. Both of these features seem to be assumed in this paper without any additional proof. An absolutely essential part of such a system would require incorporating decentralized identity systems like CanDID 1 to handle users’ information and privacy-preserving, secure oracles like DECO 1 to help ensure seamless data transfer.Both of these features should be the focus of similar papers in the future.Implications and Follow-upsThe authors demonstrate that a blockchain-based peer-to-peer energy trading system is not only possible but also can be scalable and continuously make the accurate predictions needed to maintain such a system.Thus, they provide a strong foundation for future research in this area and give entrepreneurs as well as city, state, and local governments developing these technologies greater confidence in their ability to successfully transition from the existing, centralized electricity distribution system to a decentralized distributed energy resource (DER).', 'How would something like this intersect with the other system you recently described which seemed like it was accurately pricing in negative externalities into the environment?', 'Thats a really good question. I think if such as system were implemented on a large enough scale, it would be easy enough to funnel the data analytics from the decentralized grid into the carbon credit system to generate tradable credits for the utility company.Something along those lines, I think, would be readily achievable.', 'Would love to hear thoughts on this topic from @Larry_Bates, @eleventh and @valeriespina!', 'If someone can produce energy locally, they have the option to sell their energy back to the electric company.  I am not sure I understand how this system would work without creating an entirely new electrical grid that was not controlled by a single entity.  Peer-to-peer electricity trading works when there is a system in which the peers can sell the electricity to another peer, however, this requires a connected grid.From what I can tell, this approach adds another layer to the already implemented system of selling energy in attempt to give “prosumers” the capacity to sell their energy without going through the utility company.It just does not seem feasible to me that the utility companies would let individuals use their lines without some form of fee, which might then make this process more expensive to acquire energy than if a consumer just dealt directly with the utility company.Further, I can see this system creating problems in a situation where power had to be rationed and thus it creates a situation where the price of electricity could snowball out of control.On the one hand, I understand the impetus to put more power in the hands of prosumers (not to make a pun), but in a real application that is not just testing tracking transactions, it does not seem possible that this system could get electricity to consumers who are not prosumers at a cheaper rate than the utility company providing the power lines.  This is not the first attempt I’ve seen at this type of peer-to-peer electricity trade, and almost none of them ever talk about building infrastructure.I am concerned that this is unrealistic and will not scale, but I am not sure if I am understanding their approach correctly to know if that feeling is valid or not.  Do you have any insights into how they approach infrastructure and whether renting infrastructure is the only approach mentioned?', 'Fully agreed. It is a highly regulated environment in which few players are allowed to act. It’s also not clear to me if the short-term markets would include ancillary services, or if those need to be fulfilled by the existing actors.Another issue not discussed is the privacy in micro-grids of such prosumer networks.', ' Larry_Bates:I am not sure I understand how this system would work without creating an entirely new electrical grid that was not controlled by a single entity. Peer-to-peer electricity trading works when there is a system in which the peers can sell the electricity to another peer, however, this requires a connected grid.First off, I love this article. Thanks for posting @jasonanastas !To respond to Larry’s comment:  this is ~sort of~ a new grid. It’s the model of microgrids. That, yes, are currently not supported by utility companies/centralized authorities in every state, but can be installed today without the help or approval of an Independent System Operator (ISO)/Investor-Owner Utility(IOU). That doesn’t mean you’ll be able to transact with your neighbor but it’s a cart before the horse right now, and groups are trying to localize energy creation and become prosumers as the first step in decentralizing energy production from coal and oil. There is a model of this in Brooklyn 1, work going on in California 2, and Maine. It’s my opinion that the market will continue to see prosumers in Single-Family dwellings, and some form of community solar/renewables supported by ISOs for Multi-Family. I think only after the market is saturated with renewables that they’ll be this optimization (transacting). So I agree that it seems unrealistic now, or to think we’ll see it in 3 or 5 years, but I say give it a decade. The demand to get off of coal and oil is not only so high but necessary that I don’t think this is unrealistic, we’re just not anywhere close to optimizing current use of renewables. The cost to own and install solar on your home only recently became cost-effective (i.e., again, we’re just early).I know that utility companies typically do not like any form of distributed energy resources (DERs) because they do not make the utility money in most cases–they are profit oriented  . This has also limited the rollout of DERs, and that is why in places with innovations off-grid they’ve been pushed by grassroots efforts fighting for regulatory and policy change. This is a public policy issue more than, if not equal to, it is technical.I’m very confident we’re going to see some kind of P2P energy market place though. If we look at Australia, they’re almost a decade ahead of the US (and California), and have had plenty of lessons learned for less progressive energy markets to build off of. Check out Powerledger 1, Energy Web, and Grid Singularity 2. This is not an endorsement of the projects, but they’re doing interesting work.I’ve attached this image below. I recently created a deck outlining all the innovation going on in energy/power blockchain. Again, this says less about the technical capabilities and infrastructure needed to actualize these new models (P2P energy trading; microgrids; etc), but the demand and the innovation is there and the problems to solve aren’t technical, they’re political.image800×450 139 KB', 'This makes complete sense!  Thank you for this detailed and referenced response!', 'To add a little more detail to @valeriespina’s response:Like most other innovations, microgrids (more recently known as smart grids) require that various independent technologies become equivalently mature as well as technically aligned. This can be a long (sometimes decades-long) process during which it will seem for years that nothing is happening, and then, as if by magic, “the future is suddenly here.”Obviously, no magic is involved, just a lot of hard technical work.Note that the smart grid “does not replace the existing electric system but rather builds on the available infrastructure.” Smart grids are essentially subsets of larger electrical utility grids, the two being linked by a “point of common coupling” that maintains voltage at a constant level until there is a problem on the utility’s grid.If the utility-provided power starts to fluctuate, the smart grid evens out the flow by bringing in locally produced energy. But smart grids can also operate in island-mode, so if the central utility goes down altogether the smart grid disconnects and uses its own local generating and/or storage capacity. To make this “magic” happen requires sensors sampling the power-flow many times per second, and specialized software to manage selling as well as buying electrical energy from both utilities and peers.And, as @valeriespina rightly says, a lot of politics is involved at every level. Which of those two things—technology development or politics—slows down the arrival of the future more is a matter of debate.', 'My comment is not really related to the topic of P2P smart grids, and I don’t want to sound picky but it seems to me that this table comparing blockchains is wrong: jasonanastas:Definitions and TerminologyTable 1: Blockchain characteristics.1- For the Smart Contract column: different languages are possible for Hyperledger Fabric and Ethereum so it would be better to describe the compiler (or list the different languages). We could also discuss the presence of smart contracts on Bitcoin but it is ‘touchy’.2- For the Transactions column, transactions are not anonymous on Bitcoin nor on Hyperledger Fabric in which, on the contrary, nodes are identified (although they can be masked in some way using their Identity Mixer concept). They are both pseudonymous. And I don’t see why Ethereum is different with its public/private…3- Hyperledger Fabric’s consensus algorithm is not PBFT but has been Kafka and then Raft for the latest versions (illustration of RAFT here: Raft). These algorithms are not BFT (Byzantine Fault Tolerant) but only CFT (Crash Fault Tolerant) because they use a leader/follower system, i.e. they are not tolerant to dishonest nodes but only to a network failure.4- For the Programming language column, Hyperledger Fabric is coded in Golang (Java and NodeJS are only supported for smart contracts). Bitcoin and Ethereum have several implementations, the most widespread being C++ for Bitcoin (Bitcoin core) and Golang for Ethereum (Geth). I think there was a confusion between the language supported for smart contracts and the client implementation language.Moreover, I don’t really agree with this sentence (page 3 of the article) that is used when describing the distinction between permissioned and permissionless blockchains:“Therefore the permissionless blockchain is less transparent, less anonymous, and less secure as it depends on the participants’ integrity. Likewise,the permissioned blockchain is more secure, high customizability, better scalability, and enhanced access control mechanism”I know it doesn’t change much to the topic and the smart grid model described by the authors but I think it was still important to mention it for the forum.', '@Sami_B Some great points here Sami. I would seriously suggest contacting the authors and mentioning some of these points. You would be doing them and those reading the paper a public service!']"
                Discussion Post: Can a Carbon Credit Ecosystem Powered by Smart Contracts Reduce Greenhouse Gas Emissions?              ,https://www.smartcontractresearch.org/t/discussion-post-can-a-carbon-credit-ecosystem-powered-by-smart-contracts-reduce-greenhouse-gas-emissions/1128,Oracles and Data,27,[],"['summary', 'scalability', 'oracles', 'iot', 'defi']","['CTA: ""In these threads, we attempt to further the discussion of a key problem in this category—particularly where research work has not yet answered the specific problem or question being considered.”Citation: Saraji, Soheil, and Mike Borowczak. “A blockchain-based carbon credit ecosystem 7.” arXiv preprint arXiv:2107.00185 (2021).BackgroundWhat are carbon credits?“Carbon credits” were proposed as a solution to excess greenhouse gas emissions in the international Kyoto Protocol agreement of 1997. In this system, carbon emissions are treated as commodities and traded in a carbon credit market. Those who have more credits are allowed to emit more greenhouse gases than those with fewer carbon credits.Carbon credit systems have had varying levels of success and have faced many challenges. A number of people claim that blockchain-based carbon credit ecosystems, powered by IoT and decentralized oracles like Chainlink, can solve many of these challenges.In this post, I describe a proposal for a blockchain-based carbon credit ecosystem 7 put forth in a 2021 whitepaper by computer scientists Soheil Saraji and Mike Borowczak at the University of Wyoming and ask whether this can solve many of the challenges carbon credit systems have faced.I also discuss some of the potential impediments to the success of a carbon credit ecosystem powered by blockchain, oracles, and smart contracts 1.Carbon credit challengesWhile carbon credits can potentially reduce greenhouse gas emissions, they face several challenges including:Uneven implementation – Different jurisdictions and countries have implemented different types of carbon credit systems, leading to a lack of standardization that prevents the widespread adoption of carbon credits.Over-crediting and excess spending – Also related to lack of standardization. Different rules across carbon credit markets and lack of transparency regarding credit life and exchange value lead to over-crediting and double-spending.Transaction costs – Brokers and agents are required to facilitate carbon credit transactions, resulting in very high transaction costs.Distorted incentives – Third-party verifiers are paid by the very people they investigate, creating incentives to approve projects consistently.These challenges, according to Saraji and Borowczak (2021) have resulted in a failure to mitigate rising levels of greenhouse gas emissions.Smart contract solutions?The authors propose a blockchain-based carbon credit ecosystem built on smart contracts and decentralized oracle networks. This ecosystem can solve each of the extant problems through the creation of a decentralized carbon credit ecosystem. In this system:Uneven implementation is solved by a decentralized autonomous organization (DAO) whose rules all participants agree to.Over-crediting and excess spending is solved through fast and secure data sharing enabled by decentralized oracle networks (DONs) 1.Transaction costs from brokers and agents are eliminated through trustless, cryptographic truth via smart contracts.Distorted incentives can be eliminated through independent, third-party validation, potentially with the help of IoT sensors 1.Carbon credit ecosystem architectureVision: The creation of an ecosystem/DAO in which carbon credits are tokenized with transparent minting and burning protocols as well as trading rules.Plan:Credit stakeholders – eco-friendly green “generators” of carbon credits such as wind farms, tree-planting operations, etc.Credit consumers – greenhouse gas-generating “consumers” of carbon credits such as power plants, manufacturing plants, and so on.Validators – specialized and accredited consultants.Minting – minting of credit tokens on the blockchain occurs after validation of projects.Exchange – credit exchange will occur on a special decentralized exchange platform. Token price is determined by supply and demand.Carbon removal NFT Certificate – tokens are retired through burning by sending them to a smart contract, and are then converted to a carbon removal NFT certificate.Smart Contracts:  The carbon credit ecosystem requires a minimum of 4 smart contracts interacting with 3 stakeholders and liquidity providers. Each of these smart contracts requires decentralized oracle networks to bring external data to the credit ecosystem.Smart contract 1 – registry stores information about stakeholders, consumers and validators.Smart contract 2 – used to mint tokens.Smart contract 3 – used for verification of minting and burning tokens.Smart contract 4 – automated market maker (AMM) smart contract that allows for trading with digital currency (CBDCs etc), gives liquidity providers incentives, provides dynamic price discovery.1600×811 112 KBFigure 1: Smart contract architecture of the carbon credit ecosystem.Key Problem or Topic AreaCurrent carbon credit systems are broken. Can we reduce greenhouse gas emissions through a more effective, blockchain-based carbon credit ecosystem?Specific Question or Problem StatementWhat are some of the positives and negatives of the authors’ proposed solution and what would be the biggest impediment to implementing this solution?Approach or MethodologyThe authors give us a broad framework for a blockchain-based carbon credit ecosystem. They do a great job pointing to some of the current failures of carbon credit systems and outline how each of those failures could potentially be solved with a blockchain-based carbon credit ecosystem.One of the most significant challenges that such a system will face is convincing those with currently existing carbon credit systems to agree to a single, decentralized blockchain-based system.This is where established providers of oracles and hybrid smart contracts like Chainlink can provide reassurance to participants that their transactions will be handled in a secure environment based on cryptographic truth 1.While great technology will solve many of the technical adoption hurdles, politically-based hurdles related to nation- or region-specific financial, electoral or security interests remain in play and will likely remain contentious—at least for some participants.Indeed, achieving global adoption of a standardized blockchain-based carbon credit ecosystem may require international coordination via global political institutions such as the United Nations (UN) and the International Monetary Fund (IMF).Conclusions or Key TakeawaysReducing greenhouse gas emissions using carbon credits requires a standardized ecosystem that is transparent, minimizes transaction costs, and retains a high level of integrity and honesty among participants. The authors provide a good framework for a blockchain-based system that can hit all of these points given the current state of smart contract and oracle technology.However, their carbon-credit ecosystem can only be effective if it is adopted, which also requires overcoming technological and political hurdles. While current technological hurdles are solvable, it would be useful, for future study, to consider the political hurdles that would prevent adoption of a blockchain-based carbon credit ecosystem like the one proposed.', 'Excellent post! @jasonanastas. This framework of the smart contract mechanism enables policy-makers and engineers to influence controlling CO2 emission further. However, the challenge is how to integrate this framework with the existing policy based on shared interests. I look forward to the development of this idea on SCRF and other platforms!', ' jasonanastas:politically-based hurdles related to nation- or region-specific financial, electoral or security interests remain in play and will likely remain contentious—at least for some participants.How successful have earlier carbon markets been? Is there any information about the volume of business transacted through it. I’m also curious how this kind of system might price in new technologies like carbon dioxide capture. Would the DAO pay for offsets?', 'Thanks Mark. Yes I agree and I think this is the biggest challenge/impediment that a system like the one that the authors proposed will face. The technology is there, but if the political will is not, this is something that has to be addressed.', 'Great questions! I’ll start with the second since the authors address this more directly:I’m also curious how this kind of system might price in new technologies like carbon dioxide capture. Would the DAO pay for offsets?As I understand it, initial pricing for a technology that generates carbon credits, like CO2 offsets, would be determined by the DAO community in some manner (perhaps voting?) and be subject to a veto by the “Validators” who are more specialized and accredited experts in environmental science.How successful have earlier carbon markets been? Is there any information about the volume of business transacted through it.I need to get back to you on this because there are ALOT of emprical studies on this topic! Stay tuned.', 'I absolutely love this post @jasonanastas!I am currently working with a small nonprofit to turn their carbon credits into NFTs. Though this is a small scale project that is utilizing the collectible quality of NFTs, I believe the market for carbon credits will be hugely impacted by the tokenization of carbon. Creating what could be a very efficient ecosystem of applications and companies that embed these tokens into their products or business models (because it’s already digitized and accessible!). I would like to add to the discussion a bit of perspective on the carbon market from the nonprofit lens.Most of the carbon token projects I have seen (KlimaDAO, Toucan Protocol/Base Carbon Tons, Moss) are using credits that sequester carbon from the atmosphere rather than generate carbon savings (i.e., ‘we did something that didn’t put carbon into the atmosphere and saved potential X amount of carbon from being produced’). The latter type is less valuable in the current market (around less than $5 USD per ton). The market is also heavily dependent on brokers, which has typically been an inefficient third-party intermediary. Brokers are often looking for a substantial number of tons generated, leaving smaller credit producing organizations on the sidelines.From a nonprofit perspective, carbon credits hold a lot of potential for cash-stricken organizations to generate additional revenue from the activities they are already doing to help accelerate the green energy transition and impact climate efforts. From my experience, nonprofits are not producing enough carbon credits to even sell to brokers. Additionally, nonprofits in the green energy space are often supporting low-income populations and those on the margins that will feel the effects of climate change at a much more substantial rate than those that are affluent. This adds a uniqueness value to the generation of the credit, because the sale will fund specific efforts in fighting poverty, multifamily affordable housing, community solar, or other niche areas of social and sustainable impact. Today, the carbon credit market is both inefficient and unable to account for the uniqueness value of how the carbon ton was generated. Therefore, the option of carbon credit NFTs became promising for niche orgs in the green energy space to tap into an asset that the current market didn’t support. NFTs in particular, can allow a P2P marketplace (overriding brokers) and engage a user to directly support a mission alongside achieving offsets.This is a new avenue for carbon credits and, though the success of a NFT carbon credit will be dependent on plenty of other factors, the blockchain is enabling small organizations to simply participate in new markets.Lastly, I wanted to answer your discussion question… jasonanastas:Current carbon credit systems are broken. Can we reduce greenhouse gas emissions through a more effective, blockchain-based carbon credit ecosystem?…I would say the system is vastly prime for innovation and efficient transfer of value. I do believe a more efficient market would incentivize sequestering efforts–which we drastically need to reduce greenhouse gas emissions. The blockchain makes the most sense to me in markets where provenance is the value, and where the efficient transaction of data could produce new markets…I think this is exactly the carbon credit space. Hope this sums my thoughts up!', ' jasonanastas:Carbon removal NFT Certificate – tokens are retired through burning by sending them to a smart contract, and are then converted to a carbon removal NFT certificate.How will the oracles check who is emitting as much carbon as they are allowed? isn’t that an issue with the current system? valeriespina:From my experience, nonprofits are not producing enough carbon credits to even sell to brokers.How hard is it for non profit to go find those with a lot of carbon credits that want to sell them? Why would those sellers join a blockchain base system too?', 'Hi! Great question. Might need you to rephrase it a bit, but answering to the best of my ability. jyezie:How hard is it for non profit to go find those with a lot of carbon credits that want to sell them?The context that I was referring to about the nonprofit was in relation to not producing enough credits to sell. Serious buyers are looking for both volume and quality. If there was a nonprofit looking to buy carbon credits, there are marketplaces, brokers, and individual sellers. It just depends how serious you are to buy and if you’re in the category of a pollutant emitting business. Some carbon credit brokers won’t even sell to organizations that have not exhausted all efforts to stop polluting in other ways. jyezie:Why would those sellers join a blockchain base system too?Sellers might join a blockchain system for a few reasons:To track the provenance of the credit through a secondary marketTo tokenize the credit and participate in protocol activity (like KlimaDAO or BCT – I endorse none of these projects and link them here as examples)To engage a nontraditional peer-to-peer marketplace that cuts out the middle-men of the traditional marketplaceTo tap into a growing market of tokenized carbon that has effects on the primary marketEnhance a reputation for innovationEnhance current marketing efforts and generate attentionI would summarize this by saying the reasons are less technical and more marketing or return-based (returns are variable). Though, you can’t get a higher price for the credit in the blockchain systems (that I know of). There are very specialized NFT projects backed by carbon credits that see higher prices, but they have put a premium on artwork and function more like a fundraiser for the project behind it and the dual impact that you’re creating by buying the NFT (See Carbon Creatures).Hope that answers your question. If it didn’t, please ask again!', ' jasonanastas:While great technology will solve many of the technical adoption hurdles, politically-based hurdles related to nation- or region-specific financial, electoral or security interests remain in play and will likely remain contentious—at least for some participants.Indeed, achieving global adoption of a standardized blockchain-based carbon credit ecosystem may require international coordination via global political institutions such as the United Nations (UN) and the International Monetary Fund (IMF).Not meaning to dismiss out of hand a nice technical idea (and aren’t they all nice?), but have you heard of Project Haystack 2? Probably not, so I’ll tell you quickly that it’s an open source effort to standardize the semantic model for the IoT and the built environment. Haystack has entire libraries of tags, each one precisely defined to ensure common understanding and interoperability.I mention it because I once interviewed one of the founders of the project who told me that if every manufacturer agreed to tag their objects (e.g., a chiller on the roof of a building) with Haystack’s tags, the whole smart-building space would be vastly farther along than it is today. But after 10 years, many major manufacturers still refused to use a standard that made referencing their objects and data easy and foolproof. And those that did use Haystack’s tags went out of their way to make it non-obvious that they were doing so.Leveraging “cryptographic truth” would indeed make the world vastly more transparent and optimized. But we need to recognize that people in power need to want the truth. And frankly, they don’t. Those people got their power (and money) in large part by hiding the “externalities” of their behavior, which “the truth” would expose. They’re not going to give up their many advantages (like vendor-locking their customers) simply because being more transparent would be “good for society.”To @jasonanastas’s point, which I quoted above: Looking at the track record of the UN and the IMF over the last 3 or 4 decades doesn’t instill much confidence. Aren’t those the people who gave us the Greek austerity debacle?Am I saying there’s no hope for our poor planet’s climate crisis? Not exactly (though it’s getting awfully late). But our efforts can’t be confined to installing blockchains and fancy IoT sensors, and parsing the resulting clouds of data. The cigarette industry had tons of data too, and you see how long that farce went on. I think we have to start much lower down on the human chain, where the “fix gets put in” in the first place.', 'thanks for your answers.That’s what I was wondering. If non profit were too small to have meaningful volume, it made sense for them to try to seek bigger producers. But you laid out why this his hard in practice. It is a really great insights from your experience to understand how hard it would be to apply such protocol in practice.', 'Thanks @rlombreglia some great info in your reply, especially regarding Project Haystack.I also agree with many of your points. Personally, I would like to see more causal evidence on the effectiveness of carbon credits to reduce pollution more generally and more work describing how carbon offset systems can effectively change the behavior of polluters.']"
                Research Summary: Governing Decentralized Complex Queries Through a DAO              ,https://www.smartcontractresearch.org/t/research-summary-governing-decentralized-complex-queries-through-a-dao/1127,Governance and Coordination,27,[],"['summary', 'discussion', 'governance']","['TLDRThis paper focuses on improving the efficiency of peer-to-peer file transport protocols. It proposes a Distributed Hash Table (DHT) system structured as a hypercube, a type of network topology that efficiently manages decentralized keyword-based queries executed on data stored in DFS.The authors provide a framework to govern the above network, based on implementing a Decentralized Autonomous Organization (DAO) and smart contracts. This enables organizational decision-making and rewards nodes that have actively contributed to the DHT.Core Research QuestionHow can we search data stored on novel decentralized storage technologies, such as DLT, DFS, IPFS, by content rather than identifier or location?How do node operators, who positively host and share information to manage rewards and organizational decisions with smart contracts, work in a more scalable and decentralized organization framework?CitationZichichi, M., Serena, L., Ferretti, S., & D’Angelo, G. (2021). Governing Decentralized Complex Queries Through a DAO. arXiv preprint arXiv:2107.06790.Governing Decentralized Complex Queries Through a DAO 3BackgroundDistributed Ledger Technologies (DLTs) 3: A peer-to-peer (P2P) system consisting of nodes which maintain information with a consensus mechanism, allowing them to each have the same version of that information. DLT ensures that information stored on it features integrity, immutability, and authenticity.Distributed Hash Table (DHT) 3: a decentralized system for distributed storage, which distributes information by providing a routing mechanism to move resources between system nodes. Its notion is to distribute storage workload among the DHT nodes in a way that allows for fast identification of where the key/value pair resides. DHT nodes are obtained through the use of a hash function, a one-way function that maps any item into a binary sequence of 𝑛 bits. Its key is an 𝑛 bit string obtained after having applied the hash function. Specifically, each DHT is identified itself by an 𝑛 bit ID, which exists in the same ID space used to identify contents. Each node maintains information on corresponding contents that are in a specific ID space interval. DHTs were proposed by “Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web”.Hypercube StructureThis paper proposes a DHT system that is structured as a hypercube. The hypercube is a logical layout where there are 2𝑟 nodes, each one labeled with an 𝑟 bits ID and connected to the 𝑟 nodes whose ID differs by only one bit. Each node is responsible for a specific keywords set, derived from their ID.Decentralized Autonomous Organization (DAO): An organization that “(1) participants maintain direct real-time control of contributed funds and (2) governance rules are formalized, automated and enforced using software”. However, its definition may vary as time goes on, as relevant industrial development may amplify or reconstruct its concept. DAOs aim to be governed by democratic or highly participatory processes or algorithms, the form of DAOs is community-driven using software and following the rule of code.Inter Planetary File System (IPFS): Stores and shares files in a peer-to-peer network. It’s one of the most widely used DFS protocols where files and data are replicated globally on hundreds of nodes in the network. It combines a DHT, an incentivized block exchange, and a self-certifying namespace.IPFS users can only search files stored in the system when owning them or knowing their CID because IPFS files are stored and shared in the form of IPFS objects that are identified by a CID (Content IDentifier), which comes from the outcome of applying that file to a hash function.Content Identifiers (CIDs): Are labels used to point to material in IPFS as objects. CIDs are products of a hash function when applied to a data and used to retrieve the referenced IPFS object.IPFS search engine: A search engine for the IFPS used to overcome the limitation above. It’s used to sniff the DHT gossip, index files, and directory hashes. However, this paper argues that it is too centralized.A decentralized solution called Siva had been proposed that would allow users to search through an inverted index of keywords. However, it does not feature any optimization for a keyword storage structure apart from the use of caching.The Graph network is a “Decentralized Query Protocol”, a system built on the Ethereum blockchain and IPFS, which allows users to search data stored in these two technologies. The organization of the network is like a DAO but has different storage indexes, a method this paper proposes.Decentralized finance (DeFi): Refers to smart-contract-based novel P2P financial infrastructures, that are non-custodial, permissionless, openly verifiable and composable. Anyone can conduct a non-custodial exchange of on-chain digital assets with DeFi protocols. Most DeFi protocol liquidity is provided algorithmically through a simple pricing rule within a smart contract, instead of the bid- and-ask-order prices which traditional finance’s asset liquidity is based on. .On Uniswap, when a new ERC20 token such as the DAO Token is created and initially distributed to its creators, users can provide liquidity by locking up their newly created tokens in a liquidity pool. As an incentive they receive Liquidity Pool (LP) tokens representing their stake. The creators may redeem them at any time by burning the LP tokens, which makes the value of the DAO token highly unstable.The Factory pattern: is an Object-oriented programming design pattern that implements the Factory notion.The EIP-1167 Minimal Proxy pattern: aims to provide a simple and cheap clone contract functionality in an immutable way, by specifying a minimal bytecode implementation that delegates all calls to a known, fixed address.SummaryIn a DHT structured as a hypercube topology, let 𝐾 ⊆ 𝑊 represent a keyword set in a keyword space 𝑊. Such set 𝐾 can be used to search for data contents characterized by keywords contained in 𝐾. Specifically, let 𝑂 be a set of data objects referenced in the DHT which are distributed on all the network nodes based on the associated keywords. The node responsible for such a keyword set 𝐾𝑜 maintains all the objects 𝑜 ∈ 𝑂 mapped to a keywords set 𝐾𝑜 ⊆ 𝑊. Then we deem 𝑂 as the set of all the CIDs in IPFS and leverage the DHT to map keywords to IPFS Objects.This paper proposes a system that multiple keywords are stored in IPFS which is structured as a 𝑟-dimensional hypercube 𝐻𝑟 (𝑉 , 𝐸) DHT, where 𝑉 is a set of vertices representing logical network nodes and 𝐸 is a set of edges that connect pairs of neighbor nodes. The node’s ID comes from an 𝑟-bit string associated with the keyword set that the node is responsible for. Each bit position represents a specific keyword. A given keyword set 𝐾 contains a given keyword 𝑘, which is assigned to the 𝑖-th bit of the string by a function. Then, keywords are given in input to an uniform hash function ℎ : 𝑊 → {0, 1, . . . , 𝑟 − 1} that returns positions to set to 1 in 𝑟-bit string, i.e. one(𝑢) = {ℎ(𝑘) | 𝑘 ∈ 𝐾}.Every node will locally store an index table that stores CIDs of all the IPFS Objects associated with the keyword set that the node is responsible for. The associated 𝑟-bit string for a query for a keywords set 𝐾 is used to reach the responsible logical node through a routing mechanism based on the hypercube form.There are two query types: 1) Pin Search, getting all and only the objects that are exactly associated with the keywords set 𝐾, {𝑜 ∈ 𝑂 | 𝐾𝑜 = 𝐾}, that is data objects are retrieved only from one node; 2) Superset Search, getting objects that can be described by keyword sets that include 𝐾, i.e., {𝑜 ∈ 𝑂 | 𝐾𝑜 ⊇ 𝐾}, data objects are retrieved from all nodes that are responsible for a superset of 𝐾. Then, since its possible outcomes can be quite large, a limit 𝑙 on the results is set, that is, search until the number of objects is equal to 𝑙 or no more nodes shall be contacted.The DHT network nodes operators can leverage Ethereum smart contracts to form a DAO characterized following :Token economy - The DAO is built around the use of a unique token that is used for transferring value or for staking purposes. The ERC20 interface presents the functions.Members Registry - Use smart contracts to develop a members registry. It allows token holders to time-lock DAO Tokens and become DAO members.General Voting - Use smart contracts to develop a mechanism that allows all DAO members to make a proposal, submit a suggestion vote, and then decide on a proposal. Each proposal has its own debate period. Any member can vote within that time period. Every member’s vote weight is proportional to the number of tokens locked until a date after the debate period ends.Value Transfer Voting - Using another smart contract, a DAO can develop an extension of the previous voting smart contract to allow a decision that directly executes on-chain.This paper envisions three use cases of a DAO. These use cases are possibly overlapping:DAO use case 1): To prevent instability caused by DAO tokens creators redeeming LP tokens, this paper envisions a use case where the DAO is based on the timelock of the LP tokens obtained by locking DAO tokens in liquidity pools, instead of based on the timelock of the DAO tokens directly. This allows the stability of the DAO to be directly proportional to the value that the DAO tokens can take, and the power of the DAO’s members is directly proportional to the gains/losses they want to make through their behavior.DAO use case 2): With the purpose of assisting “browsable sister networks to the internet”, this paper envisions a use case where different networks implement their own DHT, generating a large number of “islands” where keywords-based queries are possible. They could have different rules but all share a similar querying protocol. They could be built on several DLTs or only one DLT.DAO use case 3): This paper envisions a use case that combines IPFS-search with keyword-based hypercube DHT where “several IPFS nodes crawl the network by monitoring the IPFS logs for new files; these nodes download the files added through the ‘sniffed’ CID; for each file the metadata are extracted and transformed into keywords; the association between the keywords obtained and the CID is stored in the hypercube DHT.”This paper provides experimental validation of implementation results showing how the size of the hypercube and the number of objects stored in the DHT affect the search procedures.MethodBy mathematical proof, this paper proposes an optimized routing that is structured as a hypercube DHT for DLT stored data to be searched by its content.Multiple keywords search: The hypercube geometric form has been leveraged to organize the topological structure of a DHT in this paper.By scientific experiment, this paper proposes a test statistic of the efficiency of a DHT software implemented in Python which manages keyword-based queries for contents stored in IPFS, through the use of a hypercube DHT.ResultsThis paper proposes a five layered (bottom-up) structure. Layers 1 to 4 provide a method for keyword-based search that can be adapted to any type of DFS or DLT for data storage: “(1) First layer: the nodes of the IPFS public network running the standard IPFS protocol. (2) Second layer: all the files that the IPFS nodes keep in their storage. These are indexed using CIDs. (3) Third layer: the files can be described by keywords, which are then used to execute queries and find the files. (4) Fourth layer: these keywords are saved, together with the file association via the CID, using the Hypercube DHT.” The 5th layer is DAO, the governance of the hypercube DHT network.This paper develops smart contracts that implement the proposed framework in Solidity and stores it as Open Source code in Zenodo. Its cost execution in terms of gas for the main operations shows in Table 1. The most expensive operation in the proposed framework is the lockTokens() function. This is because, according to the OpenZeppelin library, each lock request creates a new smart contract that locks tokens for a unique account. This paper uses the EIP-1167 Minimal Proxy pattern to avoid the enormous amount of gas (at least 501818 gas units) the Factory pattern requires.The researchers ran two types of tests, i.e. the Pin Search and the Superset Search, with the network configuration for 8, 16, 32, 64, and 128 nodes and populated the network each time with 10, 100, and 1000 objects generated randomly. Then they repeated 50 random queries for Pin Search and 50 for Superset Search with objects limit set to 10, with a random node and a random keyword set every time. The result shows respectively in Figure 2.766×353 87.9 KBPin SearchThe test result shows that when the number of objects varies, there will be similar average hops, and when increasing the number of nodes, there will be an increase from 1.28 to 3.52The result corresponds to the researchers’ expectation because “the Pin Search average number of hops should theoretically be with the order of the logarithm of the number of logical nodes, i.e. log(𝑛)/2 or 𝑟/2.”Superset SearchThe test result shows that when the number of objects increases, the average number of hops decreases, and it increases when the number of nodes increases.The minimum value is 1.36 for 1000 objects and 8 nodes.The maximum is 20.36 for 10 objects and 128 nodes.“Theoretically, the average number of hops should be equal to the average hops required to get to the node responsible for query keywords set 𝐾, i.e. Pin Search log(𝑛)/2, plus the average hops to get from that node to all the nodes that include 𝐾, until the limit of objects 𝑙 (or nodes including 𝐾) is reached.”Discussion & Key TakeawaysThe four-layered keyword-based query system this paper proposes can apply to any DLT system and improve its efficiency: “(1) First layer: the nodes of the IPFS public network running the standard IPFS protocol. (2) Second layer: all the files that the IPFS nodes keep in their storage. These are indexed using CIDs. (3) Third layer: the files can be described by keywords, which are then used to execute queries and find the files. (4) Fourth layer: these keywords are saved, together with the file association via the CID, using the Hypercube DHT.”A fifth layer can also add to the above-mentioned four-layer system to enact a DAO, which leverages smart contracts to build functions of token economy, members’ registry, general voting, and value transfer voting.Implications & Follow-upsThe researchers will focus on two future works: 1) Research on the feasibility of a “pay-per-query” model, i.e. the node operators will be rewarded by query granularity; 2) The load balancing issues.ApplicabilityThe hypercube structure routing framework is applicable to any DFS system, such as IFPS, to build a decentralized keyword-based query data storage system.The DAO mechanism which is developed by smart contracts can add to the above hypercube structure routing framework, for node operators who are interested in keeping the network operational and healthy, and, in general, where node operators act in the context of a sharing economy, e.g. Wikipedia editors, to orchestrate their operational decisions and rewards.', '@Astrid_CH I’d like to ask a question publicly that you might recognize from one of our conversations on DM, but I thought it might be interesting for others to hear your reply. What was it about this research that you found most compelling?', 'Thanks for the helpful summary of this paper. After reading your summary and skimming the paper, I’m left wondering what advantage a hypercube representation offers? Maybe the hypercube has some search efficiencies or compactness?We leverage such a protocol and make use of these 𝑟 -bit strings to identify logical nodes in our system, e.g. for 𝑟 = 4, a node that has ID 0100 handles all those the objects 𝑜 ∈ 𝑂 associated to keywords sets whose the one function returns 0100. In the DHT, connections between nodes, i.e. edges, are created among those nodes whose IDs differ of only one bit, e.g. 1011 and 1010But does this do any better than graph or tree transversal? What about ALL the other requirements, like security?Worse still, it’s not at all clear what the DAO is supposed to be doing here. They seem to claim some innovation that there is a DAO for governance, but this is clearly just a bolt on and the purpose of this specific DAO is completely undeveloped. (there’s a deep literature on governance that the authors seem unaware of).Maybe I just don’t get it.', 'I got the same impression, as I initially assumed the hypercube was being introduced as some form of reference to lattice-based cryptography, but then realized it seemed to be trying to recreate the layer structure of the Internet into a hypercube.  This structure seems to be explained as a way to increase the number of edges while trying to optimize the number of hops between edges within the network; while combining the technologies into a novel arrangement.On the one hand, I can see the desire for the experimentation, but on the other hand I’m not sure the system could be deployed without qualitative research and research concerning the feasibility of this type of incentive mechanism.  Beyond the pay-per-query model, as @quinndupont pointed out I’m not certain that the increase in complexity has a correlating increase in security fitness.I am just trying to glean if they are building oracles and trying to make them seem more futuristic by arbitrarily giving it the hypercube structure in an attempt to reduce the number of hops between queries, while also attempting to add monetized incentive mechanisms within those streamlined steps.  There seem to be conflicting interests in the design that don’t seem to necessarily get reconciled, but that might be something that is more apparent in the paper?', '@jmcgirk @quinndupont @Larry_Bates Thanks for your insightful comments and questions.I think the most compelling part I found in this paper is that the authors tried to construct a 𝑟-dimensional hypercube structure to implement complex queries and storage. The authors mentioned in the paper, “The hypercube structure allows to optimize the routing of the queries, by reducing the number of hops needed to locate contents.” When the dimension could be extended, the routing of the queries may be optimized to a higher level. The authors also conducted an experiment that applied this structure to the data marketplace (see Towards Decentralized Complex Queries over Distributed Ledgers: a Data Marketplace Use-case). But maybe this theory needs more experiments and use cases to prove its rightness. Also, I didn’t see they take security issues into the paper’s consideration/discussion.Second, the idea to govern these nodes by DAO is impressive as well. As an incentive mechanism, nodes earn their DAO tokens according to the level of granularity of the query. To resolve the interest conflicts problem in DeFi fields (but not in this project’s scenario), the authors raised a possible use case where the DAO is based on the timelock of the LP tokens obtained by locking DAO tokens in liquidity pools, rather than the timelock of the DAO token directly.  For this project’s scenario, the authors proposed a DAO use case where different networks implement their own DHT with their own organization and rules. I think the interest conflicts in the DAO  governance may be raised from the divergence among these DHT organizations’ rules.', 'Thank you for this response!  I think one of the reasons this type of experimentation needs further exploration is that it’s not really clear whether these type of incentive mechanisms actually increase participation.  Have you come across any studies that show increased community participation due to the DAO structure?  I would not preclude that possibility, but it seems like there is a movement towards this specific structure without the validation that this type of structure is being perceived in the same way as the incentive mechanisms are being presented.  In other words, is there any proof that these types of incentive mechanisms have a superior chance of increasing participation than other types of structures?It is in this context that the potentially decreased security and monetized searches seem to have some conflicting aspects that might completely offset the incentives of payment.  Am I misinterpreting somewhere?', 'I’m genuinely surprised by the fortuitous discussion to have emerged from this particular paper!@Astrid_CH in your second point, it seems that you are suggesting a kind of quasi-oracle? I’m not sure I understand what these queries are actually supposed to be doing?', 'Thanks for your question. I seldom saw papers talk about this incentive issue from empirical analysis, I’m also looking for it. Because I’m not an expert in the security domain, I’m not quite sure what specific security issue scenarios would happen. The DAO proposed by the authors aims to reward the nodes, given the lack of details, let me infer the situation… nodes who are currently may not get anything from dealing with the saving and searching process would be more likely to take this job, more nodes may also join as the rewards could be expected. However, the potentially decreased security may reduce the number of people who want to save information, so there may not be so much information to be saved and processed. Consequently, I’m guessing that the number and growth of this DAO/DHT may fall on a balance point, which represents the acceptability of the security and the rewards. Or maybe it’s not the case, I’d really love to discuss this issue, it’s interesting. I think my discussion would be more accurate if you can give me more suspected situations about the security issues.', 'Thanks for your question. According to the authors’ argument, queries are more effective and efficient when they are being worked in this type of searching process (if you think keywords-based searching, such as a piece of content in the file, is more effective and efficient than searching by respective identifier or location):A file is saved in the node which is responsible for it, where nodes are structured as a hypercube. “In particular each logical node will locally store an index table where the CIDs of all the IPFS Objects associated to the keywords set it is responsible for are stored.” When a node receives a query request, the node will pass the request to its neighbor node which is nearer the destination node. By iterating the process, the request will end up reaching the destination node.I’m not quite sure what you are referring to as “quasi-oracle”, does the process above fit the quasi-oracle you mentioned?', 'So just a quick scenario I can think of based on their structure would be a malicious attacker spamming their network with searches both to simultaneously prevent actual users from being able to search while making the search results less accurate.  A sustained attack for 24 hours that made the platform unusable could have a massive impact on user engagement.  The immediate impacts would be either the cost of searches going up dramatically or the search engine coming to a halt because of the attacker hogging all the search bandwidth (assuming there is an upper limit that can be hit). In that there is a 51% attack calculator that shows the cost of this type of attack on the networks covered, having a payment-associated validating mechanism that is also attempting to decentralize the node validation makes this system susceptible to a 51% attack in theory.  Is there anything to suggest this stack is not going to be vulnerable to a 51% or malicious spam searches?', 'I think you raised a very insightful issue about this structure, I would not defend it as I think what you said makes sense and I didn’t see any defense material yet. Also, I’m guessing that this issue mainly comes from the node structure, but not the DAO’s or its reward mechanism. Do you think this problem less occurs in IPFS without applying the hypercube structure this paper proposed?']"
                Research Summary: Blockchain Technology: Data Privacy Issues and Potential Mitigation Strategies              ,https://www.smartcontractresearch.org/t/research-summary-blockchain-technology-data-privacy-issues-and-potential-mitigation-strategies/1148,Privacy,33,['https://www.davispolk.com/sites/default/files/blockchain_technology_data_privacy_issues_and_potential_mitigation_strategies_w-021-8235.pdf'],"['summary', 'privacy', 'ethereum', 'network-security', 'zero-knowledge', 'iot']","['TL;DRThe authors consider how blockchain technology conflicts with some of the provisions of the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA).To mitigate compliance risks enthusiasts should consider using private or consortium blockchains for compliance.Users should avoid or limit personal data stored in blockchain.Core Research QuestionWhat challenges for blockchain technology users are posed by complying with the GDPR and CCPA?CitationP. Shah, D. Forester, C. Raspe, and H. Mueller, “Blockchain technology: Data Privacy issues and potential …,” Practical Law. [Online]. Available: https://www.davispolk.com/sites/default/files/blockchain_technology_data_privacy_issues_and_potential_mitigation_strategies_w-021-8235.pdf 4BackgroundThe GDPR and CCPA govern the processing of personal data in the EU and the state of California, United States respectively. These regulations set out certain principles 1 guiding the processing of personal data. Particularly important is the penalty and reputational damage that comes with the breach of the regulations.The nature of blockchain technology makes it difficult for users to comply with GDPR in terms of material 2 and territorial scope, meeting requirements of legal bases such as the fulfillment of the contract in Article 6(1b) or balancing the legitimate interest in Article 6(1f), complying with rights of data subjects such as the right to rectification, erasure, access, portability, object to processing, including automated decision making.This notion of centralized entities that control both the data they collect, and their service provider relationships contrasts with blockchain technology’s distributed peer-to-peer network architecture.SummaryBlockchains “see” the information they process as anonymous because they use public-private key encryption that does not typically tie owner data or other personal information to an on-chain address.However, this is regarded as pseudonymization under article 4(5) GDPR because a blockchain method exists for linking individuals back to public keys by analyzing blockchain transactions and other publicly available data. Thus, pseudonymized data is still regarded as personal data that falls within the scope of the regulations.GDPR and CPPA foresee a centralized system that entrusts data controllers and processors with obligations and responsibilities with regards to personal data. However, the distributed nature of blockchain makes it difficult to determine who a controller or processor is.The territorial application of data is important for determining jurisdiction. However, a blockchain is decentralized and distributed, making it difficult to determine jurisdiction for the application of regulations.It is the position of GDPR that where personal data is to be transferred to another country, adequate protection must be ensured or the controller must implement additional safeguards. However, this safeguard will be difficult to implement in a public blockchain.Blockchain technology does not support consent as a legal basis for processing personal data since the withdrawal of consent will make later processing unlawful.The immutable nature of blockchain does not support the right to deletion and achieving strict technical deletion in blockchain will amount to a hard fork.The potential mitigation is to evaluate whether blockchain technology is a good fit for current business and processing objectives, prefer private or permissioned blockchains to enforce stricter usage rules, and to adopt alternative data encryption and destruction approaches.MethodQualitative research methods were used, drawing sources from regulations, guidelines, and other researched works to arrive at the conclusion.ResultsThe research found that blockchain technology, especially public blockchains, does not comply with data protection regulations. However, as an alternative the research recommended that users should:evaluate whether blockchain technology is a good fit for current business and processing objectives,evaluate whether they prefer private or permissioned blockchains to enforce stricter usage rules,adopt alternative data encryption and destruction approaches.Discussion and Key TakeawaysCharacteristics of BlockchainBlockchain technology gained prominence during 2017’s cryptocurrency boom, and has been used in many sectors such as smart contract development, supply chains, supply chain management, asset registers, fintech, real estate, health care and retail. The elements that distinguish it from other technology are:distributed ledger technologyconsensus mechanismsselection of public versus private participationtransaction immutability.Trends in Data Privacy LawGDPR and the CCPA pose a challenge to decentralized technologies like blockchain because they envision a data controller (an entity that determines the means and purpose of processing personal data) and a data processor (an entity that processes data on behalf of data controllers).The nature of blockchain technology makes it difficult for users to comply with GDPR in terms of material and territorial scope, meeting requirements of legal bases such as the fulfillment of a contract or balancing legitimate interest, complying with rights of data subjects such as the right to rectification, access, portability, object to processing, including automated decision making.This notion of centralized entities that control both the data they collect, and their service provider relationships contrasts with blockchain technology’s distributed peer-to-peer network architecture.Tension Between Blockchain Technology and Common Data Privacy RequirementsAnonymity, Pseudonymity and Privacy Law ApplicabilityThere is a sort of tension between blockchains and data protection on what amounts to personal data. GDPR and CCPA define personal data widely to include any information that directly or indirectly identifies a natural person.Blockchains see information they process as anonymous because they use public-private key encryption that does not typically record public key owner data or other personal information. This contrasts with the definition of personal data which includes pseudonymised information in so far, a method or link exists for re-identification. In blockchain technology, a method exists for linking individuals to public keys by analyzing blockchain transactions and other publicly available data.Some businesses offer services to identify individuals using their public keys, blockchain transactions and other available data. The public-private key encryption in the blockchain is a pseudonymization technique that lowers risk but does not remove regulatory obligations.Data Controller and Data Processor IdentificationThe concept of data controller and processor are key to GDPR and CCPA. However, the distributed nature of blockchain technology makes it hard to determine who the data controller or processor is.In a private or consortium blockchain, it may be easier to determine who they are because of their seeming centralized nature. The central operator or consortium may likely qualify as a controller or joint controller if they have control over the blockchain system and determine the purpose and means for any personal data processing. Other actors like nodes or miners can take the processor role.This may be difficult in a public blockchain because they lack a central operator, each node operates independently at least during the block verification process, which might prompt a conclusion that each node is a joint controller, although authorities and commentators are reluctant to support this conclusion for all nodes.Commission Nationale de l’informatique et des libertes (CNIL), a French data protection authority, attempted to provide guidelines for determining these concepts. The guidelines classified participants as controllers while accessors and miners are not because they do not determine transactions. The guidelines also noted that participants entering personal data on a blockchain for strictly personal purposes are not controllers under the GDPR household exception.Third parties who act on behalf of participants may become processors and should enter into data processing agreements with the participants.Miners who are not involved in the object of transactions are not controllers in CNIL’s view but may be processors if they follow the controller’s instructions. This tends to suggest that in certain circumstances miners may not be a data controller or a data processor. The guidance is not clear enough.Territorial ConsiderationsAn individual’s location and their personal data processing location are important for GDPR and CCPA compliance and enforcement. However, it is difficult to ascertain the jurisdiction of decentralized technology like blockchain and in terms of the applicability of jurisdiction’s laws.Private blockchains more often set restrictions in their governance models and agreements to limit the regulatory scope.Cross-border data TransferThe nature of blockchain poses a challenge to transborder data flows. GDPR and currently many data protection laws require that where data is to be transferred outside jurisdiction, the recipient country must ensure an adequate level of protection or the controller must implement additional safeguards such as standard contractual clauses, binding corporate rules, codes of conduct or certification mechanisms.However, these safeguards will be difficult to implement in a public blockchain with an undefined participant group.Legitimate reasons for processing personal dataPersonal data can only be processed on a specific legal basis. Federal sector-specific laws in the US like the GLBA and HIPAA limit the use of certain personal data without an individual’s consent with few exceptions such as uses for treatment, payment, and health care operations.For GDPR the lawful bases for processing must be one or more of these: consent, the performance of a contract, legal obligations, vital interests, public interest or official tasks and legitimate interests.While a blockchain may request consent from their users, compliance with requirements of consent may be difficult. This is because a given consent, among others, should be able to be withdrawn at any time but a blockchain ledger records data in such a way that it is hard to remove, thereby making later processing unlawful. Thus, organizations must carefully consider scenarios like consent withdrawal when determining what data they store in blockchain applications and how they record it.Immutability and individuals’ rightsData protection endows individuals with many rights, one of which is the right to delete data and effectively be forgotten. These rights conflict with blockchain technology’s transaction immutability.Blockchains can address data updates by recording additional transactions. However, later transactions do not technically delete data previously stored on the blockchain.Strict technical erasure of blockchain data may be achieved albeit in an effort comparable to a hard fork. This will be very difficult to implement every time an individual seeks to exercise their rights.Erasure may be more feasible in private blockchain governance models with a central operator. But this will greatly impact the celebrated distributed nature of Blockchain.Potential mitigation stepsOrganizations should follow several risk management strategies when considering whether to use blockchain technologies. They should:Evaluate whether blockchain technology is a good fit for current business and processing objectives.Evaluate whether they prefer private or permissioned blockchains to enforce stricter usage rules.Avoid and limit personal data stored on blockchain.Adopt alternative data encryption and destruction approaches.Blockchain companies may adopt certificate mechanisms and code of conduct.Implications and Follow-upsThe attendant consequence of GDPR is that every person adopting blockchain technologies must comply with its provision to avoid being penalized for a breach of data protection and the reputational damage that may come with it. Therefore, every organization deploying blockchain technology should consider data protection impact assessment to determine the risk of the technology to data subjects, and consider privacy by design as a default if necessary.ApplicabilityBased on the research, blockchain users should consider carrying out data protection impact assessment (DPIA) and privacy by design and privacy by default during the initial period of developing blockchain applications to ensure that the principles of GDPR and CCPA are complied with.', 'Learnt a whole lot from this summary. Thank you', 'Thank you for this wonderful summary!  Have you seen any sets of practices or protocols that would make it possible to comply with the aforementioned laws without compromising the privacy of the user?', 'I have not seen myself, however experts in privacy space suggest that private permissioned blockchain is  most likely to comply with GDPR and CCPA. It will all depend on the case by case analysis of the facts.', 'Nice summary of the tensions between blockchain and GDPR & CCPA. I actually only knew GDPR and never heard of CCPA.I have always wondered if introducing encryption of users information make the blockchain compliant. For example I remember proposals like  Hawk: The Blockchain Model of Cryptography and Privacy-Preserving Smart Contracts.Say there is a service provider (a DeFi app or something) that encrypt users’ data. Since they cannot remove the data that is on the blockchain, are they compliant if they propose users to just delete the corresponding private key?', 'They won’t be compliant because the encrypted personal data in the blockchain is still being processed.  Encryption reduces security risk but doesn’t take aware an information from being personal data since it can be decrypted.I will recommend pseudonymisation techniques against Encryption.', ' Samuel94:Data protection endows individuals with many rights, one of which is the right to delete data and effectively be forgotten. These rights conflict with blockchain technology’s transaction immutability.I realize this will seem naive of me, but we are all now routinely obliged to click buttons accepting or rejecting cookie-collection on web sites. Public blockchain technology surely has as much right to exist as cookies do, as long as it informs users of the consequences.Many people want to participate in decentralized, peer-to-peer communities. So why can’t public blockchains simply require a “checkbox agreement” during signup that says:“This community is managed by a public, peer-to-peer blockchain. By joining it you give up certain rights covered by the GDPR and other agreements. These include the right to delete data and the right to be forgotten. If you don’t agree with this stipulation, please join a private or permissioned blockchain more capable of protecting your privacy.”', 'Data Protection rights as contained in the GDPR are fundamental human rights. See Articles 7 and 8 of Charter of Fundamental Human Rights which further provides under Article 52 that the rights shall not be limited except as outlined by the Charter. Specifically, your excerpt suggests or tends to rely on consent as a legal basis. However, for consent to be valid, it must be freely given, informed, specific, unambiguous and easy to be withdrawn at all time. You can see from the excerpt that consent cannot be said to be freely given. You can check Article 6(1)(a) of GDPR.', 'I also want to add that blockchain technology has prevented a difficult situation to regulators. Presently, there is no guidance from European Data Protection Board (EDPB) on the best way to achieve compliance but there is serious optimism that it will materialise this year.', 'can you expand on what the authors mean by Encryption?Most blockchains usually only use digital signatures and hashing.', 'I believe the authors were referring to hashing. It is possible they did not bother much about the technical difference between the two.', 'Thank you for this summary detailing how concepts in blockchain and data privacy converge and diverge. It appears to me that the general intent of privacy laws, including the GDPR, CCPA, NDPR, etc, is to protect personally identifiable information from unauthorized access and use. The paper mentioned pseudonymization as a possible solution, although not sufficient for full compliance.My question is, would privacy/pseudonymization technologies like ZK-SNARK (Zero-Knowledge, Succinct, Non-Interactive Argument of Knowledge) help in this instance? (Ignore the almost tongue twister). Here’s 1 a simplified non-technical explanation for ZK-SNARK, and here is a technical/mathematical version. Essentially, ZK-SNARK allows a data owner to confirm the existence, accuracy and validity of data without disclosing the data. Would this remove the need for meeting the requirement for deletion since there is ‘nothing’ (personally identifiable information) to delete?Considering that zero-knowledge proofs (ZKPs) have been in existence since the 1990s, is there a reason the authors did not mention it in their paper as a possible pseudonymization solution?', 'ZKPs based on the explanation is a tool for pseudonymisation. Pseudonymisation has been acknowledged as one of the best tools for reducing security risk in Data Protection.  However, it is still personal data and must comply with GDPR because there is possibility of reidentifying the information.  So it can mitigate risk but doesn’t take it away from being personal data.']"
                Research Summary - SoK Decentralized Finance (DeFi)              ,https://www.smartcontractresearch.org/t/research-summary-sok-decentralized-finance-defi/1055,Mechanism Design and Game Theory,44,['https://arxiv.org/abs/2101.08778'],"['defi', 'summary', 'mev', 'discussion', 'governance', 'front-running', 'game-theory']","['TLDRWhy is DeFi interesting? It allows new financial services that are non-custodial, permissionless, openly auditable, and composable. While DeFi also faces many challenges and security issues, these properties can help to scale back trust assumptions and increase some aspects of efficiency in financial systems.Security in DeFi is separable into “technical security” and “economic security”.Technical security describes exploits that are atomic, and therefore instantaneous and risk-free, in nature. These typically abuse the technical implementations of protocols and transaction ordering/inclusion within blocks. It is best addressed with program analysis and formal specification of protocols.Economic security involves the manipulation of economic equilibria over time in a non-atomic, and therefore risky, manner for an attacker. It is sparsely studied yet growing in importance. Resolving economic security requires synthesizing insights and models from across computer science, economics, and finance.Core Research QuestionThe paper exhaustively delineates the DeFi security challenge into technical security and economic security, centering on the property of atomicity, and connects these categories back to the fundamental research work that is needed to make DeFi secure.CitationWerner, S.M., Perez, D., Gudgeon, L., Klages-Mundt, A., Harz, D., Knottenbelt, W.J.: Sok: Decentralized Finance (DeFi). arXiv preprint arXiv:2101.08778 (2021). 30BackgroundAtomicity: A transaction property dictating that the transaction either succeeds fully, resulting in a state update, or fails entirely, leaving state unaltered, such that no execution can result in an invalid state.Composability: A property of smart contracts that are able to communicate with one-another, via message-calls, within the same execution context. Composability means that smart contracts can be snapped together like Lego bricks, with the possibility of building complex interconnected financial architectures.Miner extractable value (MEV 1): Blockchain miners have the ability to control the sequence in which transactions are executed. A rational miner will order transactions in ways that earn them revenues and even insert their own transactions to extract further revenues. MEV is the value that miners can extract by selectively ordering, censoring, or inserting transactions within a block or across blocks.Governance extractable value (GEV 1): Many DeFi protocols have governors who perform a governance function to update the protocol over time. GEV is the value that governors can extract from the system through this role, including potentially perverse incentives to deviate from the best interest of the protocol, for instance, by effecting changes that provide outside benefit to governors but may be harmful to overall system health. GEV includes short-termism and explicit governance attacks.Sandwich attacks/transaction ordering attacks: A type of exploit in which an attacker orders contract calls in a way to set up a profit opportunity through manipulating the technical implementation and state of a system. It usually involves inserting, or “sandwiching”, contract calls before and after a targeted contract call, usually an asset swap. It can take the form of a single transaction attack, in which a smart contract system is usually being exploited, or a multiple transaction attack, in which new transactions are inserted before and after a user-generated transaction or swap. A typical example aims to manipulate the instantaneous price at which a targeted swap is executed.Timelock: A smart contract mechanism that requires a non-zero amount of time to pass before an action can be completed, for instance, if an action can only be completed in a subsequent block. Timelocks are often applied in the context of governance updates, so that users have time to react to proposed changes.SummaryConsider two important views on DeFi, that of an optimist and that of a pessimist. According to the optimist, DeFi extends the innovation of non-custodial transactions to complex financial operations, enabling a non-custodial, permissionless, openly auditable, and highly composable financial system. In contrast, the pessimist views DeFi as an unregulated ecosystem prone to hacks and that can be used to facilitate financial crime. While part of this debate is moral in nature, another part is analytical. For DeFi to fulfill the vision of the optimist, it must be secure, which is something that can in principle be evaluated with formal models.The paper provides a concise introduction to DeFi with a focus on enabling newcomers to start evaluating the technical innovations of DeFi. This includes DeFi primitives, such as smart contracts, keepers, oracles, and governance, as well as a range of protocol types, including decentralized exchanges, protocols for loanable funds, stablecoins, portfolio management, derivatives, and privacy-preserving mixers.The meat of the paper provides a new characterization of the security risks in DeFi, delineating between technical security and economic security. The delineation centers on atomicity: whether the attack is near-instantaneous and can costlessly fail, and is therefore risk-free, or has a non-instantaneous duration and where failure comes with a cost related to manipulating an economic equilibrium over time. They illustrate with many examples and exploit types and discuss the state of the art in modeling and addressing these security issues. They connect with the existing research literature and demonstrate where this literature has significant gaps, particularly around economic security.MethodThe paper is a systematization of knowledge (SoK). It overviews the new and wide space of DeFi protocol design and synthesizes new takeaways about the fundamental security problems to be solved in DeFi, both technical and economic.ResultsThe researchers provide a conceptual overview of the different constructs within the DeFi ecosystem, summarized in the following figure. These start with basic distributed ledger properties, such as smart contracts and tokens, which enable DeFi primitives, like oracles, governance frameworks, and market mechanisms. DeFi protocols then assemble primitives to perform specific functions, such as asset exchange and loanable funds markets, among several others. DeFi composability then enables nested interconnections of different protocols, for instance, providing liquidity on an asset exchange that is simultaneously used as collateral in a loanable funds market.658×562 19.9 KBTechnical Security. The researchers classify a DeFi security risk as technical if an agent can atomically exploit the technical structure of the system, for the sequential and atomic execution of transactions. Technical exploits can be performed near-instantaneously and risk-free because the outcomes for the attacker are binary: either the attack is successful or the transaction reverts and the attack effectively doesn’t happen. In particular, the costs of attack failure are minimal gas fees.Technical security typically coincides with (1) manipulating an on-chain system within a single transaction, which is atomic for anyone, and (2) manipulating ordering/inclusion of transactions within the same block, which is atomic for a miner generating that block. This includes concepts such as atomic MEV and GEV, sandwich attacks and other ordering attacks, and smart contract code vulnerabilities, such as reentrancy and logic bugs.A particularly interesting inclusion is sandwich attacks here, which is usually described under the vague term “economic risk”. An intuitive way to think of technical security is from networking smart contract “vending machines” together and exploiting the joint structure of how they are programmed. This is essentially what a sandwich attack does. While the vending machines may be motivated for economic reasons, the sandwich attack exploits the particular way that they are implemented and networked. For this reason, the researchers suggest that sandwich attacks are best understood as technical in nature.Economic Security. A DeFi security risk is classified as economic if an exploiting agent can manipulate the incentive structure of the protocol non-atomically to realize a profit. The researchers discuss how this leads to exploits with distinctly different properties from technical exploits. Economic security exploits inherently involve manipulating a market equilibrium over some time period. Since economic exploits are non-atomic, they come with upfront tangible costs, a probability of attack failure, and risk related to mis-estimating the market response to the attack. Thus an attacker bears significant risks in performing such exploits.Economic security includes non-atomic GEV and MEV, including chain reorganization attacks, most cross-chain MEV, as well as market manipulation exploits. A key point is that, while a hypothetical poorly designed system could allow some of these exploits in an atomic fashion, the underlying problems are not solved by removing atomicity, for instance by introducing a timelock. The remaining issues are inherent economic problems about what the market equilibria are and how they can be manipulated over time. For instance, GEV exploits could be performed atomically, but the introduction of a governance timelock doesn’t solve GEV issues entirely. Another example is using an AMM spot price as an oracle, which is technically insecure, as opposed to using a time-weighted average AMM price, which moves the problem into economic security as the time-weighted average can be manipulated non-atomically through manipulating the AMM market over time.In market manipulation attacks, an adversary manipulates the market price of an asset over a time period in order to realize a profit in a related market, for instance, a DeFi protocol that uses the manipulated market as a price oracle. The attacker bears an upfront tangible cost here of maintaining a market imbalance over time. The researchers illustrate the potential of such an attack in Compound. In Nov 2020, DAI traded at a temporary price of $1.30 over a course of 20 minutes on Coinbase, before returning to the $1 peg.As a result, the Compound Open Price Feed, which in part uses prices signed by Coinbase, reported a DAI price of $1.23 to Compound for a short time period. This incident triggered liquidations in Compound worth $89m, costing liquidated Compound borrowers 28% on liquidated assets. While this incident was not clearly an exploit, the market structure could be exploited in this way, allowing an attacker to profit by performing the triggered liquidations. A related exploit later occurred in the Venus Protocol.Discussion and Key TakeawaysWhile DeFi may have potential to create a permissionless and noncustodial financial system, the view of the DeFi optimist, the open technical and economic security challenges remain strong. Solving these challenges in a robust and scalable way is a central challenge for researchers and DeFi practitioners.The delineation of technical and economic security helps illuminate the fundamental challenges in DeFi. Technical security is a first bar: if a protocol is not technically secure, then it will break in the presence of rational agents. Economic security makes sense as a further bar. For instance, if a protocol’s funds can be exploited because it is not technically secure, then in an economic sense no rational agents should participate. On the other hand, economic security involves economic problems that cannot be fundamentally solved by technical means alone.Economic security risks remain largely unexplored. Practically speaking, full understanding of economic security problems requires models of economic equilibria in these systems, and protocol incentive structures need to be designed with this understanding in mind. These models differ considerably from traditional security models and require synthesizing insights from across computer science, economics, and finance.With high protocol composability, security risks become increasingly complex. A critical gap in DeFi research in formalizing models to quantify composability risks. This problem is elevated as a holistic view on the integrated protocols is necessary: failures might arise from both technical and economic risks.Implications and Follow-upsDesigners of DeFi protocols need to understand and address both technical and economic security challenges, both in the protocol they are designing and in how their protocol composes with other protocols.Technical security now has a sizable literature to draw on and is best addressed through tools such as program analysis 3.Literature on economic security is sparse, with limited work on economic attacks on stablecoins 1, governance incentives, and time-bandit attacks 1. Recent work on cross-chain MEV 1 also mostly fits in this category.ApplicabilityThe paper provides a basis for understanding security challenges in DeFi, both for practitioners and researchers. The paper covers how to address these issues to the extent that defensive measures exist today. Where mitigations are not yet developed, the paper discusses the next research steps that are needed as well as new mechanism proposals that may help solve underlying security issues.', 'Thank you very much for this post, as it gives some insight into how security needs to be addressed from different arenas instead of as a single catch-all approach.  Do you have any of the researchers referencing the ACID framework 2?The language seemed to be in that context, but I was unsure if they were framing it from the database management system paradigm or trying to assert that DeFi needs its own specific lexicon?About the recent attack on BadgerDAO, was there any discussion of UI/UX by the researchers; in other words, was there any explicit acknowledgment of UI/UX problem within DeFi from which to establish a new security framing or lexicon?', 'I have some seemingly dumb questions here - but I think the same would be asked by newbie-ish ppl from time to time, so I’ll be the one asking.Would such high dependence and composability risks trigger a blockchain equivalent subprime mortgage crisis?You summarized the composability risks well: flash loans, malicious updates to DAO/protocol contracts, algorithmic stablecoin, protocols for loanable funds, wrapped coins and tokens, bridges between chains…And yet we already have Black Thursday for MakerDAO - hell, we even have an insurance dapp 3F Mutual - Collective Insurance Against MakerDAO Risks - Defi Pulse Blog to hedge against such liquidation risks…This is just from one dapp, and you already described market(oracle) manipulation well, that’s between (at least) two dapps.I wonder, how would you personally think about this?Would such a chain reaction/domino effect happen in the near future cause defi legos are too reliant on each other?Apart from that, what can we do (apart from urging everyone to do their f***ing audit and don’t label your product as beta to prevent public backlash from future attacks), before thing’s too late? (that is, it’d be very appreciated if you can further clairfy section 6.1.)Thank you for your hardwork, It filled in many knowledge gaps in my mid - I personally learned a lot from it, and I believe others will, too!', 'Thank you for the summary!I have some problems with the difference between Technical Security Risk and Economic Security Risk. I saw you mentioned it’s the atomicity and also that a vulnerability that can be atomically exploited won’t be solved by removing the atomicity, therefore an economic attack in 1 transaction (e.g. the Cream yUSD Exploit) which is considered as a technical security risk can always be turned into an economic security risk. Then, we can use ways which are used to verify the economic security risk to verify this kind of risk. Why don’t we just put it into the category of economic risk?I also came up with the attack on Fomo3D. For people who forgot or don’t know about this attack, the hacker submitted a transaction in sequential blocks that will fail by using all gas in a block when the hacker didn’t win in the block. (Details here 1) In this sense, maybe we can turn an economic security risk technical? (I don’t know if it’s impossible now.)', '@aklamun  Thank you so much for presenting this wonderful summary. It contains many interesting topics that are worthy to discuss.Concerning the DeFi risks, BIS Quarterly Review, December 2021 - DeFi risks and the decentralisation illusion 1 has a similar observation, particularly the risks that were inducted as Economic Security Risks in this paper. The report mentioned several vulnerability causes of DeFi, including high leverage, liquidity mismatches, built-in interconnectedness, and the lack of shock absorbers. What is most emphasized in this report is the “illusion of decentralization” of DeFi, which may be corresponding to GEV in the summary. It also implicitly indicates that this GEV would be one of an impediment for DeFi to be more adopted by the conventional financial system. The way it provides to resolve the risks is the governing by public authorities. Did the authors provide some approaches or discussions to reduce GEV risks? Which do you think is the most realizable except for being governed by public authorities?', 'In the study of financial institution management, one about traditional finance, risks are classified into interest rate risk, credit risk, market risk, liquidity risk…risks that do not typically result from unintended design or behavior, and some not manipulatable by individuals.Do you think your framework should or can cover the risks that are widely discussed in traditional finance? If so, will your proposed framework remain useful as the DeFi space develops and matures? Or will these issues, especially technical security, continue to dominate as it is inherently related to the nature of DeFi?For more context, there is one class, technology and other operational risks, that does stem from unintended behavior. But that refers to computer failure, human error, fraud, and catastrophes such as natural disasters, which do not belong to our scope of discussion.', 'I think the ACID framework would be most relevant in describing properties of transaction execution, which DeFi builds on top of.We (I was one of the researchers) did not specifically discuss UI/UX in this work, though that is also a critical area. My first thought is that it is functionally a similar issue to non-blockchain UI/UX although the user has a lot more personal control.', 'Yes certainly, with a high composability comes a lot of potential risk. A specific example is Stableswap/Curve pools: if any asset in such a pool fails for any reason, then all LPs are effectively wiped out. And most things that build on top of these pools would be affected as well. In my opinion, we should be designing new protocols to be robust to these sort of events. I think the best way to do that is to carefully segregate the risks the protocol is taking on. While it is not always straightforward to quantify all of these risks (and correlations are not usually the right way of measuring them), I hope that the general discussion in the SoK helps identify some of the main types.', 'One useful distinction in technical vs economic security is in understanding the type of risk taken on by the attacker. To illustrate, it’s much easier to attempt an attack if you know that at most you lose the gas fee, but potentially you come away with millions of dollars in assets. On the other hand, if you have to commit millions of dollars in assets that you would lose if the attack fails (and there is some probability that it would fail), it’s a much different story.For your example of using all the gas in the block, the attacker would need to outbid the entire market for block space over a time horizon, which has a fairly high cost. Note that there is still a cost if the attacker is a miner–either in opportunity cost or from the fact that they may not mine each of these blocks. So in the end, the attacker would have to put a fair amount of money at risk even if the attack ends up being unsuccessful.', 'That’s a good question. We wrote a further article on GEV 7 that discusses some ideas to help resolve it. In particular, optimistic approval, which is an optional veto mechanism that could better align incentives between protocol users and governors, and conditional cashflows, which gives governors more skin in the game in case they make poor decisions. But these are not too widely explored yet.', 'We took a particular focus in this work on security risks in DeFi in this work as opposed to other financial risks. But you’re right that there are also many financial risks that should be considered.In some ways, the security risks are somewhat relatable to the technology and other operational risks category. If a security risk is realized, it means a system has been somehow compromised, which is unintended behavior.I think these sorts of security issues are core to DeFi and so are not going to go away. Although we should aim to improve best practices for mitigating them.']"
                Research Summary - Chainlink Off-Chain Reporting Protocol              ,https://www.smartcontractresearch.org/t/research-summary-chainlink-off-chain-reporting-protocol/230,Oracles and Data,82,['https://chain.link/ocrpaper'],"['scalability', 'summary', 'oracles', 'iot', 'defi']","['CitationBreidenbach, L., Cachin, C., Coventry, A., Juels, A., & Miller, A. (2021, February 24). Chainlink Off-Chain Reporting Protocol. Retrieved from https://chain.link/ocrpaper 97Linkhttps://chain.link/ocrpaper 97Core Research QuestionHow does the Chainlink Off-Chain Reporting protocol work, what are its design goals, and what are the algorithms used for its implementation in the Chainlink Network?BackgroundOracles are off-chain agents that connect on-chain smart contracts to external resources such as data APIs that reside outside of a blockchain network. Such data is often required in the execution of automated smart contract applications.Price Feeds are on-chain reference contracts updated by oracles and provide smart contracts access to financial market data regarding various assets, enabling the creation of decentralized finance (DeFi) applications.Gas is the unit that measures the amount of computational effort required to execute and validate transactions on the Ethereum blockchain. More complex transactions performing many operations consume more units of gas.Gas price is the amount of ETH that is required to be paid per unit of gas to miners on the Ethereum network. Gas prices are denoted in Gwei, with each unit Gwei being equal to 10-9 ETH. Times of higher network congestion result in a higher gas price and more expensive transactions.On-chain Aggregation is an oracle network model where each node fetches data from an external data source and posts it on-chain within separate transactions. Each transaction consumes gas and thus each node must pay an individual transaction fee determined by the current gas price.Off-chain Aggregation is an oracle network model where each node fetches data from an external data source and then collectively aggregates their responses off-chain into a single transaction containing a sorted list of values. Only a single node submits the transaction on-chain, reducing the total amount of gas consumed per update.Oracle Report is a collection of responses from nodes within an oracle network during one update period. A report includes each node’s individual observation and their associated signature.SummaryThe Off-Chain Reporting (OCR) Protocol is a scalability upgrade of the decentralized oracle network Chainlink 1 which decreases the on-chain gas costs of generating updates for the Price Reference Feeds 5 by moving the data aggregation process off-chain using a distributed peer-to-peer network. OCR reduces the number of on-chain transactions required per update in an oracle network with n nodes from O(n) to O(1).The OCR protocol is described as being developed with four primary goals:Resilience: The protocol should be resilient to different kinds of failures involving Byzantine nodes and infrastructure crashes. An honest OCR node that is temporarily disrupted should be able to recover and rejoin the protocol quickly and without manual intervention.Simplicity: In order to quickly meet market demand for oracle scalability, the protocol is designed to favor a straightforward implementation.Low Transaction Fees: On-chain interactions should be minimized to lower the amount of fees nodes are required to pay. Ethereum transactions can carry a significant fee, particularly during periods of network congestion. OCR favors off-chain communication and computation wherever possible.Low Latency: The time between when an update is initiated off-chain and when the data is included on-chain should be minimized as DeFi smart contracts require fresh data. The protocol should be able to produce an off-chain report within a few seconds and have it confirmed on the blockchain as soon as possible.MethodThe paper’s authors first formalized a model of the OCR protocol’s liveness and safety thresholds when there is at least n > 3 nodes:Any f < n/3 oracles may exhibit Byzantine faults and behave arbitrarily as if controlled by an adversarial actor.It is expected the OCR protocol operates with n = 3f + 1 oracles as this gives optimal resilience (Byzantine nodes are less than ⅓ of network).If f oracles are Byzantine-faulty (malicious node) and c oracles are benign-faulty (unresponsive honest node) with f < n/3 but f + c ≥ n/3, then the protocol may lose liveness but always satisfies the safety properties.Using the median value from at least λ = 2f + 1 observations (responses from more than ⅔ of the network) ensure the final report is plausible in the sense that faulty oracles cannot move the median outside the range submitted by correct oracles.The OCR protocol is described as being structured into three primary protocols run by each node in the network:Pacemaker:The pacemaker protocol drives the report generation process, which is structured into epochs.Each epoch has a different leader node who coordinates the creation of a predetermined number of reports with observations provided from the follower nodes.The pacemaker protocol runs continuously and periodically initiates a new epoch and pseudo-randomly selects a new leader.Each follower node monitors the performance of the leader and if not enough progress is made within a specific timeframe, a new epoch is initiated and a new leader node is selected.Report generation:The report generation protocol divides each epoch into numerous rounds, with each round corresponding to the creation of a new report.In each round, observations from each oracle node are collected and an aggregated report is generated that is signed by a threshold of oracles.To prevent unnecessary on-chain transactions, a report is only created and validated by oracles if the previous on-chain update has deviated beyond a specific threshold against an off-chain data source (e.g. 0.5%) or a specific time interval has passed (e.g. 1 hour).Transmission:The transmission protocol encapsulates how the report is submitted on-chain and does not require communication between nodes.The transmission protocol delays each oracle’s submission on-chain pseudo-randomly to ensure a staged sending process, ensuring not too many redundant copies are submitted on-chain.Once a report has been validated by miners and added to the blockchain ledger, the transmission protocol process ends for the current round.The specific steps taken for the creation of a single report within a round follow the below procedures:A new round starts and the leader of the current epoch requests an observation from all follower nodes in the network.Each follower node fetches data from a predefined data source API, signs the data using their private key, and sends the result back to the leader node.The leader node waits for at least 2f + 1 follower nodes to respond, plus a grace period, then sorts the responses by value, generates a report, and sends it to all follower nodes.Each follower node validates the report by checking the values are sorted, contains observations from at least 2f + 1 follower nodes, all signatures are valid, and that the median value exceeds the deviation threshold of the previous on-chain update or a time-based heartbeat condition has occurred.If all conditions are met, each follower node generates a compressed report with just the node observations and oracle identities, signs it, and sends it back to the leader node.Once the leader node obtains signed reports from more than f follower nodes, the leader assembles a final report from the followers’ signed reports, which is then broadcasted to all follower nodes.When each follower node receives the final report for the first time, they rebroadcast it to every node, ensuring all nodes have received the report.When a follower node receives broadcast from more than f nodes, the transmission protocol is started.The report generation protocol for a round is now complete and the leader node waits a predefined amount of time until starting a new round. If a new epoch occurs or the leader does not make progress in a specified amount of time, then a new epoch is created and a new round initiates.In the transmission protocol, to prevent unnecessary gas costs, the report is put through a filter and passes if 1) there is no backlog of reports or 2) the median value in the new report deviates beyond the median value of the report in the backlog by a sufficient threshold.If the report passes the filter, a staging process begins where one or multiple nodes are pseudorandomly chosen to create a transaction to submit the report on-chain.If a transaction is not confirmed on-chain within a specific time delay, a round robin approach is started where additional nodes begin to make an on-chain transaction in a time-staggered manner.Once the report is added to the blockchain, an on-chain smart contract validates the signatures, stores the median value, pays nodes who contributed an observation and compensates the transmitter for its transaction gas costs.The transmission protocol for a round ends when the corresponding report has been accepted by the contract.854×1334 79.4 KBResultsThrough the creation of a technical specification for the Off-Chain Reporting Protocol and its sub-protocols, the authors of this paper have defined how the decentralized oracle network Chainlink reduced the on-chain gas costs of Price Feed updates by up to 90% through batching node observations into a single on-chain transaction.The authors also modeled the security assumptions of the Off-Chain Reporting Protocol including the proportion of Byzantine nodes for optimal resilience and the security measures implemented to protect against malicious leaders, ensuring progress cannot be halted beyond a predetermined amount of time.Follow-up and Future WorkMultiple avenues are described in how the Chainlink Off-Chain Reporting Protocol could improve in the future.Currently, reports are generated on a static interval. It may be desirable to produce more frequent reports upon observing changes within the data feed itself, such as increased volatility. However, such a design could generate reports at a faster rate than the frequency with which Ethereum produces blocks.The OCR protocol can adopt an aggregated threshold signature scheme instead of the separate ECDSA currently used. The constant on-chain gas costs of verifying a single threshold signature would allow for larger oracle sets and further lower on-chain gas costs.A new oracle list could be signed by the current quorum of oracles to change the off-chain oracle set without requiring the owner to intervene. This would allow for dynamic changing of the oracles used within an OCR-based oracle network.ApplicabilityThe current application of the Chainlink Off-Chain Reporting Protocol is primarily focused on improving the Price Reference Feeds 2, which provide a secure source of high-quality financial market data for the Decentralized Finance (DeFi) ecosystem. By reducing the gas costs of updates, deviation thresholds can be lowered, more price feeds can be launched, and a greater number of nodes can be added to each network due to the more efficient use of Ethereum’s transactional bandwidth.The Off-Chain Reporting Protocol can also be used for the creation of any type of on-chain reference feed required by smart contract applications, such as the current temperature of a specific real-world location, the amount of off-chain US dollars in a bank account backing an on-chain stablecoin, and numerous other datasets. Additionally, Chainlink OCR networks can be natively launched on other blockchains beyond Ethereum to extend the Chainlink Network’s cost-effective data computation model to the growing ecosystem of interacting chains.Discussion TopicsWill the gas improvements from OCR lead to a reduction in blockspace demand from oracles or will more oracle networks be launched to fill the gap?What is the lowest deviation threshold at which an OCR-based price feed could realistically update, particularly during times of extreme volatility and network congestion?How can the OCR Protocol be used for other oracle network models beyond on-chain reference feeds?', 'I wonder, from the perspective of DeFi protocols or anyone making price feed queries from chainlink nodes, besides the increased frequency of updates possible, is there any other draw to this less tested and more novel approach (via OCR) rather than the standard onchain feeds, which will run in parallel and not be removed?', 'Increased frequency (fresher data) and reduced absolute cost for the use of an individual feed are a couple of the immediate benefits. Both of these factors can potentially enable additional usecases for smart contract based platforms that rely on extremely fresh data (a number of trading usecases where very small changes in the price of an asset have significant consequences, locational data, etc) and absolute number of users, as the cost of using an individual feed is driven down.Ultimately OCR represents an additional version of Chainlink’s core client, which will run in parallel with the FluxMonitor client.', 'What is the process for adding my node to an oracle set / feed ?', 'This is a question better served for a Chainlink specific channel, like their Discord 9. This forum is intended for technical discussion of academic research relating to computer science, blockchain, cryptography, etc.', 'You can’t measure the security of the system without describing how a node is added to the validator set.Is is permissioned? Is it permissionless? What guarantees does the listing process provide?All very relevant research questions.', 'Hi @thegostep, Chainlink nodes are permissionless to operate, while the users (data requesters) themselves choose which nodes are composed into their oracle network. The selection of nodes can be based upon different metrics such as a node’s off-chain reputation, historical uptime, average response time, selection of data sources offered, etc. Operators can list their node on third party marketplaces and reputation services like https://market.link 7 and https://reputation.link 10 for discoverability.However, I would like to note that the discussion topic within this thread is specifically regarding the Off-Chain Reporting paper and the new method of aggregating data it presents.', 'I can see how this type of system might be the actual type of infrastructure that gets implemented into a grocery store or warehouse as a means of tracking transactions with an aggregated chain of events.  The issue becomes the simultaneous needs to track events, inventory, and sales transactions on the same item whether it is a banana or even an NFT to put it in a crypto context.  A grocery store needs a point of sale system in order to accurately track all those elements of a single item going through the system.  If an item is tracked directly on a blockchain, putting each event on chain will unquestionably result in more fees accrued on whatever respective chain the data is being stored.Taking events and aggregating them into a consolidated transaction off-chain to be input onto a chain later in the process not only reduces fees, but for an actual business organization would be more in line with daily operations and how inventory flows in from a warehouse to be tracked, placed on the shelf, and eventually sold to a customer.  While it is highly unlikely a grocery store will use Chainlink’s OCRP directly, this type of protocol may in fact be the exact type of infrastructure that a point of sale system provider would implement.If we reframe it, would it even make sense for a business to want to put every single transaction "" directly on-chain"" when aggregating transactions cuts operational costs?', 'Dovetailing off of your example above @Larry_Bates I view the idea of being able to aggregate data off-chain and then upload it all at once in a single transaction through one leader node may be highly beneficial in developing economies, especially agriculturally centered ones where market prices may fluctuate drastically.For example, we’ve seen how the introduction of cell phones to agricultural markets in Niger and India helped decrease price dispersion and bring the markets closer to the Law of One Price 2. This allows for fairer market practices, lowered inequality, and higher bargaining power for small-scale producers. Before cell phone integration to markets, there was high price disparity allowing for large firms to take advantage of smaller producers.Oracles, like Chainlink, have the ability to take this one step further. If buyers peg their purchasing price to global/state-level prices obtained via a network similar to that of the existing Price Reference Feeds (but for goods not cryptocurrencies) then inequality regarding market prices may approach zero. However, one of the biggest challenges of implementing this system would be gas costs/fees.The implementation of OCR would allow for this data to be aggregated in developing economies for a fraction of the price currently required. Personally, I do not think the idea of using oracles in developing economies, especially agriculture-centered ones, was feasible due to the data upload and cost requirements for on-chain aggregation. Now with the potential of allowing a single node to submit a transaction of aggregated data on-chain this theoretical implementation may be feasible due to the substantial reduction of costs.Back to your original question and idea of supermarkets using this type of data, I’m not sold that is the best use case. In time it may very well be, and I hope to see blockchain, in general, being adopted more and more by supermarkets. I view agricultural producers and industrial manufacturers acting in the B2B space, as well logistic companies, to be some of the sectors outside of the current DeFi ecosystem, that could benefit from OCR in the long run.', 'I realize that my allusion to “point of sale” systems might need some more context.  “Batch-processing” has been a staple of computer science theory to reduce cost of processing transactions and increase efficiency in processing large volumes of data for decades.  Batch processing is a standard in large-volume data processing, and is one of the main methods of consolidating data for global networks including the Internet of Things.OCRP effectively reproducing batch processing in a decentralized environment represents a leap in applying computer science theory to decentralized computing frameworks, in that distributed networks have already been using batched processing for a significant amount of time with proven results in cost reduction and processing efficiency improvement 2.', 'Like @Larry_Bates has stated I don’t think the real-world benefits of OCR have been fully realized. Right now, I would argue most people simply think of price oracles when they think of Chainlink, but the implementation of OCR enabling what @Larry_Bates states above:OCRP effectively reproducing batch processing in a decentralized environment represents a leap in applying computer science theory to decentralized computing frameworksimplies that any organization that processes large batches of data can now do so on blockchain at a fraction of the price they could have before. This has large-reaching implications for all sorts of industries that we’ve yet to discuss here. I can see this being used by governments for things like batches of FOIA requests, insurance companies for large batches of claims after natural disasters, or even an application for voting where we could local voting locations all uploading results via OCR which would reduce the number of locations that would have to independently upload data.Also, I want to point out, a SCRF member @patrickalphac created a very easy-to-follow video explaining OCR and what it means in basic terms here 4 for anyone interested in taking a look.', 'Thanks for sharing the video, @tjd233. And obviously, thanks to @patrickalphac for making it.I’ve enjoyed reading this discussion so far, but it is leaving a lingering question in my mind. Since OCR is so seemingly overwhelmingly beneficial, why isn’t it being more readily and rapidly implemented everywhere? Before that question becomes a rabbit hole, I know there are a bunch of “people are the problem” elements here. Certainly awareness of blockchain technology and/or trust in blockchain are likely contributors (as has been somewhat noted by others, e. g. Craggs & Rashid, 2019 1). I don’t want to move the conversation away from the technical capacities here, so the question I’m ultimately asking is are there technological downsides to OCR that would need to be overcome in order to facilitate more widespread adoption?', 'Thanks all! Sorry I’ve not been as active here.Basically the question to your answer is “it is”. Price Feeds are slowly being rolled over to OCR networks. These are the networks used by Aave, Synthetix, Set Protocol, etc.On the technical side, there isn’t much for downsides. Obviously, the upsides are that it drastically cuts down on gas due to the 1 transaction being sent. The upsides come from a much more sophisticated implementation of the network.One thing to mention is that due to this, the networks are reaching a quorum off-chain. The off-chain quorum you may have noticed is actually higher. You need 2/3rds majority (similar to PoS vs PoW) to reach a quorum. This is why adding more nodes is not only important but necessary for the networks. Each node needs to connect to the peer-to-peer network of other nodes, and therefore has to add some security features since they are now talking with multiple systems.Most nodes additionally are adding a cache like redis. OCR pings off-chain data providers much more often, and multiple jobs could be hitting the same API multiple times. To not hit API thresholds, a cache will actually ping the API just once, update all values, and then the node will just pull from the cache. So this is a technical barrier that node operators need to add and be aware of. If they don’t understand how to setup a cache, this of course could be a barrier. They could still have an OCR node without a cache, however they then will probably have increased data costs by hitting APIs more often.Hope that helps.', '@Eric after reading through this again, and taking a look at some other discussions online I have two questions that are related to how data is aggregated using OCR and then how that impacts the overall security of the system. I’m not 100% familiar with how some of these work on the Chainlink network so if they come across as basic questions my apologies.Before the implementation of OCR, not all nodes needed to report all data being reported to the network correct? And with the implementation of OCR, I’m assuming this is the same, where not all nodes/groups need to verify all types of data being reported?Basically, if we take @Larry_Bates example above of batch transactions, and let’s say the insurance industry decides to implement OCR for batch payment reporting, then only certain nodes within the OCR network would need to verify these transactions before the singular node reports the data correct? It seems like once new industries enter the ecosystem, and more use cases for Chainlink/OCR are implemented, we would start seeing fragmented groups forming to aggregate transactions/data within their industry specialization. Is this idea correct?If my theoretical implementation above is accurate, wouldn’t this potential fragmentation of who is aggregating data off-chain and then uploading it lead to easier attacks and data manipulation through off-line coordination?Again, if we take the insurance example above. If a conglomerate of insurance companies decided they wanted to manipulate data for their companys’ gain couldn’t they coordinate off-line, aggregate falsified data, and upload that data as one transaction? While I understand the cost-saving benefits of OCR it does seem to allow for malicious actors to upload falsified data easier/cheaper as well. How does this implementation impact the security of the data being reported? I believe @patrickalphac touches on this in his previous post a little but I was hoping to expand the conversation around this a little more.', 'Hi @tjd233, I can provide some insight on this topic. You are correct on the first point, Chainlink isn’t a monolithic network that requires global consensus like a blockchain, but is more like a framework for constructing independent and heterogenous oracle networks. This means nodes do not deliver data to every network, but only to networks that they explicitly have been added to. This ensures each node does not need to purchase API subscriptions to every data source that is needed by users (becomes expensive as the network scales), but nodes can instead specialize in the oracle services they provide. This enables support for permissioned data sources that not all nodes may have access to (e.g. enterprise backends, premium APIs). Oracle networks following a monolithic model can usually only support data sources that are accessible to all nodes like lower quality free open APIs. Chainlink has a blogpost on this topic that dives more into the network’s architecture topic more here 1.For the second point, there is a few considerations in regards to how Chainlink oracle networks are secured. The cryptoeconomic incentives of being an oracle node operator is derived from revenue and reputation. This includes the opportunity cost of losing all future revenue/profit in the oracle ecosystem if they’re malicious (requesters would remove that node from any network they serve due to the destruction of trust), a reduction in the node operator’s reputation (which could be significant for enterprise run nodes), the loss of revenue from external services a node operators provides (e.g. telecommunications, PoS validation pool services, blockchain DevOps services, etc), as well as the implicit stake node operators have in the network by getting paid in LINK tokens (whose value is derived from the health of the network and its security).These collective incentives raise the cost of attack, making it so it’s more profitable for node to be honest. It’s similar to the incentives that ensure Bitcoin miners are honest (they want to uphold their future revenue and the value of their mined BTC, while ASIC manufactures and exchanges want to uphold their reputation). In the future, security can be bolstered through the addition of explicit staking where nodes deposit LINK tokens as stake in service agreements, which can be slashed by the network for malicious or non-performance. These economic incentives combined with decentralization makes it harder from both a game theory and social coordination perspective to successfully pull off a collusion attack.OCR networks can also be considered as having a higher cost of attack compared the previous Chainlink oracle network version because while the OCR upgrade improve efficiency, the additional scalability allows for more decentralized networks to be created and larger profit margin for nodes, thus future a greater revenue. All of these factors together contribute to the security of oracle networks, but you do bring up a good point that security fragmentation should be considered when using an oracle. If there already exists a secure oracle network for the data you need, it’s likely better to contribute and join that network rather than trying to launch your own network.', '@Zach thank you for this detailed response and the added medium article. After reading both I think I have the answer I was looking for regarding my previous question.One main thing I took away from your post is how important the implementation of staking will be in the future for the security of the network. Your statement:In the future, security can be bolstered through the addition of explicit staking where nodes deposit LINK tokens as stake in service agreements, which can be slashed by the network for malicious or non-performance.seems critical to me to help ensure the validity of the independent/heterogenous oracle networks, especially given the implementation of OCR.', 'Yes agreed, I see explicit staking as a key aspect of scaling up network security in the medium to long term, particularly when the underlying smart contracts consuming this data begin to secure increasing amounts of value from traditional markets, which often involve trillions of dollars (compared to the billions in DeFi today). Future revenue and reputation are the initial foundation of oracle node security, but the staking of collateral significantly strengthens these guarantees, as well as further quantifies security for users (x amount of collateral gets you y amount of network security for a contract holding z amount of value).', 'Great analysis and discussion.Why is OCR 2 used in cross chain communication to generate the payload of the message?For example, if the contract on one chain wants to bridge to another chain both an asset and an instruction set for the destination chain, OCR 2 generates that message after receiving the original payload.There’s no computation off chain since the payload goes to the bridge contract.  It’s not a queryable api so multiple nodes asking thwarts byzantine.Is the benefit reliability that the transmission will be executed?']"
                Research Summary: Coding a DeFi arbitrage bot              ,https://www.smartcontractresearch.org/t/research-summary-coding-a-defi-arbitrage-bot/282,Tooling and Languages,42,['http://Extropy.IO'],"['arbitrage-bots', 'dex', 'defi', 'summary', 'flash-loans', 'oracles', 'zero-knowledge']","['TLDR:The author provides a detailed guide to coding a DeFi arbitrage bot. The bot uses flash loans to borrow assets from dYdX and sells them on 1inch exchange when profitable.CitationExtropy.IO 226. Coding a DeFi Arbitrage Bot, Medium. Oct, 29, 2020. Accessed on: Mar, 16, 2021. [Online] Available: Part 1: Coding a DeFi Arbitrage Bot | by Extropy.IO | Medium 1.1k, Part 2: Coding a DeFi Arbitrage Bot — part 2 | by Extropy.IO | Medium 698LinkPart 1: Coding a DeFi Arbitrage Bot | by Extropy.IO | Medium 1.1kPart 2: Coding a DeFi Arbitrage Bot — part 2 | by Extropy.IO | Medium 698Core Research QuestionHow can an arbitrage between DEXes be automatically performed using flash loans?BackgroundArbitrage is the purchase and sale of an asset in order to profit from a difference in the asset’s price between marketplaces.Price slippage refers to the difference between the expected price of a trade and the price at which the trade is executed. It usually happens when the market is highly volatile within a short period of time or when the current trade volume exceeds the existing bid/ask spread.Flash Loan is a type of uncollateralized loan that is only valid within a single transaction. It can be implemented through a smart contract. The transaction will be reverted if the execution result is not as expected.For more details on flash loans, please refer to the research summary “Attacking the DeFi Ecosystem with Flash Loans for Fun and Profit 212”.Decentralized exchanges (DEX) are a type of cryptocurrency exchange which allow peer-to-peer cryptocurrency exchanges to take place securely online, without needing an intermediary.DEX aggregators source liquidity from different DEXs and thus offer users better token swap rates than any single DEX.Liquidity pool is a collection of funds locked in a smart contract to provide liquidity for DEXes. The advantage of a liquidity pool is that it doesn’t require matching orders between buyers and sellers, and instead leverages a pre-funded liquidity pool with low slippage.An Orderbook consists of a collection of bid-and-ask orders. Orders are matched and executed only when a bid and ask price are the same.An Automated Market Maker (AMM) uses a liquidity pool instead of an orderbook and relies on mathematical formulas to price assets. The assets can be automatically swapped against the pool’s latest price, making it more efficient than traditional orderbooks.Wrapped ETH (WETH) is the ERC20 tradable version of Ethereum. WETH is easier to trade within smart contracts than ETH is. Users can also revoke access to their WETH after sending it to an exchange, which is not possible with ETH.SummaryThe author describes the advantages of DeFi arbitrage over centralized exchanges:DeFiInsolvency risks are minimized as smart contracts execute automatically following predetermined parameters. A trade will be reverted if it cannot be executed as expected.Traders can perform arbitrage using borrowed funds with flash loans.Centralized exchangesSince a trader cannot execute trades simultaneously, they may suffer from price slippage if a trade is delayed.Traders need to own funds or borrow them from a bank.Arbitrage between DEXes that use AMMPopular platformsKyber Network 35, Uniswap 12, Balancer 14, and Curve Finance 12.ResultBring prices into efficiency between two liquidity poolsScenarioWhen the pools on different DEXes offer different prices to exchange assets.ExecutionExchange from asset A to asset B on one pool and exchange it back on another pool to benefit from the price spread between two pools.Arbitrage between DEXes that use classic orderbookPopular platformsRadar Relay 75, powered by the 0x protocol 33.ScenarioTraders can fill limit orders from a DEX and then see if the tokens acquired could be sold to any other liquidity pools for better returns.The author describes the basic operation of an arbitrage bot.For example, to arbitrage the pair WETH/DAI:The bot will query the 0x API 78 looking for WETH/DAI pair limit ordersThe 0x API can get the limit orders for a currency pair from every exchange that uses the 0x protocol.Example API url for orders buying WETH with DAI: https://api.0x.org/sra/v3/orders?page=1&perPage=1000&makerAssetProxyId=0xf47261b0&takerAssetProxyId=0xf47261b0&makerAssetAddress=0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2&takerAssetAddress=0x6b175474e89094c44da98b954eedeac495271d0f 184Parametersthe maker token’s contract address (WETH)the taker token’s contract address (DAI)Sample response from the above request1600×672 245 KBThe DAI price of 1 ETH = takerAssetAmount / makerAssetAmountThe bot will then query the 1inch exchange 40 DEX aggregator to determine if there is any open order that could be sold for a higher price on any other liquidity pool.Reason for using 1inchIt offers the best rates by discovering the most efficient swapping routes across all leading DEXes.GoalGet the DAI price for 1 WETH.Detailed stepsExecute the getExpectedReturn function of the 1inch smart contract function on Etherscan 16.ParameterInputfromToken: WETH contract address 11toToken: DAI contract address 5amount: the number of WETH we are selling, followed by 18 decimal places.OutputreturnAmount: this number divided by the input “amount” parameter is the DAI price for 1 WETH.Arbitrage may take place if this amount is greater than the one returned by 0x.690×996 46.2 KBHowever, the above opportunity may not actually exist in practice, because we would need both limit orders to exist at the same time for the arbitrage to work.The author thus proposes to perform the arbitrage using a flash loan.StepsGet a flash loan DAI from DyDx exchange 66Buy WETH from 0x using the DAI borrowed with the flash loanUse 1inch to find the best exchange to sell the WETH acquired in step 2Pay back the flash loan DAI and keep the remainder as profitThe author provides the code and explains how the arbitrage smart contract worksThe contract inherits the DyDxFloashLoan smart contract.FunctionsSwapPerform the tradegetExpectedReturnGet the current asset pricegetWethTurn any ETH sent into WETHapproveWethApprove the 0x proxy to collect WETH as the fee for using 0x.getFlashloanCalled whenever a profitable arbitrage is found by the client.All of the parameters for this function will be constructed and passed in from the client script.callFunctionHas to be deployed in our smart contract to receive a flash loan from dYdX._arbArbitrage function that is called when the loan is successful.Tracks the balance of our smart contract before and after the trade.If the end balance is not greater than the start balance, the operation will revert.MethodThe prerequisite for the arbitrage bot is to have a browser with the Metamask extension installed.The programming language used is NodeJS.Project structure: only two filesindex.jsa node.js server that continuously fetches crypto prices on exchanges looking for arbitrage opportunities, trying to guarantee that the trade is possible before even attempting to execute it.TradingBot.sola smart contract that gets called by the node app only when a profitable arbitrage is found, it will borrow funds with a flash loan and execute trades on DEXes.Detailed setupInstall Metamask browser extensionCreate an Ethereum Mainnet account with some ETH for paying gas fees.Don’t use your personal account for this, create a new account for the bot in order to limit accidental losses.Go to Remix online IDE 52 and paste the smart contract solidity codeCompile the code using compiler version 0.5.17.Deploy with an initial 100 wei, which is enough for 100 flash loans on dYdX.Environment setupBy cloning the project’s code repository, users will find a file called .env.example inside the /src folderFill in the fields:RPC_URL : the public address of an Ethereum node, the easiest one to set up is the Infura RPC provider, register an account in order to get an API key.ADDRESS and PRIVATE_KEY: fill in the public Ethereum address of the bot account, and its corresponding private key.CONTRACT_ADDRESS : paste in the smart contract’s address that was returned from Remix after the deployment step.GAS_LIMIT : how much gas the contract is allowed to use, leave as 3000000 or decrease to 2000000 which should be fineGAS_PRICE : change this depending on how fast you want the transaction to be mined, see https://ethgasstation.info/ 21 for info.ESTIMATED_GAS : leave as isRunning the botExecute the command from the project’s root directory.node src/index.jsResultsThe full working code can be found at GitHub - ExtropyIO/defi-bot: Tutorial for building DeFi arbitrage bots 929.Discussion & Key TakeawaysThe author created an open-source DeFi arbitrage bot that uses flash loans to borrow assets from dYdX and sells them on 1inch exchange when profitable.The author explains the main components of the arbitrage bot and the underlying logic of how arbitrage works.After following this tutorial, users can create a working example of a customizable flash loan arbitrage bot.The most efficient way to perform a flash loan arbitrage is to continuously fetch the real time prices using NodeJS client and execute the contract with profitable parameters when an opportunity is found.Implications & Follow-upsArbitrage is a zero-sum game. There are a finite number of arbitrage opportunities for a large group of people competing to find and execute them.To make the bot more efficient, the author suggests the following improvements:Consider taker fees when calculating profitsUse Partial fillsCheck orders againHandle failures to continue executionExecute multiple orders simultaneouslyDynamically calculate gas feesIf such arbitrage bot becomes prevalent, the price differences between different DEXes will be minimized. DEX aggregators such as 1inch may no longer be needed as the price differences become more and more negligible in the future.It may be interesting to measure the actual APR of running this bot, considering the cost of server hosting and contract deployment.ApplicabilityInterested readers can refer to the working code to have their own arbitrage bot: GitHub - ExtropyIO/defi-bot: Tutorial for building DeFi arbitrage bots 929.Currently, the example only supports flash loans from dYdX. Users can add other flash loan provider’s support.', 'Always fun to see so many edgy concepts combined in a single bot, especially the use of DEX aggregators in conjunction with flash loans.Given the historical volatility in gas prices, seems like a dynamically adjusted GAS_PRICE field would be a requirement for this bot to survive in the long run. A predefined gas price of 200 provides a good cushion at current levels, but the bot would likely be priced out in times of high gas vol.If unsupervised, such a bot could ultimately lead to losses if transactions are continuously priced out by more sophisticated arb traders. If the bot’s creator does not adjust the GAS_PRICE field quick enough, transactions will start failing and the contract’s ETH drained. Extropy is a fun PoC, but experimenters beware! ', 'Has anyone published the results of running the bot? Even if it’s consistently not profitable it would be useful to understand why.In Flash Boys 158 they setup multiple globally distributed nodes in order minimize the impact of latency a node can experience. Any consideration given to latency?Given that price movements create arbitrage opportunities are there any studies examining the relationship between price volatility and gas prices?', 'Can you give us a sense of how important these bots are to the DeFi ecosystem? Are they simply a nuance or do they provide some kind of stability to the system? I’d also be really curious about how arbitrage bots and trading bots fit into the debate about Miner Extractible Value.', 'Most DeFi applications rely on arbitrageurs to function properly so these trading bots are critical for markets to reach price efficiency.When it comes to node latency, that seems to be of less relevance in recent times because of new approaches to MEV. Don’t get me wrong, it’s still important to have sufficient data on the different corners of the network’s topology. Since the mempool functions as an auction, you need to know if another arbitrageur outbid your tx and whether it’s worth it to engage in a bidding war.However, it has been observed that new approaches to MEV 60 entail sending transactions directly to the mining pool and issuing payments out-of-band (without even sending it to the mempool). That circumvents the need for low latency as miners will receive your transactions directly, thereby circumventing the need to engage in bidding wars.', 'What other non-human (or quasi-) human actors are lurking in the DeFi space, @tina1998612? Seems like a forbidding environment for a retail investor to enter. Are there any other takeaways from Flash Boys and Towards Understanding Flash Loan and Its Applications in DeFi Ecosystem 40 that you think might be pertinent to this discussion?@Larry_Bates had some really interesting thoughts about anonymity and identity, and who might be lurking within the dark forest…@Barry where do you think all of this ends up in five or ten years?', ' jmcgirk:where do you think all of this ends up in five or ten years?Hopefully some of the research and development today around scalability will be online in the next five years to increase the transactional capacity of the network(s). Arbitrageurs can then continue to ensure prices are properly reflected across markets with less impact on cost of transaction execution globally.', 'This arbitrage bot works specifically for 0x and 1inch exchanges. How much tweaking would something like this need for a different exchange? Or what about a non-Eth-based DEX? I wonder what will happen now that the code is out in the wild, I guess in theory it’ll make arbitrage unprofitable on those DEXes. I wonder how all of this parallels with the introduction of high frequency trading in TradFi.', 'I’m also interested in Miner Extractable Value to include this framework. DeFi Arbitrage bot is a very funny thing to implement, but it does not guarantee profitable exclude some parameters in this method such as Miner Extractable Value, which means your transaction may not be successfully taken by the miner. And the different DEX may have different MEV to calculate.', '@kanad had a question in our chat: “What is the underlying technical difference between bots and flashbots?” @Larry_Bates you had a great response, but perhaps @tina1998612 you might like to weigh in as well @jyezie', 'We recently wrote an article on the MEV Crisis with some solutions. Here’s the link: https://extropy-io.medium.com/illuminating-the-dark-forest-748d915eeaa1 100', '@Kirsty_G - thank you so much for the update, looking forward to reading it! @tina1998612 - I’d be really interested to see what you think of this.', 'Will thisCoding a DeFi Arbitrage Bot | by Extropy.IO | Medium  25, Part 2: Coding a DeFi Arbitrage Bot — part 2 | by Extropy.IO | Medium  24work as is, given that there’re front-run bots out there? Or will it necessitate some tweaks first?', 'Yes it requires tweaks, we did find that it was being front-run', 'What would be a possible fix? Would running it via Flashbots fix it?', 'According to flashbots yes, overview | Flashbots Docs 132']"
                Post Idea: Tornado Cash              ,https://www.smartcontractresearch.org/t/post-idea-tornado-cash/366,Post Ideas,23,[],"['post-idea', 'summary', 'dao', 'privacy', 'network-security']","['Paper / Discussion TitleTornado CashLink to sourcehttps://tornado.cash/ 2https://tornado.cash/Tornado.cash_whitepaper_v1.4.pdf 11https://tornado.cash/audits/TornadoCash_cryptographic_review_ABDK.pdf 5https://tornado.cash/audits/TornadoCash_circuit_audit_ABDK.pdf 1[ZKP 讀書會] Tornado Cash | Taipei Ethereum Meetup 2Content type tag (summary, discussion)discussion? Apparently not a direct summary of the tornado cash whitepaper.CategoryPrivacyProposed tagsZero knowledge, privacy, anonymityDescription of why this would be an interesting postTornado cash uses zk merkle tree opening/path proofs as its foundation, like zcash.This post shall introduce tornado cash as a native way to unlink any previous transactions on Ethereum mainnet.It can be a great way (popsci) for people to have a better understanding on how to use mixing/shielded transaction solutions correctly i.e., without voiding the security benefits that the project provided, on Blockchain.The usage of such…I can think of one.If your legal dept ask your ethereum address to be absolutely clean i.e., without previous transactions - because they worried that the funds that were in/out of the address could be illegal or against the tax law - you may have a slim chance to persuade your legal dept that: since any transaction out from tornadocash contract are (under certain assumptions) unlinkable to any of the input transaction, the funds are clean now.Just an imaginary scenario though.Links to background reading (0 to 3 items)Research Summary: Zerocash: Decentralized Anonymous Payments from Bitcoin', 'Thanks for the thorough post idea, @Jerry_Ho. I think a summary on Tornado cash could be interesting. Mixers have been underutilized in Ethereum because of how easy it is to trace (and taint) balances under the account model.However, there have been some promising improvements in this area. A paper came out on Research Pulse a couple of weeks ago describing an interesting non-interactive coin mixer for account-based chains: https://eprint.iacr.org/2021/327.pdf 3We could follow the same approach as Plonk and write a summary citing different sources in the “background”, “key takeaways”, and “implications” sections. What do you think?', 'I do agree with your point: Ethereum is inferior for privacy, in a sense. (worse than UTXO model for bitcoin, for example.)I breifly skimmed Veksel, and I think it’s a good work:It doesn’t assume itself to be Ethereum onlyNo restrictions on anonymity set sizeA fully? homomorphic commitment scheme, sounds fun and (cryptic to me).That’d be a great summary to be written, but I don’t think my summary would be beneficial to the community - an ideal way would be to walkthrough the maths used in Veksel, and guide readers in the process, as Vitalik did in his (many) blog posts. This is especially good for this kind of protocol article cause people (engineers) just need a research guy to explain the math, step by styp, to them, in an implementable fashion, thus they can build a library for it.I don’t have enough background nor knowledge doing so at the moment, but I’ll definitely find someone to do a pair reading in the future.Meanwhile, if we were to limit the scope of the summry on “comparision of mixers on Ethereum” as a discussion post, I do have some in mind:(2019) GitHub - EYBlockchain/nightfall: Nightfall protocols for private transactions on the Ethereum blockchain using zk-snarksEY(accounting firm, the big 4), no updates recently as you can see. Could be that the projects moved to their own chain, rather than Ethereum-based with ZoKrates.(2019) https://twitter.com/ZeroPoolNetwork 2Zeropool, from ethDenver. No actual prototype at the moment. But at least they’re still there, I saw some of my EF friends following their twitter https://twitter.com/ZeroPoolNetwork 2(2019) ZkDAI | DevpostZkDAI, there was a repo: GitHub - atvanguard/ethsingapore-zk-dai: ZCash like private DAI transactions on ethereum using ZkSnarks. MakerDAO API prize winner. 1 However the author retreated to defi. https://twitter.com/defidollar(2021) https://zk.money/zkmoney, to be honest I can’t find shit, documentations, faqs, whitepapers, blogposts, no no thing. It seems like under rebranding, or just a small, MVP part of the whole aztec layer2 network (zkassets), for people to experience and experiment with cause the former is not online yet. This General - zk.money and this AZTEC DocsWell so here we are, having tornado.cash as the only active and usable mixer project. (2020) No wonder I saw some ppl complaining here: Ethereum (ETH) MixerI could add this paper into the summary/discussion post of Tornadocash, if you think the original direction is not technical enough:(2020) https://arxiv.org/pdf/2005.14051.pdf 1https://twitter.com/istvan_a_seres/status/1266192703307632649?s=21Check section 6 and section 7:圖片1155×369 44.6 KBHonestly, I have 0 idea where to find all the living and ongoing projects of sorts(mixer/zk shielded transaction natively on Ethereum). Please kindly inform me if I did not mention some famous mixers, before I start writing and researching on the discussion post.', ' cipherix:Mixers have been underutilized in Ethereum because of how easy it is to trace (and taint) balances under the account model.Mixers like tornado cash address this. I think their lack of usage is due to how expensive SNARK operations are currently on Ethereum.A transaction fee equivalent of upwards of ~$125 to enter the mixer and a similar fee to later exit makes such transactions only palatable for large transactions.Ethereum (ETH) Blockchain ExplorerEthereum Transaction Hash (Txhash) Details | EtherscanEthereum (ETH) detailed transaction info for txhash 0x9046e23242b9fe6f9c8fc14fc8bf42867185c917dd922e1d2445547274d84cc8. The transaction status, block confirmation, gas fee, Ether (ETH), and token transfer are shown.', ' Jerry_Ho:Ethereum is inferior for privacy, in a sense. (worse than UTXO model for bitcoin, for example.)Perhaps there is a paper that explains this? I would be interested to know why the account model is inferior to UTXO when it comes to privacy.', 'Yeah, this one did that exactly:arxiv.org2005.14051.pdf 41301.69 KBAside from the discussion on TornadoCash in Sec6 and Sec7, the whole paper is trying to answer your question.Introduction:圖片515×816 108 KBJudging solely from the paragraph in the introduction: I’d say he has a point. It’s not even a security assumption thing - people really tend to reuse address even with deterministic generatable? wallet standard implemented.And the paper gave a practical result on fingerprinting users with their addresses, although I haven’t check its methodology yet.', 'Blockchain is Watching You: Profiling and Deanonymizing Ethereum Users 3 is an excellent suggestion! It touches upon a lot of these trade-offs and empirically analyses tornado cash.', 'K, I think I can write this one instead, and put aside mixers/tornadocash at the moment. Will create another thread/github issue.Just one quick question:I’m not too familiar with the network stack of internet. While PERIMETER feels like targeting transport layer, network layer, and datalink layer attack/analysis, it seems like that “Blockchain is Watching You:  Profiling and Deanonymizing Ethereum Users” is solely focusing on application layer analysis and fingerprinting (without active attack).So there’s no need to compare the two in the incoming summary, amirite?', 'Broadly speaking, privacy attacks on blockchain networks tend to fall into 2 categories. “PERIMETER” is an excellent example of a transport layer attack whereas “Blockchain is Watching” provides a good background on transaction graph linkage attacks.Covering “Blockchain is Watching” could provide a pathway for us to discuss the predominance of both attack types.', 'I have a question what will happen if the merkle tree is full?', 'Welcome to the forum, @HowJMay!It would take a gigantic number of mixing rounds for it to become an issue if the hash function used is believed to have what is called “preimage resistance”. Here’s a good resource to learn more about it:crypto.stackexchange.comWhat are preimage resistance and collision resistance, and how can the lack thereof be exploited? 2hash, collision-resistance, security-definition, preimage-resistance  asked by        John Gietzen    on 01:07AM - 12 Nov 11 UTC']"
                CommunityRule: What should an interface for designing governance do?              ,https://www.smartcontractresearch.org/t/communityrule-what-should-an-interface-for-designing-governance-do/952,Governance and Coordination,29,[],"['dao', 'discussion', 'governance', 'summary', 'privacy']","['The user experience for most organizational governance is pretty lousy. In meatspace, it generally consists of impenetrable legal prose that, in order to be precise, loses meaning for most people who have to use it. Even fairly simple bylaws are, at best, boring.In crypto, the problem is different but related. Interfaces like those of Snapshot and Aragon make governance seem like a deceptively simple token vote, without disclosing the layers of process occurring off-chain in chat threads, forums, and DMs.Beginning in 2020, I and a few others have been working on CommunityRule 9, a Web app experimenting with what a better governance interface might look like. The goal is to explore how designing community governance might actually be fun, accessible, and transparent. As with open-source software, designers should be able to share their designs with others and fork the work of other designers, adapting it to new contexts. As a result, a design commons might emerge that allows more rapid exploration, iteration, and knowledge sharing. As more and more governance occurs through software, also, interfaces should also be capable of outputting usable code.With the support of SCRF, and in partnership with SQGLZ, we are now developing a new chapter in this work, which we will be reporting on here at SCRF. Our goal is to develop a third iteration of CommunityRule.The first iteration was a text-based tool that asked users a set of questions and invited answers in prose. This was simple to develop, and it attracted some interest, but it was only a modest step beyond the interface of standard textual bylaws, and the questions could be constraining.cr-create925×279 14 KBThe second iteration replaced the questions with drag-and-drop modules. Users can now choose modules from menus, add them to their rule, and specify their contextual meaning in text. Modules can be inserted into other modules. This presents a more visual authoring and reading experience. But many users seem to have had difficulty understanding the modules, finding the right ones, and customizing them. User adoption remains quite slow.The protoype works, but it has massive limitations. There are no user accounts; published rules can only be deleted by an administrator. There is no real data model (other than raw HTML!), so the rules are not very portable. We have put off such niceties until the core experience is creating the kinds of user response we’re hoping for: playful authoring, sharing, and building on others’ work.I don’t know if we’ll get there in this third iteration, but I hope we can get closer. Here are some possible goals to work toward, now or later—and I would love to hear feedback on what seems most urgent or useful.Reorient the design from a structure-based approach to a more subjective one based on actions and choices and agreementsIntroduce gamification and interactivity to make authoring a more playful experienceCreate more space for culture and other elements of governance outside of rules and structuresRethink the layout to make it easier to find and choose modulesReplace the linear structure of the authoring space with something more 2D, enabling more relational representationsEnable diverse, custom module sets for specific applications (eg, a certain organization might present a certain limited set of modules to its chapters)Integration with PolicyKit, enabling rules to translate into working code for governance in popular online platforms, or integration with a DAO governance platform like Boardroom or AragonWhat do you think? What else should we be exploring?We are developing the code here 3, and we invite more collaborators. The app is based on Jekyll and JavaScript, with a Google Sheet for a database. If you would like to get involved, please reach out.', '@ntnsndr thank you for sharing this with us. @kelsienabben I figured this tool might be something you’d be interested in (as I’m sure @stephaniev you would as well, particularly from a UX/user interface perspective or @Fizzymidas or @Astrid_CH, from a legal perspective).My question is a little silly – is there an optimal amount of time that an average citizen or user should spend thinking about governance? In the project management world, I’ve heard the rule of thumb is that more than ten percent of time spent on organization and planning is usually a waste… Is there an equivalent for governance? (Maybe this is an argument for occasionally using some of the less democratic templates in your arsenal.)', 'I love that question about optimal time. I can’t imagine that there would be. But I’m intrigued by the question of what a personal governance dashboard would look like, where one can see one’s various communities and make choices about how best to channel one’s limited attention. There are projects like Tally and Snapshot that offer something like that, or a more social version like Ethereum.world. What do you think?', 'I like that. A portfolio tracker but for all the DAOs you belong to. The more social option is particularly interesting. I’m talking to an artist friend about creating a self-portrait DAO. One of our proposed rules is that every new piece of work has to be unanimously approved, so we need to figure out how to run a portfolio review/showing for the community. I think Ethereum.world could work. Of course, when I think of interacting with DAOs, I can’t help but imagine something a little sinister and uncanny like Bruce Sterling’s Maneki Neko 1.', 'Thanks for the tag! It wasn’t until I saw CommunityRule in practice that I realised its utility.A group of MetaGov colleagues and I were figuring out management of a common resource (a multisig wallet) and were brainstorming the rules of how we might spend funds for a grant program, and what kind of consensus we needed amongst the group. It was actually Nathan who whipped up the discussion into a CommunityRule framework that I understood how handy it is as a tool and framework. I recommend playing with it to describe an existing system or apply it to a new situation', 'I wanted to post some of my work relative to Decentralized Conglomerates to give some background as to why people like myself saw inherent problems with the DAO theory from the very beginning:Bitland Global: Updates on South Africa, Mauritius, Nigeria, Uganda, Ethereum Compatibility — Steemit 1""Institutional memory becomes an imperative when it comes to keeping a unified organizational evolution moving. An example of institutional memory would be “Congress” or the “Supreme Court” in the United States government. The idea was that instead of having agnostic principles that were to have authority over a specific set of rules or actions, democracy would be applied to congress to decide a group of elected officials that would attempt to balance the desires of the people against the knowledge of the institutional memory of congress.In the case of the Supreme Court, the idea is that a group of officials that are appointed by an elected official to represent balanced views of the country will also retain the institutional memory necessary to make informed decisions about establishing new rules and laws. In this context, having institutional memory reduces the necessity of retreading debates and theoretically is an attempt to move debate forward with the knowledge of everything that has occurred previously.When DAO theory was emerging, the concept of Digital Leadership had not quite been articulated, and the theoretical foundation of the DAO was ultimately rooted in a Mandate of Heaven that created a leaderless system where organizations were to function based around goals and objectives that were agreed upon, rather than the decision of a specific individual or group. Many attempts at creating DAOs have been attempted, with the recent Slockit DAO being the largest on record. While the Slockit DAO was the largest, the success of the project is debatable depending on what metric of “success” is being discussed.""', 'Thanks for posting this! It looks like a great conceptual framework and definitely needed in the governance space, especially as people who haven’t been a part of teams or have any experience of governance really figure out how to do it. And good interfaces can give them the bumper rails to do so…I would consider modern management frameworks as well. There’s a difference between governance (voting) and actually getting work done. The DAO has to accomplish both. Maybe that’s an integration with git’s PM flow or other PM applications, but a dashboard that connects those two facets puts governance in context. ntnsndr:Create more space for culture and other elements of governance outside of rules and structuresI’m interested in the cultural component you have listed. Culture is really important to create and what makes each of these DAOs unique. I actually think this should be the biggest component of a interface for DAOs. Culture tells a user how to engage here, what’s allowed and how, what we believe in, and overall what it means to be a part of this community. What are you @ntnsndr thinking about how to create a user experience for culture?I would think the culture component could include:Language (What will we communicate in? What language might we exclude?)ValuesRelationships (How do we not want to engage with each other? Do we need to define how we do?)Mindsets (How are you asked to show up to this space?)Rituals (What are our regular engagements?)Ceremonies (How will we celebrate? What does success look like here? How do we know we’ve achieved it?)Hygiene (How do we take care of this space together?)Crisis/Risk (What do we define as a threat and how do we manage it when it happens?)Metrics/Dashboard (What is the makeup of this DAO [dependent on what personal data people provide]: Where are people from, average length involved, could also include protected classes like gender/age/race…I think you need these things to ground people. Though it may be in conflict with web3’s values of psuedo-anonymity/anonymity.)Would love to know if I’m off on this or if this is the kind of input you’re looking for.', '@valeriespina that was a really wonderful reply, I’m really curious to read @ntnsndr reply. You also made me think of de-onboarding people from communities… how do you remove people from certain responsibilities or roles they once inhabited without rupturing all of those carefully created social ties. Can we automate our way around etiquette? And what happens to the rest of the community, do they grieve a lost DAO member?', 'Thanks so much for this—really interesting ideas.First, the distinction at the start is something I’ve been thinking about a lot; you frame it as voting vs. work, I’ve thought of it as decision vs. action. Compare, for instance, Aragon (decision/voting) and Colony (action/reputation) as examples of this. I am interested in the relationships among them, as well as the underlying assumptions of these approaches.Your culture suggestions are really great. We already had modules for Values and Rituals under culture (see the list at communityrule.info/modules/), and I think several of the things you suggest can fit under those. Friendship is also there, which might include Relationships. Perhaps friendship should be replaced with relationship. Language might fit under Norms?I’m just trying to be careful about not veering into module bloat, as I already feel the list is overwhelming for some users.@jmcgirk There is also a module under “process” for Exclusion, which is meant to cover removal.', 'Thank you so much for your reply, I’ll take a look at the exclusion module. Given your earlier post about Cryptoeconomics as a limitation on governance 1, I’m curious whether CommunityRule might eventually include tools for designing tokenomics too. It would be really interesting to have access to pre-built designs and how they might reinforce or modulate existing structures.', 'Potentially. Though unlike the related Token Engineering Commons project, and in keeping with the spirit of the paper, CR is taking an approach that focuses on political governance more than tokenomics.']"
                Research Summary: Smart Contract Security: A Practitioner’s Perspective (2021)              ,https://www.smartcontractresearch.org/t/research-summary-smart-contract-security-a-practitioner-s-perspective-2021/877,Auditing and Security,35,['#background-4'],"['testing', 'summary', 'network-security', 'defi', 'flash-loans', 'scalability']","['TLDR; Smart contract engineers are required to be more aware of security issues than regular software engineers. They use multiple tactics to address security issues and continually address them throughout the lifecycle of smart contract development.Smart Contract Engineers keep up to date with the latest security trends and hacks via Q&A websites (47%), blockchain forums (60%), and research papers (53%) [p. 1416].Seventy-two percent of smart contract engineers surveyed used more than one security strategy in smart contract development, including code review, code style checking, reuse of code from reliable sources, and integration of fuzzing into the development lifecycle.The most commonly used tools for smart contract security were Mythril (19%), Oyente (12%), SmartCheck (14%), and Slither (12%).Future work may seek to do three things:(1) systematize smart contract security approaches for the industry;(2) identify common smart contract libraries for reuse of snippets of validated and debugged code;(3) investigate various blockchains and coding tools through comparative analysis lenses for platform-level vulnerabilities.Core Research QuestionFor smart contract engineers, what security practices are common throughout the lifecycle of smart contract development?CitationZ. Wan, X. Xia, D. Lo, J. Chen, X. Luo, and X. Yang, “Smart Contract Security: A Practitioners’ Perspective,” 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), 2021, pp. 1410-1422, doi: 10.1109/ICSE43902.2021.00127.BackgroundThe intended audience of this survey was smart contract auditors who are curious about general tactics. It was not intended to be a rigorous deep dive into methods and software development lifecycles.Smart contracts are an appealing decentralized application technology for enabling computation on top of blockchains in response to off-chain events.Smart contracts consist of executable code that enforces the terms of an agreement between untrusted parties on a blockchain.Smart contracts are plagued with security issues. For example, The DAO was hacked for USD $34 million worth of Ether. (For more information on the latest hacks, please visit the rekt.news 2 site.). Researchers have proposed language-based security, static analysis, and runtime verification to address security issues.Academic research, before this article, did not cover practitioners’ tactics for addressing those security concerns.Using a mixed-methods approach, the authors conducted 13 qualitative interviews with smart contract engineers and deployed a quantitative survey of 156 smart contract engineers to gather intel on their security practices during development.SummarySmart contracts are executable code built on top of blockchains. They are extremely useful for executing agreements between untrusted parties, and promise to be a key enabler of social automation in the 21st century. However, smart contracts remain rife with security bugs that can result in very costly exploits. Smart contract engineers care about protecting their users as well as their companies’ reputation for secure code. The security issue mitigation tactics used most commonly are code reviews, code reuse from reliable sources, and fuzzing tools incorporated into engineers’ continuous integration/continuous development process.Keeping up to date with security issues is a challenge. The most common sources of information for the latest security news according to the paper are blockchain forums, question and answer websites, and research papers.Practitioners are using more than one tactic for securing their smart contracts, but are in need of a way to systematize an industry standard approach to security within their development process, as none currently exists. Future work may seek to do three things: (1) systematize smart contract security approaches for the industry, (2) provide common smart contract libraries reused and snippets of helpful code, and (3) investigate the different blockchains as well as different tools through comparative analysis lenses.MethodsThere were two methods used: qualitative interviews (16 participants) and a survey (156 participants) deployed.Qualitative Interview Methodology:Three categories of questions were asked:(1) Asked demographic questions(2) Asked open-ended questions about security awareness and practices of smart contract development.(3) Asked interviewees to discuss their sources for security-related knowledge, strategies, and tools for smart contract security management.Qualitative interviews used to open card sort and determine a list of statements and potential answers for the developing survey questions.Survey Methodology:Includes multiple-choice and free-text answer questions. Multiple-choice questions derived from qualitative interviews.(1) Asked demographic questionsDo you have experience with smart contracts?What best describes the primary blockchain platform you currently work on? Potential answers were: (1) public blockchain (2) consortium blockchain, (3) private blockchain, and (4) other.Used 5-point Likert Scale ( strongly disagree, disagree, neutral, agree, strongly agree) to rank the importance of each questions in examining the following categories:the importance of security for smart contractssecurity motivators and deterrentsefforts towards security of smart contractsexperience of security problems with smart contractssources of security knowledge related to smart contractsstrategies for addressing smart contract securitytools used to detect security vulnerabilities in smart contractsResultsWho took the survey?Most respondents were based in China (61 participants) and the United States (16 participants).They averaged two years experience with smart contracts per respondent.Most respondents to the survey worked on public blockchains (80) such as Ethereum.Results are divided into 3 categories:(1) Perceptions of Smart Contract Security,(2) Security Practices in Smart Contract Development,(3) Effect of Blockchain Platforms on Security Perceptions and PracticesThese following are the results gained divided into the above 3 categories.RQ1: Perceptions of Smart Contract Security:Importance of Security85% rated security very important69% rated security extremely importantSecurity Motivators“I care about smart contract security because…”protect usersprotect the reputation of their companiesSecurity Deterrents40% responded they had had at least one out of three potential security problems:vulnerabilities in unshipped codevulnerabilities in shipped codesecurity breachesSecurity problems occurred when?Before code shipped (22%)After code shipped (19%)Security breach after code shipped (10%)Sources of Security Knowledgeblockchain forums (60%)research papers (53%)question and answer website (47%)support from professional security experts (47%)RQ2: Security Practices in Smart Contract DevelopmentEfforts toward Security29% of time overall developing smart contracts14 respondents indicated that no time was spent on security.Strategies for Handling Smart Contract Securitystrategies for handling smart contract security894×534 202 KB72% of respondents use more than one tactic for security mitigation72% rely on code review often or very often61% do code style checking58% reuse code from reliable sources28% incorporate fuzzing into the development lifecycleFree text answers included:Security By DesignProgramming LanguagesDependency ManagementLearning from Past ExperienceSeeking Support from ExpertsTools to Address Smart Contract Security54% adopt smart contract security tools in their development45% rely on security plugins in IDEs19% use Mythril12% use Oyente14% use SmartCheck12% use SlitherRQ3: Effect of Blockchain Platforms on Security Perceptions and PracticesSecurity Motivatorspractitioners of public blockchains are more motivated to address smart contract security concerns than those of consortium or private blockchains.Security Deterrentspractitioners of public blockchains are more likely to take responsibility for addressing smart contract securitySecurity Efforts across StagesPractitioners of public blockchains spend more effort towards security throughout the six stages of the development lifecycleSecurity Strategiespublic blockchain practitioners tend to perform code reviews more frequently than consortium or private blockchainsDiscussion and Key TakeawaysSmart contracts are more prone to attacks than regular software.More frequent code reuse specifically exposes more security risks than in regular software development.Most smart contract engineers use more than one method of securing their contracts throughout the lifecycle of their development.Implications and Follow-UpFuture work may be standardization and operationalizing the process of building security into smart contracts for practitioners.Future studies may include code snippets and smart contract libraries with helpful examples and tools to facilitate library updates.Future studies could investigate the differences between blockchains, as well as examine and detail the differences between different tools used for smart contract security.ApplicabilityAs a smart contract engineer, more than one method of security smart contracts throughout the lifecycle of development should be used.Incorporate code reuse, code review, and fuzzing tools into your development cycle for security best practices, according to this study.Security tools most commonly used by smart contract engineers are Mythril, Slither, Oyente, and SmartCheck.Be mindful of blockchain platform limitations, language design limitations, and smart contract specific potential attacks as outlined in codified references such as the SWC Registry 3 or the DASP registry 3.', 'Hi, Max. Thank you for contributing.I am curious why you might think the division among smart contract engineers regarding security tools is so varied. I say this because it seems that naturally 1-2 tools stand out in general but not in this case (aside from the IDE plugins).When you refer to standardizing/operationalizing security building, do you have any examples of this already taking direction in the status quo?If not, what are the industry factors that are preventing systems from doing so?', 'I’m interested in your thoughts about the claims versus the sample sizes for the survey and interviews. The cautious researcher in me would be putting caveats with the findings and suggesting some follow-up steps of larger sample sizes and maybe longitudinal surveys…', '@maxgrok it’s a real pleasure having this in the forum, and thank you so much for contributing this interesting summary. A quick question to kick things off: can you provide us with some context for what the crypto auditing and security landscape looks like at the moment? Are there any institutions devoted to ensuring standards or trade organizations?', '@maxgrok, many thanks for submitting this summary to the forum. Obviously, the subject is extremely important. For me, this statement jumped off the page: maxgrok:Practitioners… are in need of a way to systematize an industry standard approach to security within their development process, as none currently exists.Because smart contracts promise widespread social automation, I would think that code security in this domain would be at least as important as that of, for example, autonomous vehicles. Competition and different approaches exist in that field, too, but the idea that an adversary might take over the steering or accelerator of multiple cars is a sobering prospect indeed. No one would say that “an industry standard approach to security” doesn’t exist for autonomous vehicles.So how do you account for this situation in the field of smart contract coding? Is the field itself (or this study) simply too immature to base any real-world conclusions on?', 'My guess is that the field immaturity hypothesis is a strong one, @rlombregliaA few months back, there was a great discussion that sprang up in response to a post by @lnrdpss that might also enhance our understanding of this research summary.Mini-post: The Art of Auditing Auditing and Security    CTA: In these threads, we attempt to further the discussion of a key problem in this category and evolve our understanding of the domain space where research work has not yet answered the specific problem or question being considered. These posts are living documents, and it is our hope that the community will continue to contribute to their structure and content. Key Problem / Topic AreaThis post discusses the basic approach to a security audit, using Ethereum smart contracts as an example…  ', 'I agree. I think that the sample size could have been bigger. For what it is, it’s a good entry-level understanding of the smart contract security space, but would have liked it to have gone deeper and longer.For example, instead of just one interview and a survey at mass, it would be nice to have followed their practices specifically asking questions about how they conduct manual reviews and how frequently they change /update their tooling.However, the claims are pretty generally accurate. There is nothing that I came across that seemed outlandish.', 'There is the Enterprise Ethereum Alliance that is building auditing standards, as well as the SCRF compiling common best practices, but there is not a general agreed upon standard. Each smart contract security firm has their own style preferences and standards for auditing right now. What is preventing it from happening is that these firms are not incentivized to share their knowledge properly with the larger community. They profit from knowledge working and staying on the cutting edge.I believe in a more collaborative approach and am looking forward to seeing the standards evolve and grow into something more standard.', 'Right now, as for the industry, it’s pretty ad hoc and word of mouth who is skilled at what. You really need to network and be meeting people in person at conferences or getting referrals to auditors. The known and skilled auditors are booked up 6 months in advance to even take a look at one’s codebase. There is a huge gap between supply and demand. Supply being small and demand being sky-high.There is an auditing DAO I am co-founding called Spearbit (along with Jake Lang and Spencer MacDonald, as well as advisors and other team members) that seeks to ameliorate this by scaling and upskilling more junior auditors, pairing them with senior auditors, and takes a collaborative, knowledge sharing approach to smart contract security auditing.As for your second question, the EEA is dedicated to creating standards for auditors right now as one of their projects and the SCRF is attempting something similar last I checked. There are no standards or trade organizations that I know of that are enforcing standards that are reliable and up to date like accounting standards has GAAP in the US. It’s much more fluid than that and new attacks are happening all the time, so the lead time on standard creation vs. standard enforcement is rather high. i.e. by the time standards may form, they are out of date.', 'There are a lot of people who have opinions about standards, but no agreed upon standards by the large, well-known auditors or auditing firms. This would require cross-company/DAO collabs. As of right now, each has its’ own knowledge base, knowledge work, and they protect it.I’m not surprised at the shock of the lack of standards. I’m surprised too, but it’s only been 6 years since Ethereum was founded. It’s still wild, wild west territory and ,unlike ‘autonomous vehicles’, no one is literally dying directly as a result of the code. It’s mostly arbitrage financial attacks and code exploits, which, while they can be financially devastating, are not in and of themselves lethal. There is, for this reason, less lead time for launching code to production and more of a focus on smart contract security as ‘optional’ in some projects’ opinions.i.e. the field is relatively new and there is currently a coordination happening to create those standards, but all we have are opinions right now. Secureum is the closest one can come to the body of knowledge required to be an auditor, but it just started in October and the results are up in the air.That said, there are common vulnerabilities that are known such as the SWC registry that is commonly referred to or Solidity best practices, but they do not cover comprehensive protection.I hope that sufficiently answers your questions!', ' maxgrok:blockchain forums (60%)research papers (53%)question and answer website (47%)support from professional security experts (47%)In addition to community based efforts, one should also take into account platforms imposing their own security standards. For instance, centralized exchanges have strict guidelines that new tokens must comply to; same for IEO platforms (e.g., Coinlist). So, either you comply or you are out.', '@maxgrok One thing that caught my attention was the fact that 28% of respondents claimed to use fuzzing, but no fuzzing tool was listed amongst the tools used by developers that participated in the survey. Do you happen to know what fuzzing tool were respondents referring to?', ' maxgrok:Enterprise Ethereum AllianceThere is also DeFiSafety, which is a platform that ranks DeFi projects based on specific quality metrics - see their Process Quality Review Process (PQRs). A publicly available list of scores can be found here 2.This sort of effort is a great way to push projects towards better security; if a project gets a low score, it is just bad PR. In a way, the incentives are there . CertiK also has a score system, but it is less clear to me how they achieve scores.While the PQRs from DeFiSafety are by no means a bullet proof standard (and in all honesty, no standard will ever meet that level), it has great benefits:it comes from a neutral entity that has no financial incentive to increase/decrease scores;scores are verifiable, as items have clear guidelines on how to be evaluated;reports are publicly available.Standards, IMHO, will organically start to happen from initiatives such as DeFiSafety; others will likely to be imposed by central platforms (e.g, token security in exchanges); some will be pushed by auditing firms, as projects comply with practices set by auditing firms as a pre-requisite for acceptable security levels. Following a bottom-up approach, eventually, the community will converge on what is acceptable from a security stand point and what is not.']"
                A Note on Privacy in Constant Function Market Makers              ,https://www.smartcontractresearch.org/t/a-note-on-privacy-in-constant-function-market-makers/394,Privacy,43,['https://arxiv.org/abs/2103.01193'],"['anonymity', 'amm', 'cfmm', 'privacy', 'summary', 'ethereum', 'iot', 'network-security', 'zero-knowledge']","['TLDR:Privacy is impossible with most usual implementations of CFMMs under reasonable conditions.There are some strategies that can be used to mitigate this deanonymization and preserve privacy, but they have costs.Core Research QuestionIn what ways are CFMMs privacy-preserving or can users’ transactions be deanonymized?CitationAngeris, Guillermo, Evans, Alex, Chitra, Tarun (2021). “A Note on Privacy in Constant Function Market Makers”. https://arxiv.org/abs/2103.01193. 8BackgroundConstant Function Market Maker (CFMM 3) - A smart contract that allows for the exchange of coins and derivative assets. It employs a convex function which maps asset quantities to implied prices abiding by the principle of supply and demand. Uniswap and Celo utilize similar CFMMs.Decentralized Exchange (DEX) - A type of cryptocurrency exchange where traders can exchange assets with each other or with the protocol without the need for a trusted third party.1-homogeneous functions - A function is homogeneous of degree k if, when each of its arguments is multiplied by any number t > 0, the value of the function is multiplied by t^k.Hyperplane - A hyperplane is a set of potential solution spaces that are subject to a single equality constraint.ε-close - In mathematics 1, ε represents an infinitesimally small quantity, whose limit is usually taken as ε->0. ε-closeness is the property of being a distance from a value that is infinitesimally small.Constant Sum Market Makers - A market maker  1that satisfies the equation where the sum of the reserves are constant.SummaryTransactions in DEXs are public and decentralized. This means it is not necessary to trust a centralized entity to transact. All details of any trade are transparent, meaning they can be directly attributed to a user’s address. This transparency has costs, e.g. front-running and deanonymization. CFMMs like Uniswap are subject to deanonymization via statistical analysis. Zero-knowledge systems, however, might be able to privately execute CFMM transactions. Purportedly privacy-preserving modifications to Uniswap are unlikely to preserve privacy as identity is leaked from the timing of trades.The authors of this paper formalize the intuition that CFMM trades can be deanonymized. Combining information about a CFMM’s function and time-ordered trade data allows an attacker to reverse-calculate trade size. It has been shown in other works 2 that CFMMs benefit from many useful properties of convexity 2. A downside to this is that trade sizes can be easily recovered.Method & ResultsThe researchers 1) use basic convex analysis tools to show how to calculate the uniqueness of transactions and 2) use logical proofs to show how this applies to more general CFMMs and weaker attack models. They also 3) provide some mitigation mechanisms to improve privacy.Impossibility of PrivacyThe researchers define CFMMs using reserve quantities and a trading function.They assume that trading functions are strictly concave and increasing (this does not apply to constant sum market makers).Adversary definition and attackThe researchers assume a simple and general adversary model (Eve) attempting to see the quantity traded by an agent (Alice).Action space: They assume Eve can query the marginal price before and after trades, whether a trade is valid, that Alice’s transaction took place. As such, this would not apply to protocols where users do not know when transactions take place.Attack description: The researchers outline that Eve can always reconstruct the reserves from a marginal price and a single valid trade using basic nonlinear equations. This means that the adversary can compute the reserves before and after Alice’s trade, and thus the trade amounts. Now, Eve only needs to compute the uniqueness of reserve amounts that satisfy these equations. A Newton-type method can be used to get good results, but there are important special cases wthatare much simpler.Reserve discovery in Uniswap: In the case of a CPM like Uniswap, this uniqueness problem reduces to a linear system of equations which simplifies to the following equation.This produces the unique solution (R1, R2) for the reserves for all price-moving trades. This means that using marginal price only, Uniswap reserves can be recovered.Uniqueness of solutionThe researchers show that the reserve discovery equation is unique when the trade function is an increasing, nonnegative, strictly concave 1-homogeneous function. This includes CPM and CMM (eg. Uniswap and Balancer). The researchers suspect that this applies when the function is not necessarily 1-homogeneous but leave that for future work.Reserves at a fixed price: The researchers define a set of reserves consistent with the above constraint. I.e they use a series of proofs to define the set of reserves that map to the marginal prices of trades.Reserves consistent with a trade: they then use another series of proofs to show that the intersection of this set and the set of reserves for which a given trade is feasible is a singleton (i.e. a unique solution exists). The researchers also note that Curve is a counterexample since scaling reserves will change the marginal price.ExtensionsNonzero fees: Where a function has nonzero fees, there is a different equation that the CFMM must satisfy. The feasibility condition is changed but the proof of uniqueness does not change.General homogeneity: This paper assumes 1-homogeneity to make the proofs cleaner, but the proof should be similar for more general p-homogeneity (for p>0).Unknown marginal price: When the marginal price is not known (Eve cannot access), it is not difficult for Eve to compute an arbitrarily good approximation by performing a linear amount of queries.Non-strict concavity: The case where the trading function is not strictly concave is more difficult because some reserve amounts may map to the same marginal price. In some special cases (eg. constant sum market makers), Eve can keep increasing her trade size until a trade becomes feasible and then perform a binary search to get ε-close to true reserves. More general CFMMs whose concave functions are not strictly concave are harder but similar. In those cases, Eve can again query progressively larger trades until they become feasible. This requires a more involved argument that is not made by the researchers in this paper as most CFMMs are strictly concave.Mitigating strategiesThere are a number of conditions that might be used to modify CFMMs to guarantee privacy. The researchers provide examples, but do not prove these conditions in this paper.Randomness in price: The CFMM could add some amount of randomness to the marginal price (as is done in differential privacy) to prevent reconstruction of reserve values. This randomness must be fixed at each block or else it would be too easy to approximate the true price. The researchers note that this can quickly become complex and might cause losses for liquidity providers.Batching orders: Another option is to batch orders together. This does not work if all the other trades in a batch surrounding Alice’s belong to Eve. Also, the delay in batching orders can lead to a bad user experience and could negatively impact solvency with large price movements.Discussion and Key TakeawaysMore complex mechanisms are needed to preserve privacy than would be initially assumed.Impossibility of privacy: The researchers primarily make use of two tricks to show their results.First, because the set of possible trade functions is strictly convex (mathematically simple), any unique hyperplane (i.e. any cross-section of variables) maps to a unique reserve value. This provides a way of identifying reserves at a fixed liquidity. They note Curve as a potential counterexample that all convex trading functions fit the conditions outlined.The second trick used by the researchers is that due to the monotonicity and strict concavity of the trading function, a given trade will either be too expensive for a given liquidity amount (a better trade exists) or cannot be executed (not enough liquidity).Further thoughts: There is a tradeoff between privacy and the cost traders or liquidity providers will pay for it. This can be direct (eg. higher prices) or an indirect cost (e.g. risk of failure). It is not currently clear what price users are willing to pay for this. This raises the question of whether it is worth spending resources to achieve such privacy solutions.Conclusion: As shown by the researchers, privacy in DeFi is difficult to achieve. The researchers suspect there are reasonable variations of CFMM which are privacy-preserving. However, solutions to this problem are either hard to implement in practice or suffer from worse user experience.Implications and Follow-upsThe researchers identify two open questions: 1) does the attack outlined work for all strictly convex CFMMs (not just 1-homogenous) and 2) what does a privacy-preserving CFMM look like and what guarantees can be made.ApplicabilityThe work in this paper applies to all constant function DEXs, for example Uniswap. It also potentially applies to more generalizable automated market makers such as Curve. This information “leak” has broad implications for the DeFi design space.', 'It seems like any additional mechanism to obfuscate the timing of trades would negatively impact price efficiency, which makes this is an incredibly hard problem to solveCould dark pools 2 be a solution here though?Rather than obfucating trade time, the layering of wallets could essentially enable entities to trade via pooled accounts, which could make tracing individual entities a much harder endeavor. This would be analogous to an OTC desk performing trades on behalf of clients using pooled accounts. Do the authors evaluate this as a potential solution?', 'How much does privacy even matter to the average DeFi user? Has there ever been much of an expectation of privacy trading online?', 'You would think privacy only matters to someone trying to hide gains from tax agencies unless there is some other logical reason?  Maybe to prevent front-running?', 'What about hedge funds keeping their strategy to themselves or really anyone trying to keep trade secrets?', 'There is a host of reasons why someone would pursue private transactions. Privacy is a complex topic in the crypto space and a lot of conversation around “if you have nothing to hide” is a bit fallacious. In the traditional finance system, credit card purchases, amazon history, bank statements, etc. are all private. There is a large demand for having two sets of transactions in any payment system since the dawn of accounting: 1) public/business and 2) personal/private.The key takeaway here is not that someone could discern who is transacting with the Uniswap router (as that information is public anyway), but rather that even if you were using a shielded or purportedly private transaction scheme, if you have implemented a CFMM, you would be leaking information anyway.', 'I definitely do not take the perspective of “if they have nothing to hide it should be public,” however in terms of dealing with someone interacting with Uniswap there is a very limited demographic that has the capital or the technical knowledge to execute a swap using a CFMM.  While there is no reason to assume that those practices are exclusively limited to illicit transactions, it would also not be illogical to assume that at least SOME of the transactions on Uniswap are intended to be private for the purposes of skirting capital gains taxes.  As you allude to, even the shielded transactions are leaking information so it’s a false sense of privacy but it stands to reason to at least discuss the motivations of someone attempting to shield transactions even if they are unsuccessful.', 'What might an exchange that was hardened against this look like?', 'Proper AML/KYC is what is meant to protect against this exact type of nefarious activity.The notion of a DEX does not preclude the possibility of creating an AML/KYC oracle that functions as a decentralized mechanism to prevent money-laundering.  That would have to be deployed in a decentralized manner to prevent recentralization (open-source collaboration for example).', 'The best example to think about is ZEXE 3 or any protocol that attempts to build a zk/black-box add-on to Uniswap or another CFMM. Due to the nature of the CFMM convexity, its easy to guess in the face of obfuscation.There are plenty of products in the DeFi space that have touted privacy or anonymity capabilities. The main takeaway is that if they implement a CFMM anywhere in the stack, they are more than likely leaking info anyway.', 'One more follow-up question, related to the researchers’ open questions: this project looked at convex CFMMs, and potentially applies to more generalizable AMMs such as Curve; what are some of the other varieties of CFMM - why might identification be more difficult or impossible (or would it be) with a non 1-homogenous CFMM or is that what a generalizable AMM is?', 'That would be a form of a more generalized view of these AMMs. Basically convex CFMMs are more or less the simplest form of AMM that you can do this kind of statistical analysis on. The more complex and unpredictable the function, the harder it is to ‘guess’, which means the harder it is to ‘leak’ identity information. Again, primarily relevant where the AMM is purported to exist in a privacy-preserving environment.', 'One quick update: We were able to get a partial converse result by showing that you can achieve (ϵ,δ)(\\epsilon, \\delta)(ϵ,δ)-differential privacy if you use a VRF/VDF plus consensus-provided randomness:Paper 4Ethresearch PostIn some ways, this answers @cipherix’s earlier question re: Dark Pools (this is a statistical analogue of them)', 'Welcome to SCRF @tarun, it’s awesome to see you here!I was actually just about to dive into the new publication – it was featured in last week’s Research Pulse (a curation of the top papers posted that week).Privacy in pseudonymous blockchains will always be a touchy topic given the wide range of attack vectors both at the tx graph layer as well as the P2P network layer, but I really like the proposed URE approach. As you point out, it likely won’t get much better than that.URE also seems to be highly complementary to strategies that decrease the profitability of Sandwich Attacks, whereby a single swap is split into many using a relatively straigtforward algorithm. Is this something you have considered in URE’s design?', 'I was actually just about to dive into the new publication – it was featured in last week’s Research Pulse (a curation of the top papers posted that week).Appreciate the mention, thank you.URE also seems to be highly complementary to strategies that decrease the profitability of Sandwich Attacks, whereby a single swap is split into many using a relatively straigtforward algorithm. Is this something you have considered in URE’s design?Yes, in fact, for the worst case input trades, such as the “threshold” batch (T,1,…,1)∈RT(T, 1, \\ldots, 1) \\in \\mathbb{R}^T(T,1,…,1)∈RT, you have to split up trades to preserve any notion of privacy. The main improvement of this paper is that we construct a combinatorial object that coarsely measures “how much” you need to split the big tree to a) preserve privacy while b) not causing too much impact. This trade-off between cost of privacy vs. excess price impact (e.g. a loss felt by users) is a natural way to formulate MEV. The analysis in the paper provides a formal way to measure this cost for any CFMM that has non-zero curvature (e.g. isn’t constant sum).Another thing to note is that we split trades in a different manner than the paper you cited. Instead of deterministically splitting them, we sample a splitting from a Dirichlet distribution. Recall, that a Dirichlet distribution of dimension ddd is a distribution on the probability simplex Pd={x∈Rd:∑ixi=1\u2005\u200a\u2005\u200a∀i\u2009xi≥0}\\mathcal{P}^d = \\{ x \\in \\mathbb{R}^d : \\sum_i x_i = 1\\;\\; \\forall i \\, x_i \\geq 0\\}Pd={x∈Rd:∑i\u200bxi\u200b=1∀ixi\u200b≥0}. We take a trade of size TTT and split it into ddd trades of size TπiT\\pi_iTπi\u200b for π∈Pd\\pi \\in \\mathcal{P}^dπ∈Pd. We choose the parameter of the Dirichlet distribution such that the sizes of the trades are close but (crucially) not predictable. Recent work has shown how to differentially privately sample Dirichlet distributions (e.g. so that an adversary can’t perfectly guess the parameter — which would lead to grinding attacks).How would you implement this in practice? If you have a layer 1 that provides randomness from consensus (as a VRF or VDF) to smart contracts, one can sample a) a random permutation and b) a Dirichlet distribution to get a splitting and ordering that guarantee (ϵ,δ)(\\epsilon, \\delta)(ϵ,δ)-differential privacy', 'If achieving privacy in constant function market maker like Uniswap is a near impossibility, then stakeholders participating in it should have every reason to worry about its sustainability especially in the light of recent development in data protection and privacy in the world. It will be counter productive to trade off the privacy of the users. General Data Protection Regulation(GDPR) as well as laws of various countries have succinct provisions regarding privacy and protection of personal data by institutions and bodies. A situation where identity of Uniswap users can be leaked from the timing of trades and CFMM trades deanonymized by attackers is dangerous to the privacy space. The implication is that CFMM controllers maybe slammed with stiff penalty by Data Protection Authority for not complying with the principles of integrity and confidentiality and obligation to implement appropriate technical and organisation measures commensurate with the anticipated risk. Aside Privacy space, one thing that entices users to in this technology is to shun government unnecessary surveillance. I doubt if this will be the case with CFMM']"
"                Research Summary - Experiments in algorithmic governance A history and ethnography of “The DAO,” a failed decentralized autonomous organization              ",https://www.smartcontractresearch.org/t/research-summary-experiments-in-algorithmic-governance-a-history-and-ethnography-of-the-dao-a-failed-decentralized-autonomous-organization/227,Governance and Coordination,64,['http://iqdupont.com/wp-content/uploads/2018/06/DuPont-Experiments_in_Algorithmic_Governance-2017.pdf'],"['dao', 'summary', 'governance', 'discussion']","['TLDR:Quinn DuPont began an ethnography of the DAO communityDuring his study, the DAO was hacked, allowing him to document how the community dealt with the aftermath.From his findings, Dupont established a framework in which three variations of governance emerged that would affect a decentralized autonomous organization in the future.Core Research QuestionHow can an ethnography help understand the discourse surrounding governance in a decentralized community? How can that information be used to extrapolate whether the operationalization of a concept works or not?CitationDuPont, Q. (2017). Experiments in algorithmic governance: A history and ethnography of “The DAO,” a failed decentralized autonomous organization. Bitcoin and Beyond, 157-177. 12BackgroundThis is a chapter from Bitcoin and Beyond, a book about blockchain technology and cryptocurrency. The chapter’s focus was originally meant to track the Slock.it DAO and its outcomes.During the period in which the author was doing his ethnography, the DAO was compromised and effectively dissolved. The chapter’s focus shifts from attempting to understand the governance involved with the DAO’s operation to examining the community’s discussion of the implications of the attack.Slock.it is a German company that was founded with the intention of leveraging the Ethereum network to establish a DAO and connect the Internet of Things (IoT) to blockchain technology.Decentralized Autonomous Organizations or DAOs are a theoretical framework put forth to facilitate working milestones to be achieved in tandem with reciprocal payment for work completed via a decentralized smart contract.Slock.it created The DAO using that theoretical framework.The project became the largest individual project to date in the Ethereum community, raising over $150 million worth of Ethereum from investors.SummaryThe DAO was launched April 30, 2016, going live with roughly $150 million worth of Ethereum contained within its contract.There was an initial two-week “debate period” during which the community was supposed to decide how to allocate funds, and which projects were most attractive to the investors.After the initial two-week debate period elapsed, The DAO was attacked, and drained of 30% of the Ethereum contained in its contract…Developers had promised a decentralized governance structure would be the guiding principle behind the DAO.1024×479 110 KBThe DAO was meant to be governed by a smart contract acting as the mechanism that facilitated automated investments into projects based on votes by the DAO token-holding community.The collapse of the DAO resulted in the promised governance structures needing to be re-examined, which created a rift between those that believed “code is law” and those who opted to allow a centralized modification mechanism to restore funds to investors after the attack.After multiple warnings had been issued by community members raising issues of theoretical vulnerabilities, a vulnerability known as the “race to empty 6” attack was discovered.Slock.it’s core developer team assured the community that the vulnerabilities would not be a problem and continued to push forward with development.Before the Slock.it team was able to push an update to the system, an attacker drained the DAO of 30% of the ETH supply using the “Race to Empty” attack.The post-exploit community discussion revolved around the concept of “code is law,” whether rolling back transactions would undermine the principle of immutability that was meant to be one of the main value propositions of blockchain technology.The Ethereum Foundation initiated a contentious hard fork that rolled back the transactions and implemented a “withdrawal-only” contract to prevent the previous race to empty exploit from recurring.The hard fork resulted in many of the community members refusing to mine the forked chain, and continuing to mine the compromised chain, establishing “Ethereum Classic” as the ideological opposition to EthereumMethodThe research used a variant of grounded theory methodology; it specifically followed Merriam and Tisdell’s (2016) 1 “Basic” qualitative method.Initially the researcher had a project in development called “The DAO of Whales” to directly test the DAO governance structure.The DAO of Whales was intended to be a fund that coordinated capital to protect whales in the wild, using an autonomous response structure.The collapse of the DAO prevented the DAO of Whales from coming to fruition.After The DAO’s collapse, the researcher gathered comments and posts from community members from Reddit and subforums (/r/Ethereum, /r/TheDAO, etc.)The researcher employed global searches on Reddit to find commentary about The DAO.Comments were ingested into Atlas.ti to code the responses into categories and word clusters.Researcher determined chronology to be the most important axis of analysis since the discourse shifted significantly before, during, and after the exploit.ResultsThe failure of the DAO made it impossible to articulate a specific result on governance, and thus the researcher highlighted three areas of study that could advance understanding: legal authority, practical governance, and the experimental nature of using algorithmic systems for distributed action.In the DAO community, the exploit highlighted the need to reconcile algorithmic authority and judicial legal authority while attempting to establish a new form of legal authority.Lex mercatoria or medieval merchant law was invoked by community members to suggest that the response to legal matters needed to be agile and situational rather than rigid.Many community members who agreed with the hard fork lauded the reaction as an example of good, pragmatic governance.The previous held ideals of practical governance in the context of algorithmic authority were placed in question by the DAO exploit.The community was split between the perspective that the exploit was an expensive lesson for the ecosystem, that it was not philosophically consistent to revert to centralized authority to fix a mistake, and the perspective that full decentralization should not take priority over protecting the community’s funds.The DAO relied on humans to act rationally within the system’s parameters, whereas reality showed that network actors reverted to becoming tied into small self-interested networks.The governance of The DAO discredited its ideological underpinnings and showed a problematic response by developers to risk management and crisis mediation.Discussion & Key TakeawaysDuPont questions whether the Ethereum Classic community actually believed in decentralization or whether they feigned a moral code to protect their investments into the original Ethereum ecosystem.The research argued that the forms of algorithmic authority present in the discourses on The DAO properly exist in a continuum – as governance through algorithms, governance with algorithms, and governance by algorithms.The DAO might have been just a high-risk investment vehicle masquerading as a new way of doing things.The researcher found three key themes of governance emerging from this discourse:(1) the shift of legal authority from existing, juridical authority to algorithmic authority;(2) the difficulty of designing and governing algorithmic systems, and especially immutable and decentralized ones;(3) the challenging ethical terrain of experimentation with forms of distributed action through autonomous, decentralized systems.Implications & Follow-upsThe researcher questions whether blockchain technology and cryptocurrencies should be seen as apparatuses for socio-technical experimentation in society.The researcher posits whether socio-technical experimentation can occur without nefarious actors gaming the system to exploit a given community’s goals to create a profit.The researcher asserts that these technologies could have extremely damaging impact if a system is not well-governed.ApplicabilityThe notion of “code as law” was put forth in an environment which makes it impossible to know what sentiment is organic and what sentiment is social engineering.In an anonymous or pseudonymous environment, opinions can be distorted through the use of sock-puppets and coordinated messaging campaigns.The rigid mantra of “code as law” may be an unrealistic model for governance within the real world and especially in the presence of malicious attackers, but the advancing complexity of algorithms may eventually create an environment in which a malicious attacker cannot affect the entire network and is compartmentalized.', 'This is one of only a handful of ethnographies of major crypto projects that have come out. I’d be curious to compare the methodology with Koray Caliksan’s “Rise, Fall, and Rebirth of Electra Protocol 2” or Ludovica Rella’s “Steps towards an ecology of money infrastructures: materiality and cultures of Ripple 3.” An initial gloss: Rella portrays Ripple as shifting the definition of money towards something more aware of its role in creating a marketplace (which he describes as a ""New materialist take on infrastructure in Leigh Star and Easterling meets social theory of money to understand blockchain and DLT’):“Infrastructures, understood ecologically, include devices, active forms, and imaginaries in seamless webs of mutual relations of co-evolution. These ecologies are always potentially prone to slippage, dissolution, disassembling, reassembling and reappropriation, dependence, and competition.”Caliksan takes an approach that more closely resembles Dupont’s, I think, reading it as a community of actors interacting and generating the protocol between them.', 'Thanks for these links and brief overviews! It makes a lot of sense to me that we should be doing some comparisons between analyses of projects to see if there are common kernels we can pull out as generalizable knowledge to keep in mind for future projects.From this post, @Larry_Bates drew our attention to some of the key themes regarding governance. Larry_Bates:The researcher found three key themes of governance emerging from this discourse:(1) the shift of legal authority from existing, juridical authority to algorithmic authority;(2) the difficulty of designing and governing algorithmic systems, and especially immutable and decentralized ones;(3) the challenging ethical terrain of experimentation with forms of distributed action through autonomous, decentralized systems.Do you think the Caliksan and/or Rella pieces surfaced similar findings regarding governance?', 'By crypto standards, this article is paleolithic (published 2017). Decentralized Autonomous Organizations are very much in the news again. Back in 2017, for most of us, they seemed like a vague promise of crypto-powered ‘living’ organizations like self-maintaining, autonomous vehicles surviving as taxis (and presumably issuing tokens to investors). Today we’ve seen a huge growth in DAOs, there are DAOs buying art, DAOs directing stories, DAOs directing charitable organizations (including Dupont’s whale DAO) and of course DAOs being used to direct massive crypto projects.Under what conditions should someone consider a DAO?  Is the explosion of DAOs a miniature bubble like ICOs were in 2017 or are there clearer use cases this time around?', 'I believe a lot of the “DAOs” of today are not actually “decentralized” and are “DAO” in name only.  We are so far from “self-directing” pools of capital that the term “DAO” is really a stretch of truth when applied in modern times if not an outright lie.  I think we need to reconsider what “decentralized” means and what qualities must exist for an organization to be “decentralized”.  I would start with not having one 51% owner as a simple metric for “decentralization of ownership”.  If an organization can’t claim “decentralized ownership” how could it be a “decentralized organization”?I believe before we can actually achieve a “DAO” as originally envisioned and intentioned we might need to start from a more honest framework to show that we are going from “centralized” to “more decentralized”.  As of now, the space made it seem like we could make a hard transition from centralization to decentralization, but clearly that is not how the transition works in the real world.I think one of the best actual use-cases for a DAO would be to consolidate capital in a situation where a union might be desired but unattainable.  Mixed-martial arts fighters are a perfect example:  their livelihood depends upon them working as individuals, but they would have better collective bargaining if they pooled their resources.  In that they are not allowed to form unions, a “FightDAO 1” has been proposed as a solution to this specific problem.  I do believe situations like individual fighters wanting to pool resources, but not being allowed to unionize is an example of a situation where a “DAO” is the natural logical solution.  However, the parts of the DAO that would need to be “automated” to ensure there is no manipulation of the market or manipulation of the pooled resources are not yet decentralized enough to claim a true “DAO” status.', '@Fizzymidas do you know of any legal frameworks that might provide some guidance for something like a DAO? I wonder if you’d be able to merge a corporation with a governance token or something like that.', '“Merge a corporation with a governance token”It would be difficult to do this without the tokens qualifying as “securities” which would then require a whole new level of regulation.  Adding voting rights to an organization in the US creates a whole quandary of regulatory problems that the crypto space wants to avoid by operating outside of the US.', 'I think we need to reconsider what “decentralized” means and what qualities must exist for an organization to be “decentralized”.There’s so much to be learned about both the idea of measuring decentralization and in term of understanding the process of how to decentralize specific functions. A rush to poorly thought out decentralization could definitely expose the project to fundamental risks, as was witnessed with The DAO and as DuPont pointed out in his chapter.This also made me think about the risks that an ecosystem faces from the perspective of governance, or Governance Extractable Value as Leland Leeand and Ariah Klages-Mundt called it in their post on the topic.. The list of potential ways that a group of users can extract value from an ecosystem for their personal benefit is likely to grow given the amount of money being managed by DAOs and the reality that some projects just copy their governance approaches without meaningfully exploring the potential risks.As Coopahtroopa mentioned in his post 2 on the DAO Landscape, there were more than 100 DAOs managing over $10 billion in assets as of June 24, 2021.Screen Shot 2021-07-29 at 8.51.23 PM1458×888 176 KBIt will be interesting to see what the total amount of money managed by DAOs ends up being by the end of the year. Especially if more project decide to use exiting to DAOs (or ‘Dexiting’ to quote @Rich) as an attempt to avoid legal or regulatory blame, then it’s likely that we’ll see more governance exploits result from rushed rollouts.I’m also very interested in exploring the role that culture plays in helping or hurting governance exploits as well. If you happen to know any good writing on the topic, please share!Link citations:Leeand, Leland, and Ariah Klages-Mundt. “Deep Dive #2.” Our Network, April 23, 2021. Our Network: Deep Dive #2 - OurNetwork.Coopahtroopa. “DAO Landscape.” Mirror, June 24, 2021. coopahtroopa.mirror.xyz.', ' Larry_Bates:spaceAt the moment no such legal Framework exists.Wyoming tried, but the anonymity tradeoff does not please crypto people.And like Chris said, trying to adopt existing corporate and organizational frameworks automatically turns those tokens into securities.', '@Larry_Bates Thanks for sharing this research on practical observation of DAO governance behaviors, it’s super valuable for me who are interested in DAO potential implementations on social mechanisms. Larry_Bates:However, the parts of the DAO that would need to be “automated” to ensure there is no manipulation of the market or manipulation of the pooled resources are not yet decentralized enough to claim a true “DAO” status.I much agree with the need for “automated”. I’d love to know your opinion on if we can define a standard for the degree of this? Does it make sense to use a given percentage to identify its decentralized degree? For example, company laws or regulations in some jurisdictions regulate the dispersion of share ownership for a company seeking for being listed (In Taiwan, the number of registered shareholders is 1,000 or more. Excluding company insiders and any juristic persons in which such insiders hold more than 50 percent of the shares, the number of registered shareholders is at least 500, and the total number of shares they hold is 20 percent or greater of the total issued shares, or at least 10 million 1). This kind of regulation could be regarded as the decentralization standard that the regulation requires for a listed company. Do you think we can also use some similar approaches to define a DAO? On the other hand, I also learned from  Smart Contract Summit 2021: Governance Theory Panel that there are hierarchies in DAO communities by some KOL’s influence, would it make the purpose of seeking for the standard of decentralization become senseless?', 'Astrid, these are fantastic observations and connections!  One of the implicit points you make by proxy: you don’t need a blockchain to decentralize an organization.  Further, I think while there is a good intention of establishing a metric, my experience tells me that decentralization has to be a tool and not a state in order for it not to become a point of failure.  What I mean by this is that if a specific state becomes defined as the ideal “decentralized” standard, that means that nothing could ever be more decentralized than that organization.  I think anyone can extrapolate why “decentralization as a state” does not work as an ideal.I think it would be natural for a centralized organization to establish a centralized definition of “decentralization,” in effect declaring a unilateral form of decentralization in the process as their one and only form of decentralization.In this light, an ever-changing spectrum of decentralization states on which an organization or state body can move makes more sense to me than a fixed scale.  Further, the type of decentralization has to be articulated in order to ensure that indeed everyone participating in the conversation is discussing the same “thing”.In short, I do believe a single standard would not only be “useless,” but it would completely undermine the notion of “decentralized” by having a unilateral definition.  On the other hand, a spectrum of types of decentralization would also have to be ever-changing, and that does not align with how definitions work.  I think we are entering a new paradigm of inter-disciplinary definition sharing, so that the redundancies across different industries start to disappear when unnecessary; but the characteristic separators that define what “type” of decentralization is being analyzed become the spectrum on which the different definitions live and evolve.To sum: if something is identified as “decentralized” and the question is asked “can this be MORE decentralized,” inevitably the answer is almost always “yes”.  This is why I pose that decentralization should be used as a tool and not a state so that we are not continually chasing a state that cannot philosophically be achieved.', '@Larry_Bates Thanks for your reply and the explanation of the ever-changing spectrum. I agree we have to avoid defining decentralization as a state and it should more likely be a spectrum. Flexibility is necessary no matter for hermeneutics or industry development.  I just like to resolve a practical legal issue, from the lens of legal policymakers, it’s important to define DAO and find someone who is liable for a DAO’s actions. The current entities liability system’s dependent path is to categorize several types of entities, then define the responsibility of every role in a certain type of entity. Though the crypto community may not like this, a clear liability model may foster the adoption of this mechanism and reduce the bias.']"
                Research Summary: Sonic: Zero-Knowledge SNARKs from Linear-Size Universal and Updatable Structured Reference Strings              ,https://www.smartcontractresearch.org/t/research-summary-sonic-zero-knowledge-snarks-from-linear-size-universal-and-updatable-structured-reference-strings/832,Cryptography,36,['http://DOI.org'],"['summary', 'discussion', 'network-security', 'quantum-computing', 'iot']","[""TLDR:Sonic is a universal zk-SNARK scheme, which makes applications launchable without running a separate trusted setupSonic supports a universal and continually updatable structured reference string that scales linearly in supported circuit size.Core Research QuestionGiven that zero-knowledge applications require a trusted setup, which adds significant costs to builds and iterations,  how can we design an efficient zk-SNARK scheme and use it universally?CitationGroth, Jens. “On the Size of Pairing-Based Non-Interactive Arguments.” Advances in Cryptology – EUROCRYPT 2016, edited by Marc Fischlin and Jean-Sébastien Coron, vol. 9666, Springer Berlin Heidelberg, 2016, pp. 305–26. DOI.org (Crossref), https://doi.org/10.1007/978-3-662-49896-5_11.Bunz, Benedikt, et al. “Bulletproofs: Short Proofs for Confidential Transactions and More.” 2018 IEEE Symposium on Security and Privacy (SP), IEEE, 2018, pp. 315–34. DOI.org (Crossref), https://doi.org/10.1109/SP.2018.00020.Groth, Jens, et al. “Updatable and Universal Common Reference Strings with Applications to Zk-SNARKs.” Advances in Cryptology – CRYPTO 2018, edited by Hovav Shacham and Alexandra Boldyreva, vol. 10993, Springer International Publishing, 2018, pp. 698–728. DOI.org (Crossref), https://doi.org/10.1007/978-3-319-96878-0_24.Bootle, Jonathan, et al. “Efficient Zero-Knowledge Arguments for Arithmetic Circuits in the Discrete Log Setting.” Advances in Cryptology – EUROCRYPT 2016, edited by Marc Fischlin and Jean-Sébastien Coron, vol. 9666, Springer Berlin Heidelberg, 2016, pp. 327–57. DOI.org (Crossref), https://doi.org/10.1007/978-3-662-49896-5_12.Kate, Aniket, et al. “Constant-Size Commitments to Polynomials and Their Applications.” Advances in Cryptology - ASIACRYPT 2010, edited by Masayuki Abe, vol. 6477, Springer Berlin Heidelberg, 2010, pp. 177–94. DOI.org (Crossref), https://doi.org/10.1007/978-3-642-17373-8_11.Fuchsbauer, Georg, et al. “The Algebraic Group Model and Its Applications.” Advances in Cryptology – CRYPTO 2018, edited by Hovav Shacham and Alexandra Boldyreva, vol. 10992, Springer International Publishing, 2018, pp. 33–62. DOI.org (Crossref), https://doi.org/10.1007/978-3-319-96881-0_2.Bellare, Mihir, et al. “NIZKs with an Untrusted CRS: Security in the Face of Parameter Subversion.” Advances in Cryptology – ASIACRYPT 2016, edited by Jung Hee Cheon and Tsuyoshi Takagi, vol. 10032, Springer Berlin Heidelberg, 2016, pp. 777–804. DOI.org (Crossref), https://doi.org/10.1007/978-3-662-53890-6_26.Lindell, Yehuda. “Parallel coin-tossing and constant-round secure two-party computation.” Journal of Cryptology 16.3 (2003).LinkSonic: Zero-Knowledge SNARKs from Linear-Size Universal and Updatable Structured Reference Strings 5BackgroundMost zk-SNARKs require a trusted setup. A setup can only be used once, thus different applications and versions require a new setup each time. These setups are costly, they take a long time and need multiple people to participate. However, Groth [1] demonstrated an efficient scheme (known as Groth 16) which contains only three group elements. There are also proving systems that work without a trusted setup such as BulletProofs (Bunz et al. [2]), but BulletProofs verification time scales linearly. Therefore those systems are better suited to  simpler applications.Groth et al. [3] have proposed a zk-SNARK scheme with a universal and updatable structured reference string (SRS, the outcome of the trusted setup in (some) zk-SNARK schemes construction). This universal property makes its setup not application-specific and the updatable property increases the confidence of the setup. However, this scheme requires an SRS that is quadratic with respect to the number of multiplication gates in the supported arithmetic circuits, a quadratic number of group exponentiations for verifying, and a linear number of pairings for updating SRS. Besides, deriving circuit-specific strings requires an expensive Gaussian elimination process.In this work, Sonic introduces a new scheme based on Groth et al. [3] with a lot of efficiency improvements.SummaryIntroductionSonic is a new zk-SNARK that still needs a trusted setup, but the setup can be used in different circuits (applications) without being pre-processed and can be updated. These two features make Sonic more applicable and reliable(trustable) for the industry.It also brings better efficiency by making the proof elements in the same group to reduce pairing operations, introducing a method for verifying correct evaluation, and introducing a method to speed up the verifier by outsourcing some operations in batching to untrusted helper parties.Sonic defines its constraint system with respect to the two-variate polynomial equation used in Bulletproofs that was designed by Bootle et al. [4].The polynomial determined by the instance of the language can be split into monomials scaled by each element of the instance.Groth et al. [3] showed that an SRS that contains monomials is updatable.SRS is used to derive the instance of the language and the polynomial determined by the constraints is known to the verifier, thus no constraint-specific secrets were put in the SRSSonic uses a variation of PCS by Kate et al. [5] (known as Kate Commitments)The authors prove it’s secure in the AGM (Fuchsbauer et al. [6]) as this work needs to use two-variate polynomials while Kate Commitments are designed for a single-variate polynomial and this work needed another feature that an adversary can extract the committed polynomials from.If the prover and the verifier both know a two-variate polynomial that the verifier wants to calculate, the work can be unloaded onto the prover.Sonic unloads the work of computing the polynomial specifying the constraints onto the prover.There are two ways of achieving this. One provides a proof of evaluation correctness from each prover, the other introduces a role “helper” who calculates the circuit-specifying polynomial for each proof in the former way (batching)Definitions for Updatable Reference StringsThe subvertible and updatable SRS model“Subvertible” means that it’s secure when the parameters are maliciously generated at setup (Bellare et al. [7]).“Updatable” means that it’s secure when the adversary maliciously performs some update (Groth et al. [3])It’s defined by two Probabilistic Polynomial Time (PPT) algorithms, Setup and Update, and one deterministic Polynomial Time (DPT) algorithm, VerifySRS.Setup: takes a security parameter and returns an SRS and proof of its correctnessUpdate: takes a security parameter, an SRS, and update proofs and returns an updated SRS and a proof of the correctness of updatesVerifySRS: takes same inputs as Update and returns a bit indicating acceptance or rejectionBellare et al. [7] described that a protocol cannot satisfy both subvertible zero-knowledge and subvertible soundnessSonic preserves the subvertible zero-knowledge and provides update knowledge soundness, which means if an adversary has all randomness used in setup and update, the adversary can create a valid proof with invalid inputsTo prove Sonic provides update knowledge soundness, it uses a technique here called “witness-extended emulation” with respect to Lindell [8]MethodIn this summary, we will walk through the protocol and provide context for the techniques used in each steps.Pre-ProcessingIn the pre-processing session, the prover will construct polynomials from constraints. Note that it’s not a Trusted Setup.814×384 23.5 KBSystem of ConstraintsSonic represents circuits using a form of constraint system proposed by Bootle et al. [4]. In brief, there will be 4 polynomials:r(X, Y): constructed by provers with their hidden witness, designed that r(X, Y) = r(XY, 1) by protocols(X, Y): signature of correct computation, constructed from constraints onlyt(X, Y) = r(X, 1)r'(X, Y) - k(Y): constant term of it is designed to be zero when the constraint system is satisfied, r'(X, Y) stands for r(X, Y) + s(X, Y) and we will use it later.k(Y): which is used to upload instance k into constraint systemThe X and Y are indeterminates.ProtocolHere Sonic will represent an interactive protocol that can be turned into non-interactive with the Fiat-Shamir transformation. Refer to the original paper chapter 6 for details.1820×854 78.6 KBn is the number of gates and d is some number large enough (actually, 3n < d). x \\xleftarrow{\\$} S denotes sampling a member uniformly from S and assigning it to x.InputsCommon Inputs: a bilinear group, an SRS, s(X, Y), k(Y), a paring function e(g, h^\\alpha)Prover’s Inputs (witness): a, b, cStepsAs the prover and the simulator will evaluate g^{r(x,1)}, r(z,1), and r(zy, 1), the prover first extends r(X, Y) with 4 blinders to make them indistinguishable from each other and commits to r(X, 1) (extended).The verifier sends a random challenge y and the prover replies the commitment t(X, y), which has no constant term as mentioned above.The verifier sends another random challenge z and the prover opens the committed polynomials to r(z, 1), r(z, y), t(z, y)The verifier computes r'(z, y) = r(z, y) + s(z, y) (the prover or a helper can provide it with a signature of correct computation) and check r(z, y)r'(z, y) - k(y) = t(z, y)Signatures of Correct ComputationSonic uses a signature of correct computation to ensure that an element s is equal to s(z, y).  The polynomial s(X, Y) is designed to be able to apply permutation. Then, the verifier’s computational costs at step 4 are offloaded to the prover or a helper. The permutation can be reused between instances by generating a derived reference string.The helper will join protocol when available and there is a sufficiently large number of proofs in the batch. The idea is the helper commits to s(X,y_j) for each element y_j, and opens at s(z_j,y_j). The verifier will challenge it and be convinced that all the signatures are correct, thus the proof size is reduced. That is to say, instead of sending and verifying proofs of s(z,y) for each input, the helped version sends a proof of correctness of those proofs.ResultSonic is a competitive zk-SNARKs, it has an efficient performance and is universal and updatable.Proving Knowledge of PreimageThe authors implemented the helped version and obtained the following table by using BLS12-381 elliptic curve construction on CPU i7 2600K with 32GB RAM, running at 3.4GHz.1240×427 77 KBAsymptotic efficiency comparison of zero-knowledge proofs for arithmetic circuitsThe following table shows the comparison with related works. As Sonic can be considered as an evolution of Groth et al. [3] (Groth et al. [46] in the table), the major differences are 1) CRS is shorter 2) use the AGM (Fuchsbauer et al. [6]) assumption which could be considered less secure.953×437 142 KBComparison against a pairing-based zk-SNARK and against Bulletproofs953×249 102 KBDiscussion and Key TakeawaysTraditional zk-SNARKs need a trusted setup for a circuit (application). By introducing universal SRS, we can now construct it once and use it in circuits under a limitation on the number of gates and depth of the setup.By making the SRS updatable, the confidence of security is increased, as any one of the participants destroys zir waste, the adversary won’t be able to cheat.Implications and Follow-upsSonic introduces a new zk-SNARK scheme that can solve the problem of trusted setup, making developers easier to build and iterate zk apps.This work might be considered as a milestone that inspired PlonK, RedShift, Marlin, Halo, and so on. Those zk-SNRAKs are applied to many systems now, and many other universal zk-SNARK works.ApplicabilityIt seems no application directly uses Sonic, however, this work brings the applicability of zk-SNARKs to the next level."", '@flyinglimao Thanks for a fascinating contribution to the Research Summaries! I have a few simple questions about your summary:You state that “most zk-SNARKs require a trusted setup,” which may imply that some do not. Is this true, and if so, what can we learn from these “other” zk-SNARKs?Why are traditional zk-SNARKs trusted setups so time-consuming and costly?What are the major zk-SNARKs platforms and the differences among them?You state that by making SRS updatable, Sonic “might be considered a milestone that inspired PlonK, et al.,” but you also state that “no application directly uses Sonic.” Does this suggest that Sonic’s contribution is of historical interest only, or does Sonic have the possibility of real-world adoption?', 'Hi @rlombregliaI heard Spartan[1] that Microsoft published and claimed it doesn’t need a trusted setup and RedShift[2] is also zkSNARK without a trusted setup, so I said most of them require it. For the latter question, I can’t provide an answer yet.As I heard from a participant of a trusted setup ceremony in a ZKP study group in TEM 1, they usually have to run a script for hours to guarantee the randomness and then pass the result to the next participant. That has to be done by a lot of participants to convince users (e.g. if a global system was set up by a group of only 10 contributors, it’s more likely to be considered insecure). But I’d say that is a practice consideration instead of an academic research conclusion.I think these can first be split into 2 groups: non-universal (traditional) and universal. For platforms in the same group, the major differences will be proof size, verification time, proving time, and setup procedure, and other differences will be post-quantum, assumption (security), and so on. As I know so far, Groth16 might be the most important platform in former groups as it has better results than others on the major differences. The latter groups have Sonic, PlonK, RedShift, Halo…etc. I’ve not read into them so I can’t describe the exact differences here, but the major differences mentioned above may be part of those.My answer to this question is totally in my opinion. I’d say that the previous work (Groth, Jens, et al., Citation 3 in summary) brought the interest to researchers but had practice issues and Sonic solved them and brought the possibility of adoption. The reason Sonic wasn’t used is the evolution is too rapid.https://eprint.iacr.org/2019/550.pdfResearch Summary: REDSHIFT: Transparent SNARKs from List Polynominal Commitment IOPs', 'Hi @flyinglimaoVery interesting topic. What could be the next steps in terms of convincing industry players to adopt Sonic for those who already use the “mainstream” zk-SNARKs?Would there be any instances where Sonic wouldn’t be as effective as your summary claims? If so, what would be some of the limiting factors and why?', 'Hi @shouleIn my opinion, if an application had run a trusted setup and doesn’t need an update, it may not necessary for them to adopt Sonic, PlonK, and other universal zkSNARKs for performance consideration (except the construction they used is less efficient than those). But for those who are going to release a new app or delivery an update, adopting universal zkSNARKs can speed up the software iteration and cut the cost of starting. (I indeed don’t know if people will adopting Sonic instead of PlonK and if PlonK is only a more efficient scheme based on Sonic without bringing other changes.)If the amount of users of a Sonic application isn’t enough to make a helper work, Sonic might unable to be most effective in theory. Except that, I don’t have an idea about if any factor may make it not effective, but I also cannot simply say no since I don’t have enough knowledge about it, thus I can’t provide an complete answer.', 'Hey @flyinglimao, thanks for the summary.The previous zk-SNARK was impractical because the SRS (Structured reference string) resulted in large computation time. To what extent did Sonic solve this problem?What might be the other trade-offs for Sonics and universal SNARKs in general? It’s hard to imagine why no application directly uses Sonic, given that universality seems like a critical improvement.', 'Hi @TwanI’m not sure if the kinda answer is what you want and the computation is correct, according to the table (Result > Proving Knowledge of Preimage), the previous work might need an SRS with size (3.74 MB * 1024 KB/MB * 1024 B/KB / (8~1024) B)^2 * (8~1024) (element/B) ~= 1.79 TB ~ 13.99 GB for proving a preimage of Pedersen hash with 384 bits input (here assume an element in SRS takes 8 bytes (long int) to 1024 bytes), while Sonic used only 3.74 MB.As I know so far, the most trade-offs for universal SNARKs are SRS size, verification time, and proving time. I don’t know much about the industry, therefore, I can’t give an answer to the reason for not adopting.', 'Hi @flyinglimao. Thanks for your detailed response to my questions. Obviously, I’m not an expert on zk-SNARKs, but in my general reading I have seen it said that “the time for zero-knowledge proofs has arrived,” and that the time-consuming downsides of trusted setups make universal SNARKs especially appealing.You mentioned that Sonic “wasn’t used [because] the evolution is too rapid.” I’m not sure what the word “evolution” refers to, so I’ll ask part of my question again: Is Sonic (in your view) a currently applicable solution going forward, or has the “evolution” of universal, update-able SNARKs made Sonic an early pioneer that is now part of the history of this field?Or is the work on Sonic still ongoing? If that is true and “Sonic v.2” is in the works, can you disclose anything about the way it will solve some of the “evolutionary” issues that zk-SNARKs in general are facing?', 'Hi @rlombreglia.In my view, Sonic is an applicable solution but we may use Marlin or PlonK instead as they improved the efficiency (that’s the “evolution” in my previous words). Hence for short, it’s not suitable currently as we have alternatives that have better efficiency, and I’ll agree that Sonic is now a part of the history. (I’m not sure what’s “currently applicable” means here. Maybe giving an analogy here will help: Sonic is like Dial-up internet that it’s applicable but not suitable today.)I saw there is SuperSonic 1 but I’ve not read into it and it’s not a work from anyone of the Sonic authors thus I don’t consider it as “Sonic v2”. I’m trying to give a roadmap: Mary Maller (first author of Sonic) published the Marlin which improves efficiency by using algebraic holographic proof. Sean Bowe (second author) published the Halo which does too by using an aggression technique (here is a post 1 explaining Halo 2 and mentioned Sonic from him, maybe it’s a good material FYI).', 'You mentioned that SONIC has become part of SNARKS history. I was wondering if you could shed some light on something Vitalik Buterin said in response to a question about which privacy technology would be the most prevalent in 2050: he replied, “I expect ZK-SNARKs to be a significant revolution as they permeate the mainstream world over the next 10-20 years.” and when asked about the first SNARK(ed) application to hit 10mm users would be, replied “Oh my answer is something much more mundane like some cryptographic anti-sybil thing by Cloudflare.” @flyinglimao, how do you imagine SNARKS and privacy in general over the next few decades?', 'This is a question for @cipherix (SCRF research lead) who suggested this article for summary: I know that background is important to SCRF’s selection criteria, I’m curious whether or not there was an order to the way you chose which zero-knowledge summaries to feature on SCRF. We’ve had a couple of questions about the historical value of SONIC, universal generation and Kate Commitments, where there any other considerations for SONIC or the other pieces featured on SCRF?', 'Hi @jmcgirk.For me, SNARKS (or more general, ZKP) is a tool that brings applications (including blockchain, web, and many things) a way to balance transparency and privacy. Also, the awareness of privacy is increasing these years, I imagine that people may not understand how it works but why they need and will use them frequently (just like TCP/IP, SSL).', 'This is a great question and I don’t think there is a perfect sequencing for SNARK coverage. We have covered what are arguably the two most significant schemes under the Knowledge of Exponent (KoE) assumption, SONIC and PLONK, but there are several other schemes worth summarizing.@flyinglimao is right in that SuperSonic is the natural evolution of Sonic. From a taxonomy perspective, SuperSonic falls under a new variant of asymmetric assumptions generally called Groups of Unknown Order Together with DARK, these constructs can provide some usability benefits and do not require a trusted setup. Here is an incredibly helpful post by STARKWARE that does a great job contextualizing this evolution:Medium – 9 Jan 20The Cambrian Explosion of Crypto Proofs 4And the role of symmetric STARKs withinReading time: 14 min read', '@cipherix, I immediately recognized, ‘cambrian explosion’ in the reference link you provided here, with similar phraseology you used in SCRF Interviews: PlonK and SNARKS with Ariel Gabizon and Lucas Nuzzi. Digesting the referenced article put the organization of cryptographic proofs into perspective for me. Specifically, the figures are fantastic, and represent a plethora of information in a succinct manner. I see an outline for what original research papers to read and when doing so, the overarching context they fit into as a whole. Additionally, I see a reference framework for reading future research papers in this space. This article gave me an on-ramp and a reference frame to this topic…greatly appreciated!', 'Just some appendix here…For ZK proof systems, terms and glossaries are often hard to describe and harder to categorize.We have:“require trusted setup”“does not require trusted setup”“(SRS), require trusted setup but updatable”“(CRS) require trusted setup and not updatable…”etc…Although it’s far from being standardized/widely recognized, I’d say if we use the following abbreviations (usage promoted by matter labs, probably, citation needed), it can make discussing things easier=we now know what each other’s talking about faster, and also makes googling (SEO-wise) easier.so,SNARK: Groth16, the snark we knowrequire trusted setup (CRS), not updatable and thus not universal, generally accepted and widely used atm.SNORK: Started from sonic=>plonk/merlin, O stands for oecumenical, a fancy way of saying that it’s universal and updatable. Still require trusted setup as sort of an infrastructure for the first time, but the parameter can be reused by anyone for circuits under a certain size (hence universal/updatable).STARK: Started from FRI STARK(Fast Reed-Solomon Interactive protocol)=>halo/supersonic/fractal…, and T stands for transparent. Trusted setup is not needed.Although there are so many variations about the primitives (lego block) in use of those systems, I hope that by introducing the top level categorizing words in our discussions (snark/snork/stark), we can help standardize the word usage and make newer learner feel less confused about it.(Language can and will evolve - we just need to embrace the usage we think is the fittest)']"
